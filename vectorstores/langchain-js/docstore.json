[["0",{"pageContent":"# Databerry\n\nThis page covers how to use the [Databerry](https://databerry.ai) within LangChain.\n\n## What is Databerry?\n\nDataberry is an [open source](https://github.com/gmpetrov/databerry) document retrievial platform that helps to connect your personal data with Large Language Models.\n\n![Databerry](/img/DataberryDashboard.png)\n\n## Quick start\n\nRetrieving documents stored in Databerry from LangChain is very easy!\n\n```typescript\nimport { DataberryRetriever } from \"langchain/retrievers/databerry\";\n\nconst retriever = new DataberryRetriever({\n  datastoreUrl: \"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc\",\n  apiKey: \"DATABERRY_API_KEY\", // optional: needed for private datastores\n  topK: 8, // optional: default value is 3\n});\n\n// Create a chain that uses the OpenAI LLM and Databerry retriever.\nconst chain = RetrievalQAChain.fromLLM(model, retriever);\n\n// Call the chain with a query.\nconst res = await chain.call({\n  query: \"What's Databerry?\",\n});\n\nconsole.log({ res });\n/*\n{\n  res: {\n    text: 'Databerry provides a user-friendly solution to quickly setup a semantic search system over your personal data without any technical knowledge.'\n  }\n}\n*/\n```\n","metadata":{"source":"docs/docs/ecosystem/databerry.md","loc":{"lines":{"from":1,"to":41}}}}],["1",{"pageContent":"# Helicone\n\nThis page covers how to use the [Helicone](https://helicone.ai) within LangChain.\n\n## What is Helicone?\n\nHelicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\n![Helicone](/img/HeliconeDashboard.png)\n\n## Quick start\n\nWith your LangChain environment you can just add the following parameter.\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\nNow head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.\n\n![Helicone](/img/HeliconeKeys.png)\n\n## How to enable Helicone caching\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Cache-Enabled\": \"true\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\n\n## How to use Helicone custom properties\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Property-Session\": \"24\",\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\n        \"Helicone-Property-App\": \"mobile\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)\n","metadata":{"source":"docs/docs/ecosystem/helicone.md","loc":{"lines":{"from":1,"to":68}}}}],["2",{"pageContent":"# Unstructured\n\nThis page covers how to use [Unstructured](https://unstructured.io) within LangChain.\n\n## What is Unstructured?\n\nUnstructured is an [open source](https://github.com/Unstructured-IO/unstructured) Python package\nfor extracting text from raw documents for use in machine learning applications. Currently,\nUnstructured supports partitioning Word documents (in `.doc` or `.docx` format),\nPowerPoints (in `.ppt` or `.pptx` format), PDFs, HTML files, images,\nemails (in `.eml` or `.msg` format), epubs, markdown, and plain text files.\n`unstructured` is a Python package and cannot be used directly with TS/JS, Unstructured\nalso maintains a [REST API](https://github.com/Unstructured-IO/unstructured-api) to support\npre-processing pipelines written in other programming languages. The endpoint for the\nhosted Unstructured API is `https://api.unstructured.io/general/v0/general`, or you can run\nthe service locally using the instructions found\n[here](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image).\n\n## Quick start\n\nYou can use Unstructured in`langchainjs` with the following code.\nReplace the filename with the file you would like to process.\nIf you are running the container locally, switch the url to\n`https://api.unstructured.io/general/v0/general`.\n\n```typescript\nimport { UnstructuredLoader } from \"langchain/document_loaders/fs/unstructured\";\n\nconst loader = new UnstructuredLoader(\n  \"https://api.unstructured.io/general/v0/general\",\n  \"langchain/src/document_loaders/tests/example_data/example.txt\"\n);\nconst docs = await loader.load();\n```\n\nStayed tuned for future updates, including functionality equivalent to\n`UnstructuredDirectoryLoader` in `langchain`!.\n","metadata":{"source":"docs/docs/ecosystem/unstructured.md","loc":{"lines":{"from":1,"to":38}}}}],["3",{"pageContent":"---\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat_streaming_stdout.ts\";\n\n# Quickstart, using Chat Models\n\nChat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.\n\n## Getting Started\n\nThis section covers how to get started with chat models. The interface is based around messages rather than raw text.\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage, SystemChatMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({ temperature: 0 });\n```\n\nHere we create a chat model using the API key stored in the environment variable `OPENAI_API_KEY`. We'll be calling this chat model throughout this section.\n\n### Chat Models: Message in, Message out\n\nYou can get chat completions by passing one or more messages to the chat model. The response will also be a message. The types of messages currently supported in LangChain are `AIChatMessage`, `HumanChatMessage`, `SystemChatMessage`, and a generic `ChatMessage` -- ChatMessage takes in an arbitrary role parameter, which we won't be using here. Most of the time, you'll just be dealing with `HumanChatMessage`, `AIChatMessage`, and `SystemChatMessage`.\n\n```typescript\nconst response = await chat.call([\n  new HumanChatMessage(\n    \"Translate this sentence from English to French. I love programming.\"\n  ),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```\n\n#### Multiple Messages\n\nOpenAI's chat-based models (currently `gpt-3.5-turbo` and `gpt-4`) support multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:\n\n```typescript\nresponse = await chat.call([\n  new SystemChatMessage(\n    \"You are a helpful assistant that translates English to French.\"\n  ),\n  new HumanChatMessage(\"Translate: I love programming.\"),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```\n\n#### Multiple Completions\n\nYou can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.\n\n```typescript\nconst responseC = await chat.generate([\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love programming.\"\n    ),\n  ],\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love artificial intelligence.\"\n    ),\n  ],\n]);\n\nconsole.log(responseC);\n```\n\n```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" },\n      }\n    ],\n    [\n      {\n        text: \"J'aime l'intelligence artificielle.\",\n        message: AIChatMessage { text: \"J'aime l'intelligence artificielle.\" }\n      }\n    ]\n  ]\n}\n```\n\n### Chat Prompt Templates: Manage Prompts for Chat Models\n\nYou can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`'s `formatPromptValue` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\n\nContinuing with the previous example:\n\n```typescript\nimport {\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n  ChatPromptTemplate,\n} from \"langchain/prompts\";\n```\n\nFirst we create a reusable template:\n\n```typescript\nconst translationPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n  ),\n  HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n```\n\nThen we can use","metadata":{"source":"docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":1,"to":141}}}}],["4",{"pageContent":" the template to generate a response:\n\n```typescript\nconst responseA = await chat.generatePrompt([\n  await translationPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  }),\n]);\n\nconsole.log(responseA);\n```\n\n```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" }\n      }\n    ]\n  ]\n}\n```\n\n### Model + Prompt = LLMChain\n\nThis pattern of asking for the completion of a formatted prompt is quite common, so we introduce the next piece of the puzzle: LLMChain\n\n```typescript\nconst chain = new LLMChain({\n  prompt: translationPrompt,\n  llm: chat,\n});\n```\n\nThen you can call the chain:\n\n```typescript\nconst responseB = await chain.call({\n  input_language: \"English\",\n  output_language: \"French\",\n  text: \"I love programming.\",\n});\n\nconsole.log(responseB);\n```\n\n```\n{ text: \"J'aime programmer.\" }\n```\n\nThe chain will internally accumulate the messages sent to the model, and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times, and it remembers previous messages:\n\n```typescript\nconst responseD = await chain.call({\n  input: \"hi from London, how are you doing today\",\n});\n```\n\n```\n{\n  response: \"Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"\n}\n```\n\n```typescript\nconst responseE = await chain.call({\n  input: \"Do you know where I am?\",\n});\n\nconsole.log(responseE);\n```\n\n```\n{\n  response: \"Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information.\"\n}\n```\n\n### Agents: Dynamically Run Chains Based on User Input\n\nFinally, we introduce Tools and Agents, which extend the model with other abilities, such as search, or a calculator.\n\nA tool is a function that takes a string (such as a search query) and returns a string (such as a search result). They also have a name and description, which are used by the chat model to identify which tool it should call.\n\n```typescript\nclass Tool {\n  name: string;\n  description: string;\n  call(arg: string): Promise<string>;\n}\n```\n\nAn agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n```\n\nTo make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n```\n\nAnd finally, we can use the AgentExecutor to run an agent:\n\n```typescript\n// Define the list of tools the agent can use\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n// Create the agent from the chat model and the tools\nconst agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n// Create an executor, which calls to the agent until an answer is found\nconst executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\nconst responseG = await executor.run(\n  \"How many people live in canada as of 2023?\"\n);\n\nconsole.log(responseG);\n```\n\n```\n38,626,704.\n```\n\n### Memory: Add State to Chains and Agents\n\nYou can also use the chain to store state. This is useful for eg. chatbots, where you want to keep track of the conversation history. MessagesPlaceholder is a special prompt template that will be replaced with the messages passed","metadata":{"source":"docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":141,"to":306}}}}],["5",{"pageContent":" in each call.\n\n```typescript\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),\n  prompt: chatPrompt,\n  llm: chat,\n});\n```\n\n## Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/getting-started/guide-chat.mdx","loc":{"lines":{"from":306,"to":329}}}}],["6",{"pageContent":"---\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm_streaming_stdout.ts\";\n\n# Quickstart, using LLMs\n\nThis tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.\n\n## Picking up a LLM\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n\nFor this example, we will be using OpenAI's APIs, so no additional setup is required.\n\n## Building a Language Model Application\n\nNow that we have installed LangChain, we can start building our language model application.\n\nLangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.\n\n### LLMs: Get Predictions from a Language Model\n\nThe most basic building block of LangChain is calling an LLM on some input. Let's walk through a simple example of how to do this. For this purpose, let's pretend we are building a service that generates a company name based on what the company makes.\n\nIn order to do this, we first need to import the LLM wrapper.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nWe will then need to set the environment variable for the OpenAI key. Three options here:\n\n1. We can do this by setting the value in a `.env` file and use the [dotenv](https://github.com/motdotla/dotenv) package to read it.\n\n```bash\nOPENAI_API_KEY=\"...\"\n```\n\n2. Or we can export the environment variable with the following command in your shell:\n\n```bash\nexport OPENAI_API_KEY=sk-....\n```\n\n3. Or we can do it when initializing the wrapper along with other arguments. In this example, we probably want the outputs to be MORE random, so we'll initialize it with a HIGH temperature.\n\n```typescript\nconst model = new OpenAI({ openAIApiKey: \"sk-...\", temperature: 0.9 });\n```\n\nOnce we have initialized the wrapper, we can now call it on some input!\n\n```typescript\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log(res);\n```\n\n```shell\n{ res: '\\n\\nFantasy Sockery' }\n```\n\n### Prompt Templates: Manage Prompts for LLMs\n\nCalling an LLM is a great first step, but it's just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.\n\nFor example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.\n\nThis is easy to do with LangChain!\n\nFirst lets define the prompt template:\n\n```typescript\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});\n```\n\nLet's now see how this works! We can call the `.format` method to format it.\n\n```typescript\nconst res = await prompt.format({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: 'What is a good name for a company that makes colorful socks?' }\n```\n\n### Chains: Combine LLMs and Prompts in Multi-Step Workflows\n\nUp until now, we've worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.\n\nA chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\n\nThe most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.\n\nExtending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst model = new OpenAI({ temperature: 0.9 });\nconst template = \"What is a good name for a company that","metadata":{"source":"docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":1,"to":117}}}}],["7",{"pageContent":" makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});\n```\n\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:\n\n```typescript\nimport { LLMChain } from \"langchain/chains\";\n\nconst chain = new LLMChain({ llm: model, prompt: prompt });\n```\n\nNow we can run that chain only specifying the product!\n\n```typescript\nconst res = await chain.call({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: { text: '\\n\\nColorfulCo Sockery.' } }\n```\n\nThere we go! There's the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.\n\n### Agents: Dynamically Run Chains Based on User Input\n\nSo far the chains we've looked at run in a predetermined order.\n\nAgents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this tutorial focuses on the simplest, highest level API, this only covers using the standard supported agents.\n\nFor this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"\n```\n\nInstall `serpapi` package (Google Search API):\n\n```bash npm2yarn\nnpm install -S serpapi\n```\n\nNow we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\n```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```\n\n### Memory: Add State to Chains and Agents\n\nSo far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\".\n\nLangChain provides several specially created chains just for this purpose. This section walks through using one of those chains (the `ConversationChain`).\n\nBy default, the `ConversationChain` has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let's take a look at using this chain.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain }","metadata":{"source":"docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":117,"to":220}}}}],["8",{"pageContent":" from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log(res1);\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log(res2);\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\n## Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/getting-started/guide-llm.mdx","loc":{"lines":{"from":220,"to":247}}}}],["9",{"pageContent":"---\nsidebar_position: 1\n---\n\n# Setup and Installation\n\n:::info\nUpdating from <0.0.52? See [this section](#updating-from-0052) for instructions.\n:::\n\n## Quickstart\n\nIf you want to get started quickly on using LangChain in Node.js, [clone this repository](https://github.com/domeccleston/langchain-ts-starter) and follow the README instructions for a boilerplate project with those dependencies set up.\n\nIf you prefer to set things up yourself, or you want to run LangChain in other environments, read on for instructions.\n\n## Installation\n\nTo get started, install LangChain with the following command:\n\n```bash npm2yarn\nnpm install -S langchain\n```\n\n### TypeScript\n\nLangChain is written in TypeScript and provides type definitions for all of its public APIs.\n\n## Loading the library\n\n### ESM\n\nLangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you are using TypeScript in an ESM project we suggest updating your `tsconfig.json` to include the following:\n\n```json title=\"tsconfig.json\"\n{\n  \"compilerOptions\": {\n    ...\n    \"target\": \"ES2020\", // or higher\n    \"module\": \"nodenext\",\n  }\n}\n```\n\n### CommonJS\n\nLangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nconst { OpenAI } = require(\"langchain/llms/openai\");\n```\n\n### Cloudflare Workers\n\nLangChain can be used in Cloudflare Workers. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n### Vercel / Next.js\n\nLangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you want to use LangChain in frontend `pages`, you need to add the following to your `next.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```js title=\"next.config.js\"\nconst nextConfig = {\n  webpack(config) {\n    config.experiments = {\n      asyncWebAssembly: true,\n      layers: true,\n    };\n\n    return config;\n  },\n};\n```\n\n### Deno / Supabase Edge Functions\n\nLangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"https://esm.sh/langchain/llms/openai\";\n```\n\nWe recommend looking at our [Supabase Template](https://github.com/langchain-ai/langchain-template-supabase) for an example of how to use LangChain in Supabase Edge Functions.\n\n### Browser\n\nLangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n#### Create React App\n\nIf you're using `create-react-app` by default it doesn't support WebAssembly modules, so the tokenizer library `@dqbd/tiktoken` will not work in the browser. You can follow the instructions [here](https://github.com/dqbd/tiktoken/tree/main/js#create-react-app) to enable support for WebAssembly modules.\n\n#### Vite\n\nIf you're using Vite, you need to add the following to your `vite.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```bash npm2yarn\nnpm install -D vite-plugin-wasm vite-plugin-top-level-await\n```\n\n```js title=\"vite.config.js\"\nimport wasm from \"vite-plugin-wasm\";\nimport topLevelAwait from \"vite-plugin-top-level-await\";\nimport { defineConfig } from \"vite\";\n\nexport default defineConfig({\n  plugins: [wasm(), topLevelAwait()],\n});\n```\n\n## Updating from <0.0.52\n\nIf you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.\n\nFor example, if you were previously doing\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n```\n\nyou will now need to do\n\n```typescript\nimport {","metadata":{"source":"docs/docs/getting-started/install.md","loc":{"lines":{"from":1,"to":143}}}}],["10",{"pageContent":" OpenAI } from \"langchain/llms/openai\";\n```\n\nThis applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.\n\n- If you were using `langchain/llms`, see [LLMs](../modules/models/llms/integrations) for updated import paths.\n- If you were using `langchain/chat_models`, see [Chat Models](../modules/models/chat/integrations) for updated import paths.\n- If you were using `langchain/embeddings`, see [Embeddings](../modules/models/embeddings/integrations) for updated import paths.\n- If you were using `langchain/vectorstores`, see [Vector Stores](../modules/indexes/vector_stores/integrations/) for updated import paths.\n- If you were using `langchain/document_loaders`, see [Document Loaders](../modules/indexes/document_loaders/examples/) for updated import paths.\n- If you were using `langchain/retrievers`, see [Retrievers](../modules/indexes/retrievers/) for updated import paths.\n\nOther modules are not affected by this change, and you can continue to import them from the same path.\n\nAdditionally, there are some breaking changes that were needed to support new environments:\n\n- `import { Calculator } from \"langchain/tools\";` now moved to\n  - `import { Calculator } from \"langchain/tools/calculator\";`\n- `import { loadLLM } from \"langchain/llms\";` now moved to\n  - `import { loadLLM } from \"langchain/llms/load\";`\n- `import { loadAgent } from \"langchain/agents\";` now moved to\n  - `import { loadAgent } from \"langchain/agents/load\";`\n- `import { loadPrompt } from \"langchain/prompts\";` now moved to\n  - `import { loadPrompt } from \"langchain/prompts/load\";`\n- `import { loadChain } from \"langchain/chains\";` now moved to\n  - `import { loadChain } from \"langchain/chains/load\";`\n\n## Unsupported: Node.js 16\n\nWe do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.\n\nYou will have to make `fetch` available globally, either:\n\n- run your application with `NODE_OPTIONS='--experimental-fetch' node ...`, or\n- install `node-fetch` and follow the instructions [here](https://github.com/node-fetch/node-fetch#providing-global-access)\n\nAdditionally you'll have to polyfill `unstructuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).\n\nIf you are running this on Node.js 18 or 19, you do not need to do anything.\n","metadata":{"source":"docs/docs/getting-started/install.md","loc":{"lines":{"from":143,"to":182}}}}],["11",{"pageContent":"# Welcome to LangChain\n\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\n\n- _Be data-aware_: connect a language model to other sources of data\n- _Be agentic_: allow a language model to interact with its environment\n\nThe LangChain framework is designed with the above principles in mind.\n\n## Getting Started\n\nCheckout the guide below for a walkthrough of how to get started using LangChain to create a Language Model application.\n\n- [Quickstart, using LLMs](./getting-started/guide-llm.mdx)\n- [Quickstart, using Chat Models](./getting-started/guide-chat.mdx)\n\n## Components\n\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started and get familiar with some of the concepts. Each example links to API documentation for the modules used.\n\nThese modules are, in increasing order of complexity:\n\n- [Schema](./modules/schema/): This includes interfaces and base classes used throughout the library.\n\n- [Models](./modules/models/): This includes integrations with a variety of LLMs, Chat Models and Embeddings models.\n\n- [Prompts](./modules/prompts/): This includes prompt Templates and functionality to work with prompts like Output Parsers and Example Selectors\n\n- [Indexes](./modules/indexes/): This includes patterns and functionality for working with your own data, and making it ready to interact with language models (including document loaders, vectorstores, text splitters and retrievers).\n\n- [Memory](./modules/memory/): Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n\n- [Chains](./modules/chains/): Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n\n- [Agents](./modules/agents/): Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n\n## API Reference\n\n[Here](./api/) you can find the API reference for all of the modules in LangChain, as well as full documentation for all exported classes and functions.\n\n## Production\n\nAs you move from prototyping into production, we're developing resources to help you do so.\nThese including:\n\n- [Deployment](./production/deployment): resources on how to deploy your application to production.\n- [Events/Callbacks](./production/callbacks): resources on the events exposed by LangChain modules.\n- [Tracing](./production/tracing): resouces on how to use tracing to log and debug your application.\n\n## Additional Resources\n\nAdditional collection of resources we think may be useful as you develop your application!\n\n- [LangChainHub](https://github.com/hwchase17/langchain-hub): The LangChainHub is a place to share and explore other prompts, chains, and agents.\n\n- [Discord](https://discord.gg/6adMQxSpJS): Join us on our Discord to discuss all things LangChain!\n\n- [Production Support](https://forms.gle/57d8AmXBYp8PP8tZA): As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.\n","metadata":{"source":"docs/docs/index.md","loc":{"lines":{"from":1,"to":59}}}}],["12",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent.ts\";\n\n# Custom LLM Agent\n\nThis example covers how to create a custom Agent powered by an LLM.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/agents/custom_llm.mdx","loc":{"lines":{"from":1,"to":14}}}}],["13",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent_chat.ts\";\n\n# Custom LLM Agent (with Chat Model)\n\nThis example covers how to create a custom Agent powered by a Chat Model.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/agents/custom_llm_chat.mdx","loc":{"lines":{"from":1,"to":14}}}}],["14",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_mrkl.ts\";\n\n# MRKL Agent for Chat Models\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with Chat Models. If you want to use it with an LLM, you can use the [LLM MRKL Agent](./llm_mrkl) instead.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/agents/examples/chat_mrkl.mdx","loc":{"lines":{"from":1,"to":14}}}}],["15",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_convo_with_tracing.ts\";\n\n# Conversational Agent\n\nThis example covers how to create a conversational agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n````\nLoaded agent.\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Hello Bob! How can I assist you today?\"\n}\nFinished chain.\nGot output Hello Bob! How can I assist you today?\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Your name is Bob.\"\n}\nFinished chain.\nGot output Your name is Bob.\nEntering new agent_executor chain...\n```json\n{\n    \"action\": \"search\",\n    \"action_input\": \"weather in pomfret\"\n}\n```\nA steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n```json\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\"\n}\n```\nFinished chain.\nGot output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n````\n","metadata":{"source":"docs/docs/modules/agents/agents/examples/conversational_agent.mdx","loc":{"lines":{"from":1,"to":48}}}}],["16",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/agent.ts\";\n\n# Agent with Custom Prompt, using Chat Models\n\nThis example covers how to create a custom agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/agents/examples/custom_agent_chat.mdx","loc":{"lines":{"from":1,"to":13}}}}],["17",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Agents\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/agents/examples/index.mdx","loc":{"lines":{"from":1,"to":11}}}}],["18",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/mrkl.ts\";\n\n# MRKL Agent for LLMs\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with LLMs. If you want to use it with a chat model, try the [Chat MRKL Agent](./chat_mrkl).\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/agents/examples/llm_mrkl.mdx","loc":{"lines":{"from":1,"to":14}}}}],["19",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent)\n:::\n\nAn agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model. It takes in user input and returns a response corresponding to an “action” to take and a corresponding “action input”.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n```\n\n## Which agent to choose?\n\nThe agent you choose depends on the type of task you want to perform. Here's a quick guide to help you pick the right agent for your use case:\n\n- If you're using a text LLM, first try `zero-shot-react-description`, aka. the [MRKL agent for LLMs](./examples/llm_mrkl).\n- If you're using a Chat Model, try `chat-zero-shot-react-description`, aka. the [MRKL agent for Chat Models](./examples/chat_mrkl).\n- If you're using a Chat Model and want to use memory, try `chat-conversational-react-description`, the [Conversational agent](./examples/conversational_agent).\n\n## All Agents\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/agents/index.mdx","loc":{"lines":{"from":1,"to":46}}}}],["20",{"pageContent":"---\nsidebar_label: Getting Started\nhide_table_of_contents: true\n---\n\n# Getting Started: Agent Executors\n\nAgents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents.\n\nFor this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"\n```\n\nNow we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\n```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```\n","metadata":{"source":"docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":1,"to":61}}}}],["21",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Agent Executors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent-executor)\n:::\n\nTo make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n```\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/executor/index.mdx","loc":{"lines":{"from":1,"to":32}}}}],["22",{"pageContent":"---\nsidebar_position: 7\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents)\n:::\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/index.mdx","loc":{"lines":{"from":1,"to":15}}}}],["23",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\n# Examples: Toolkits\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/toolkits/examples/index.mdx","loc":{"lines":{"from":1,"to":11}}}}],["24",{"pageContent":"# JSON Agent Toolkit\n\nThis example shows how to load and use an agent with a JSON toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```\n","metadata":{"source":"docs/docs/modules/agents/toolkits/examples/json.md","loc":{"lines":{"from":1,"to":46}}}}],["25",{"pageContent":"# OpenAPI Agent Toolkit\n\nThis example shows how to load and use an agent with a OpenAPI toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);\n\n  const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```\n","metadata":{"source":"docs/docs/modules/agents/toolkits/examples/openapi.md","loc":{"lines":{"from":1,"to":48}}}}],["26",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# SQL Agent Toolkit\n\nThis example shows how to load and use an agent with a SQL toolkit.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/sql.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/toolkits/examples/sql.mdx","loc":{"lines":{"from":1,"to":13}}}}],["27",{"pageContent":"# VectorStore Agent Toolkit\n\nThis example shows how to load and use an agent with a vectorstore toolkit.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n  /* Create the agent */\n  const vectorStoreInfo: VectorStoreInfo = {\n    name: \"state_of_union_address\",\n    description: \"the most recent state of the Union address\",\n    vectorStore,\n  };\n\n  const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\n  const agent = createVectorStoreAgent(model, toolkit);\n\n  const input =\n    \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\n  console.log(`Executing: ${input}`);\n  const result = await agent.call({ input });\n  console.log(`Got output ${result.output}`);\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```\n","metadata":{"source":"docs/docs/modules/agents/toolkits/examples/vectorstore.md","loc":{"lines":{"from":1,"to":51}}}}],["28",{"pageContent":"---\nsidebar_label: Toolkits\nsidebar_position: 2\nhide_table_of_contents: true\n---\n\n# Getting Started: Toolkits\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/toolkit)\n:::\n\nGroups of [tools](../tools/) that can be used/are necessary to solve a particular problem.\n\n```typescript\ninterface Toolkit {\n  tools: Tool[];\n}\n```\n\n## All Toolkits\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/agents/toolkits/index.mdx","loc":{"lines":{"from":1,"to":26}}}}],["29",{"pageContent":"# Agents with Vector Stores\n\nThis notebook covers how to combine agents and vector stores. The use case for this is that you’ve ingested your data into a vector store and want to interact with it in an agentic manner.\n\nThe recommended method for doing so is to create a VectorDBQAChain and then use that as a tool in the overall agent. Let’s take a look at doing this below. You can do this with multiple different vector databases, and use the agent as a way to choose between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `returnDirect: true` to just use the agent as a router.\n\nFirst, you'll want to import the relevant modules:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI, ChainTool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n```\n\nNext, you'll want to create the vector store with your data, and then the QA chain to interact with that vector store.\n\n```typescript\nconst model = new OpenAI({ temperature: 0 });\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n/* Create the chain */\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);\n```\n\nNow that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n});\n```\n\nNow you can construct and using the tool just as you would any other!\n\n```typescript\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n  qaTool,\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What did biden say about ketanji brown jackson is the state of the union address?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\nYou can also set `returnDirect: true` if you intend to use the agent as a router and just want to directly return the result of the VectorDBQAChain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n  returnDirect: true,\n});\n```\n","metadata":{"source":"docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":1,"to":85}}}}],["30",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/aiplugin-tool.ts\";\n\n# ChatGPT Plugins\n\nThis example shows how to use ChatGPT Plugins within LangChain abstractions.\n\nNote 1: This currently only works for plugins with no auth.\n\nNote 2: There are almost certainly other ways to do this, this is just a first pass. If you have better ideas, please open a PR!\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n````\nEntering new agent_executor chain...\nThought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.\nAction:\n```\n\n{\n\"action\": \"KlarnaProducts\",\n\"action_input\": \"\"\n}\n\n```\n\nUsage Guide: Use the Klarna plugin to get relevant product suggestions for any shopping or researching purpose. The query to be sent should not include stopwords like articles, prepositions and determinants. The api works best when searching for words that are related to products, like their name, brand, model or category. Links will always be returned and should be shown to the user.\n\nOpenAPI Spec: {\"openapi\":\"3.0.1\",\"info\":{\"version\":\"v0\",\"title\":\"Open AI Klarna product Api\"},\"servers\":[{\"url\":\"https://www.klarna.com/us/shopping\"}],\"tags\":[{\"name\":\"open-ai-product-endpoint\",\"description\":\"Open AI Product Endpoint. Query for products.\"}],\"paths\":{\"/public/openai/v0/products\":{\"get\":{\"tags\":[\"open-ai-product-endpoint\"],\"summary\":\"API for fetching Klarna product information\",\"operationId\":\"productsUsingGET\",\"parameters\":[{\"name\":\"q\",\"in\":\"query\",\"description\":\"query, must be between 2 and 100 characters\",\"required\":true,\"schema\":{\"type\":\"string\"}},{\"name\":\"size\",\"in\":\"query\",\"description\":\"number of products returned\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"budget\",\"in\":\"query\",\"description\":\"maximum price of the matching product in local currency, filters results\",\"required\":false,\"schema\":{\"type\":\"integer\"}}],\"responses\":{\"200\":{\"description\":\"Products found\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ProductResponse\"}}}},\"503\":{\"description\":\"one or more services are unavailable\"}},\"deprecated\":false}}},\"components\":{\"schemas\":{\"Product\":{\"type\":\"object\",\"properties\":{\"attributes\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"string\"},\"url\":{\"type\":\"string\"}},\"title\":\"Product\"},\"ProductResponse\":{\"type\":\"object\",\"properties\":{\"products\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/components/schemas/Product\"}}},\"title\":\"ProductResponse\"}}}}\nNow that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter \"t-shirt\".\nAction:\n```\n\n{\n\"action\": \"requests_get\",\n\"action_input\": \"https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt\"\n}\n\n```\n\n\n{\"products\":[{\"name\":\"Psycho Bunny Mens Copa Gradient Logo Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai\",\"price\":\"$35.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Blue,Black,Orange\"]},{\"name\":\"T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai\",\"price\":\"$20.45\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,White,Blue,Black,Orange\"]},{\"name\":\"Palm Angels Bear T-shirt - Black\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai\",\"price\":\"$168.36\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Black\"]},{\"name\":\"Tommy Hilfiger Essential Flag Logo T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai\",\"price\":\"$22.52\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Red,Gray,White,Blue,Black\",\"Pattern:Solid Color\",\"Environmental Attributes :Organic\"]},{\"name\":\"Coach Outlet Signature T Shirt\",\"","metadata":{"source":"docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":1,"to":46}}}}],["31",{"pageContent":"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai\",\"price\":\"$75.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray\"]}]}\nFinished chain.\n{\n  result: {\n    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',\n    intermediateSteps: [ [Object], [Object] ]\n  }\n}\n````\n","metadata":{"source":"docs/docs/modules/agents/tools/aiplugin-tool.mdx","loc":{"lines":{"from":46,"to":55}}}}],["32",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Tools\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/tool)\n:::\n\nA tool is an abstraction around a function that makes it easy for a language model to interact with it. Specifically, the interface of a tool has a single text input and a single text output. It includes a name and description that communicate to the [Model](../../models/) what the tool does and when to use it.\n\n```typescript\ninterface Tool {\n  call(arg: string): Promise<string>;\n\n  name: string;\n\n  description: string;\n}\n```\n\n## All Tools\n\n<DocCardList />\n\n## Advanced\n\nTo implement a custom tool you can subclass the `Tool` class and implement the `_call` method. The `_call` method is called with the input text and should return the output text. The Tool superclass implements the `call` method, which takes care of calling the right CallbackManager methods before and after calling your `_call` method. When an error occurs, the `_call` method should when possible return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\n```typescript\nabstract class Tool {\n  abstract _call(arg: string): Promise<string>;\n\n  abstract name: string;\n\n  abstract description: string;\n}\n```\n\nAnother option is to create a tool on the fly using a `DynamicTool`. This is useful if you don't need the overhead of subclassing `Tool`.\nThe `DynamicTool` class takes as input a name, a description, and a function. Importantly, the name and the description will be used by the language model to determine when to call this function and with what parameters! So make sure to set these to some values the language model can reason about. The function provided is what will actually be called. When an error occurs, the function should, when possible, return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\nSee below for an example of defining and using `DynamicTool`s.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { DynamicTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new DynamicTool({\n      name: \"FOO\",\n      description:\n        \"call this to get the value of foo. input should be an empty string.\",\n      func: () => \"baz\",\n    }),\n    new DynamicTool({\n      name: \"BAR\",\n      description:\n        \"call this to get the value of bar. input should be an empty string.\",\n      func: () => \"baz1\",\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the value of foo?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```\n","metadata":{"source":"docs/docs/modules/agents/tools/index.mdx","loc":{"lines":{"from":1,"to":86}}}}],["33",{"pageContent":"---\nsidebar_label: Integrations\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Integrations: Tools\n\nLangChain provides the following tools you can use out of the box:\n\n- `AWSLambda` - A wrapper around the AWS Lambda API, invoked via the Amazon Web Services Node.js SDK. Useful for invoking serverless functions with any behavior which you need to provide to an Agent.\n- `BingSerpAPI` - A wrapper around the Bing Search API. Useful for when you need to answer questions about current events. Input should be a search query.\n- `Calculator` - Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n- `IFTTTWebHook` - A wrapper around the IFTTT Webhook API. Useful for triggering IFTTT actions.\n- `JsonListKeys` and `JsonGetValue` - Useful for extracting data from JSON objects. These tools can be used collectively in a `JsonToolkit`.\n- `RequestsGet` and `RequestsPost` - Useful for making HTTP requests.\n- `SerpAPI` - A wrapper around the SerpAPI API. Useful for when you need to answer questions about current events. Input should be a search query.\n- `QuerySqlTool`, `InfoSqlTool`, `ListTablesSqlTool`, and `SqlCheckerTool` - Useful for interacting with SQL databases. Can be used together in a `SqlToolkit`.\n- `VectorStoreQATool` - Useful for retrieving relevant text data from a vector store.\n- `ZapierNLARunAction` - A wrapper around the Zapier NLP API. Useful for triggering Zapier actions with a natural language input. Best when used in a `ZapierToolkit`.\n","metadata":{"source":"docs/docs/modules/agents/tools/integrations/index.mdx","loc":{"lines":{"from":1,"to":22}}}}],["34",{"pageContent":"---\nsidebar_label: Agent with AWS Lambda\nhide_table_of_contents: true\n---\n\n# Agent with AWS Lambda Integration\n\nFull docs here: https://docs.aws.amazon.com/lambda/index.html\n\n**AWS Lambda** is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\n\nBy including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.\n\nWhen an Agent uses the AWSLambda tool, it will provide an argument of type `string` which will in turn be passed into the Lambda function via the `event` parameter.\n\nThis quick start will demonstrate how an Agent could use a Lambda function to send an email via [Amazon Simple Email Service](https://aws.amazon.com/ses/). The lambda code which sends the email is not provided, but if you'd like to learn how this could be done, see [here](https://repost.aws/knowledge-center/lambda-send-email-ses). Keep in mind this is an intentionally simple example; Lambda can used to execute code for a near infinite number of other purposes (including executing more Langchains)!\n\n### Note about credentials:\n\n- If you have not run [`aws configure`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) via the AWS CLI, the `region`, `accessKeyId`, and `secretAccessKey` must be provided to the AWSLambda constructor.\n- The IAM role corresponding to those credentials must have permission to invoke the lambda function.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { AWSLambda } from \"langchain/tools/aws_lambda\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst emailSenderTool = new AWSLambda({\n  name: \"email-sender\",\n  // tell the Agent precisely what the tool does\n  description:\n    \"Sends an email with the specified content to testing123@gmail.com\",\n  region: \"us-east-1\", // optional: AWS region in which the function is deployed\n  accessKeyId: \"abc123\", // optional: access key id for a IAM user with invoke permissions\n  secretAccessKey: \"xyz456\", // optional: secret access key for that IAM user\n  functionName: \"SendEmailViaSES\", // the function name as seen in AWS Console\n});\nconst tools = [emailSenderTool, new SerpAPI(\"api_key_goes_here\")];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\nconst input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;\nconst result = await executor.call({ input });\nconsole.log(result);\n```\n","metadata":{"source":"docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":1,"to":49}}}}],["35",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Web Browser Tool\n\nThe Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as\n\n```\nuseful for when you need to find something on or summarize a webpage. input should be a comma seperated list of \"valid URL including protocol\",\"what you want to find on the page or empty string for a summary\".\n```\n\nIt exposes two modes of operation:\n\n- when called by the Agent with only a URL it produces a summary of the website contents\n- when called by the Agent with a URL and a description of what to find it will instead use an in-memory Vector Store to find the most relevant snippets and summarise those\n\n## Setup\n\nTo use the Webbrowser Tool you need to install the dependencies:\n\n```bash npm2yarn\nnpm install cheerio axios@^0.26.0\n```\n\n## Usage, standalone\n\nimport ToolExample from \"@examples/tools/webbrowser.ts\";\n\n<CodeBlock language=\"typescript\">{ToolExample}</CodeBlock>\n\n## Usage, in an Agent\n\nimport AgentExample from \"@examples/agents/mrkl_browser.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/agents/tools/webbrowser.mdx","loc":{"lines":{"from":1,"to":39}}}}],["36",{"pageContent":"# Agent with Zapier NLA Integration\n\nFull docs here: https://nla.zapier.com/api/v1/dynamic/docs\n\n**Zapier Natural Language Actions** gives you access to the 5k+ apps and 20k+ actions on Zapier's platform through a natural language API interface.\n\nNLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps\n\nZapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.\n\nNLA offers both API Key and OAuth for signing NLA API requests.\n\nServer-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)\n\nUser-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com\n\nThis quick start will focus on the server-side use case for brevity. Review full docs or reach out to nla@zapier.com for user-facing oauth developer support.\n\nThe example below demonstrates how to use the Zapier integration as an Agent:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport {\n  initializeAgentExecutorWithOptions,\n  ZapierToolKit,\n} from \"langchain/agents\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const zapier = new ZapierNLAWrapper();\n  const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\n  const executor = await initializeAgentExecutorWithOptions(\n    toolkit.tools,\n    model,\n    {\n      agentType: \"zero-shot-react-description\",\n      verbose: true,\n    }\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```\n","metadata":{"source":"docs/docs/modules/agents/tools/zapier_agent.md","loc":{"lines":{"from":1,"to":53}}}}],["37",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chains\nsidebar_position: 6\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains)\n:::info\n\nUsing a language model in isolation is fine for some applications, but it is often useful to combine language models with other sources of information, third-party APIs, or even other language models. This is where the concept of a chain comes in.\n\nLangChain provides a standard interface for chains, as well as a number of built-in chains that can be used out of the box. You can also create your own chains.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/chains/index.mdx","loc":{"lines":{"from":1,"to":20}}}}],["38",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport ConvoRetrievalQAExample from \"@examples/chains/conversational_qa.ts\";\n\n# `ConversationalRetrievalQAChain`\n\nThe `ConversationalRetrievalQA` chain builds on `RetrievalQAChain` to provide a chat history component.\n\nIt requires two inputs: a question and the chat history. It first combines the chat history and the question into a standalone question, then looks up relevant documents from the retriever, and then passes those documents and the question to a question answering chain to return a response.\n\nTo create one, you will need a retriever. In the below example, we will create one from a vectorstore, which can be created from embeddings.\nimport Example from \"@examples/chains/conversational_qa.ts\";\n\n<CodeBlock language=\"typescript\">{ConvoRetrievalQAExample}</CodeBlock>\n\nIn this code snippet, the fromLLM method of the `ConversationalRetrievalQAChain` class has the following signature:\n\n```typescript\nstatic fromLLM(\n  llm: BaseLanguageModel,\n  retriever: BaseRetriever,\n  options?: {\n    questionGeneratorTemplate?: string;\n    qaTemplate?: string;\n    returnSourceDocuments?: boolean;\n  }\n): ChatVectorDBQAChain\n```\n\nHere's an explanation of each of the attributes of the options object:\n\n- `questionGeneratorTemplate`: A string that specifies a question generation template. If provided, the `ConversationalRetrievalQAChain` will use this template to generate a question from the conversation context, instead of using the question provided in the question parameter. This can be useful if the original question does not contain enough information to retrieve a suitable answer.\n- `qaTemplate`: A string that specifies a response template. If provided, the `ConversationalRetrievalQAChain` will use this template to format a response before returning the result. This can be useful if you want to customize the way the response is presented to the end user.\n- `returnSourceDocuments`: A boolean value that indicates whether the `ConversationalRetrievalQAChain` should return the source documents that were used to retrieve the answer. If set to true, the documents will be included in the result returned by the call() method. This can be useful if you want to allow the user to see the sources used to generate the answer. If not set, the default value will be false.\n\nIn summary, the `questionGeneratorTemplate`, `qaTemplate`, and `returnSourceDocuments` options allow the user to customize the behavior of the `ConversationalRetrievalQAChain`\n","metadata":{"source":"docs/docs/modules/chains/index_related_chains/conversational_retrieval.mdx","loc":{"lines":{"from":1,"to":36}}}}],["39",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport QAExample from \"@examples/chains/question_answering.ts\";\nimport RefineExample from \"@examples/chains/qa_refine.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Document QA Chains\n\nLangChain provides chains used for processing unstructured text data: `StuffDocumentsChain`, `MapReduceDocumentsChain` and `RefineDocumentsChain`.\nThese chains are the building blocks more complex chains for processing unstructured text data and receive both documents and a question as input. They then utilize the language model to provide an answer to the question based on the given documents.\n\n- `StuffDocumentsChain`: This chain is the simplest of the 3 chains and simply injects all documents passes in into the prompt. It then returns the answer to the question, using all documents as context. It is suitable for QA tasks over a small number of documents.\n- `MapReduceDocumentsChain`: This chain adds a preprocessing step to select relevant portions of each document until the total number of tokens is less than the maximum number of tokens allowed by the model. It then uses the transformed docs as context to answer the question. It is suitable for QA tasks over larger documents, and it runs the preprocessing step in parallel, which can reduce the running time.\n- `RefineDocumentsChain`: This chain iterates over the documents one by one to update a running answer, at each turn using the previous version of the answer and the next doc as context. It is suitable for QA tasks over a large number of documents.\n\n## Usage, `StuffDocumentsChain` and `MapReduceDocumentsChain`\n\n<CodeBlock language=\"typescript\">{QAExample}</CodeBlock>\n\n## Usage, `RefineDocumentsChain`\n\n<CodeBlock language=\"typescript\">{RefineExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/index_related_chains/document_qa.mdx","loc":{"lines":{"from":1,"to":25}}}}],["40",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Index Related Chains\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Index Related Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/index_related_chains)\n:::\n\nChains related to working with unstructured data stored in indexes.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/chains/index_related_chains/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["41",{"pageContent":"import RetrievalQAExample from \"@examples/chains/retrieval_qa.ts\";\nimport RetrievalQAExampleCustom from \"@examples/chains/retrieval_qa_custom.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# `RetrievalQAChain`\n\nThe `RetrievalQAChain` is a chain that combines a `Retriever` and a QA chain (described above). It is used to retrieve documents from a `Retriever` and then use a `QA` chain to answer a question based on the retrieved documents.\n\n## Usage\n\nIn the below example, we are using a `VectorStore` as the `Retriever`. By default, the `StuffDocumentsChain` is used as the `QA` chain.\n\n<CodeBlock language=\"typescript\">{RetrievalQAExample}</CodeBlock>\n\n## Usage, with a custom `QA` chain\n\nIn the below example, we are using a `VectorStore` as the `Retriever` and a `RefineDocumentsChain` as the `QA` chain.\n\n<CodeBlock language=\"typescript\">{RetrievalQAExampleCustom}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/index_related_chains/retrieval_qa.mdx","loc":{"lines":{"from":1,"to":20}}}}],["42",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLM Chain\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/llm_chain.ts\";\n\n# Getting Started: LLMChain\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/llm-chain)\n:::\n\nAn `LLMChain` is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n\nAn `LLMChain` consists of a `PromptTemplate` and a language model (either and LLM or chat model).\n\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/llmchain/index.mdx","loc":{"lines":{"from":1,"to":22}}}}],["43",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport AnalyzeDocumentExample from \"@examples/chains/analyze_document_chain_summarize.ts\";\n\n# `AnalyzeDocumentChain`\n\nYou can use the `AnalyzeDocumentChain`, which accepts a single piece of text as input and operates over it.\nThis chain takes care of splitting up the text and then passing it to the `MapReduceDocumentsChain` to generate a summary.\n\n<CodeBlock language=\"typescript\">{AnalyzeDocumentExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/other_chains/analyze_document.mdx","loc":{"lines":{"from":1,"to":10}}}}],["44",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Other Chains\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Other Chains\n\nThis section highlights other examples of chains that exist.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/chains/other_chains/index.mdx","loc":{"lines":{"from":1,"to":13}}}}],["45",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SqlDBExample from \"@examples/chains/sql_db.ts\";\n\n# `SqlDatabaseChain`\n\nThe `SqlDatabaseChain` allows you to answer questions over a SQL database.\nThis example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n\n## Set up\n\nFirst install `typeorm`:\n\n```bash npm2yarn\nnpm install typeorm\n```\n\nThen install the dependencies needed for your database. For example, for SQLite:\n\n```bash npm2yarn\nnpm install sqlite3\n```\n\nFor other databases see https://typeorm.io/#installation\n\nFinally follow the instructions on https://database.guide/2-sample-databases-sqlite/ to get the sample database for this example.\n\n<CodeBlock language=\"typescript\">{SqlDBExample}</CodeBlock>\n\nYou can include or exclude tables when creating the `SqlDatabase` object to help the chain focus on the tables you want.\nIt can also reduce the number of tokens used in the chain.\n\n```typescript\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n  includeTables: [\"Track\"],\n});\n```\n","metadata":{"source":"docs/docs/modules/chains/other_chains/sql.mdx","loc":{"lines":{"from":1,"to":38}}}}],["46",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SummarizeExample from \"@examples/chains/summarization_map_reduce.ts\";\n\n# Summarization\n\nA summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a `MapReduceDocumentsChain`.\n\n<CodeBlock language=\"typescript\">{SummarizeExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/other_chains/summarization.mdx","loc":{"lines":{"from":1,"to":9}}}}],["47",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Selectors\n---\n\n# Prompt Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/prompt-selector)\n:::\n\nOftentimes, you will want to programmatically select a prompt based on the type of model you are using in a chain. This is especially relevant when swapping chat models and LLMs.\n\nThe interface for prompt selectors is quite simple:\n\n```typescript\nabstract class BasePromptSelector {\n  abstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;\n}\n```\n\nThe `getPrompt` method takes in a language model and returns an appropriate prompt template.\n\nWe currently offer a `ConditionalPromptSelector` that allows you to specify a set of conditions and prompt templates. The first condition that evaluates to true will be used to select the prompt template.\n\n```typescript\nconst QA_PROMPT_SELECTOR = new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [\n  [isChatModel, CHAT_PROMPT],\n]);\n```\n\nThis will return `DEFAULT_QA_PROMPT` if the model is not a chat model, and `CHAT_PROMPT` if it is.\n\nThe example below shows how to use a prompt selector when loading a chain:\n\n```typescript\nconst loadQAStuffChain = (\n  llm: BaseLanguageModel,\n  params: StuffQAChainParams = {}\n) => {\n  const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;\n  const llmChain = new LLMChain({ prompt, llm });\n  const chain = new StuffDocumentsChain({ llmChain });\n  return chain;\n};\n```\n","metadata":{"source":"docs/docs/modules/chains/prompt_selectors/index.mdx","loc":{"lines":{"from":1,"to":47}}}}],["48",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: SimpleSequentialChain\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/simple_sequential_chain.ts\";\n\n# Getting Started: SimpleSequentialChain\n\nAn `SimpleSequentialChain` is a chain that allows you to join multiple single-input/single-output chains into one chain.\n\nThe example below shows a sample usecase. In the first step, given a title, a synopsis of a play is generated. In the second step, based on the generated synopsis, a review of the play is generated.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/chains/simple_sequential_chain/index.mdx","loc":{"lines":{"from":1,"to":16}}}}],["49",{"pageContent":"# CSV files\n\nThis example goes over how to load data from CSV files. The second argument is the `column` name to extract from the CSV file. One document will be created for each row in the CSV file. When `column` is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's `pageContent`. When `column` is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.\n\n## Setup\n\n```bash npm2yarn\nnpm install d3-dsv@2\n```\n\n## Usage, extracting all columns\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\"src/document_loaders/example_data/example.csv\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 1\ntext: This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 2\ntext: This is another sentence.\",\n  },\n]\n*/\n```\n\n## Usage, extracting a single column\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\n  \"src/document_loaders/example_data/example.csv\",\n  \"text\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":1,"to":91}}}}],["50",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\n# Folders with multiple files\n\nThis example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.\n\nExample folder:\n\n```text\nsrc/document_loaders/example_data/example/\n├── example.json\n├── example.jsonl\n├── example.txt\n└── example.csv\n```\n\nExample code:\n\n```typescript\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\n  \"src/document_loaders/example_data/example\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/directory.md","loc":{"lines":{"from":1,"to":43}}}}],["51",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Docx files\n\nThis example goes over how to load data from docx files.\n\n# Setup\n\n```bash npm2yarn\nnpm install mammoth\n```\n\n# Usage\n\n```typescript\nimport { DocxLoader } from \"langchain/document_loaders/fs/docx\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.docx\"\n);\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/docx.md","loc":{"lines":{"from":1,"to":26}}}}],["52",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# EPUB files\n\nThis example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.\n\n# Setup\n\n```bash npm2yarn\nnpm install epub2 html-to-text\n```\n\n# Usage, one document per chapter\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\"src/document_loaders/example_data/example.epub\");\n\nconst docs = await loader.load();\n```\n\n# Usage, one document per file\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\n  \"src/document_loaders/example_data/example.epub\",\n  {\n    splitChapters: false,\n  }\n);\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/epub.md","loc":{"lines":{"from":1,"to":39}}}}],["53",{"pageContent":"---\nlabel: \"File Loaders\"\nhide_table_of_contents: true\nsidebar_class_name: node-only-category\n---\n\n# File Loaders\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThese loaders are used to load files given a filesystem path or a Blob object.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/index.mdx","loc":{"lines":{"from":1,"to":18}}}}],["54",{"pageContent":"# JSON files\n\nThe JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.\n\n### No JSON pointer example\n\nThe most simple way of using it, is to specify no JSON pointer.\nThe loader will load all strings it finds in the JSON object.\n\nExample JSON file:\n\n```json\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n### Using JSON pointer example\n\nYou can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.\n\nIn this example, we want to only extract information from \"from\" and \"surname\" entries.\n\n```json\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"BD 2023 SUMMER\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"LinkedIn Job\",\n  },\n  ...\n]\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":1,"to":105}}}}],["55",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# JSONLines files\n\nThis example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.\n\nExample JSONLines file:\n\n```json\n{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}\n```\n\nExample code:\n\n```typescript\nimport { JSONLinesLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLinesLoader(\n  \"src/document_loaders/example_data/example.jsonl\",\n  \"/html\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/jsonlines.md","loc":{"lines":{"from":1,"to":48}}}}],["56",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Notion markdown export\n\nThis example goes over how to load data from your Notion pages exported from the notion dashboard.\n\nFirst, export your notion pages as **Markdown & CSV** as per the offical explanation [here](https://www.notion.so/help/export-your-content). Make sure to select `include subpages` and `Create folders for subpages.`\n\nThen, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.\n\nOnce the folder is in your repository, simply run the example below:\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/notion_markdown.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/notion_markdown.mdx","loc":{"lines":{"from":1,"to":19}}}}],["57",{"pageContent":"# PDF files\n\nThis example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the `splitPages` option to `false`.\n\n## Setup\n\n```bash npm2yarn\nnpm install pdf-parse\n```\n\n## Usage, one document per page\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\");\n\nconst docs = await loader.load();\n```\n\n## Usage, one document per file\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  splitPages: false,\n});\n\nconst docs = await loader.load();\n```\n\n## Usage, custom `pdfjs` build\n\nBy default we use the `pdfjs` build bundled with `pdf-parse`, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of `pdfjs-dist` or if you want to use a custom build of `pdfjs-dist`, you can do so by providing a custom `pdfjs` function that returns a promise that resolves to the `PDFJS` object.\n\nIn the following example we use the \"legacy\" (see [pdfjs docs](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions#which-browsersenvironments-are-supported)) build of `pdfjs-dist`, which includes several polyfills not included in the default build.\n\n```bash npm2yarn\nnpm install pdfjs-dist\n```\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":1,"to":51}}}}],["58",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Subtitles\n\nThis example goes over how to load data from subtitle files. One document will be created for each subtitles file.\n\n## Setup\n\n```bash npm2yarn\nnpm install srt-parser-2\n```\n\n## Usage\n\n```typescript\nimport { SRTLoader } from \"langchain/document_loaders/fs/srt\";\n\nconst loader = new SRTLoader(\n  \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/subtitles.md","loc":{"lines":{"from":1,"to":26}}}}],["59",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Text files\n\nThis example goes over how to load data from text files.\n\n```typescript\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/text.md","loc":{"lines":{"from":1,"to":16}}}}],["60",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Unstructured\n\nThis example covers how to use [Unstructured](https://www.unstructured.io) to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.\n\n## Setup\n\nYou can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker [here](https://docs.docker.com/get-docker/).\n\n```bash\ndocker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n## Usage\n\nOnce Unstructured is running, you can use it to load files from your computer. You can use the following code to load a file from your computer.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/unstructured.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured.mdx","loc":{"lines":{"from":1,"to":25}}}}],["61",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Document Loaders\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/index.mdx","loc":{"lines":{"from":1,"to":11}}}}],["62",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# College Confidential\n\nThis example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CollegeConfidentialLoader } from \"langchain/document_loaders/web/college_confidential\";\n\nconst loader = new CollegeConfidentialLoader(\n  \"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/college_confidential.md","loc":{"lines":{"from":1,"to":26}}}}],["63",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitBook\n\nThis example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Load from single GitBook page\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\n  \"https://docs.gitbook.com/product-tour/navigation\"\n);\n\nconst docs = await loader.load();\n```\n\n## Load from all paths in a given GitBook\n\nFor this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have `shouldLoadAllPaths` set to `true`.\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\"https://docs.gitbook.com\", {\n  shouldLoadAllPaths: true,\n});\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/gitbook.md","loc":{"lines":{"from":1,"to":40}}}}],["64",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitHub\n\nThis example goes over how to load data from a GitHub repository.\nYou can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.\n\n```typescript\nimport { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nconst loader = new GithubRepoLoader(\n  \"https://github.com/hwchase17/langchainjs\",\n  { branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/github.md","loc":{"lines":{"from":1,"to":19}}}}],["65",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Hacker News\n\nThis example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { HNLoader } from \"langchain/document_loaders/web/hn\";\n\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/hn.md","loc":{"lines":{"from":1,"to":24}}}}],["66",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# IMSDB\n\nThis example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { IMSDBLoader } from \"langchain/document_loaders/web/imsdb\";\n\nconst loader = new IMSDBLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\");\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/imsdb.md","loc":{"lines":{"from":1,"to":24}}}}],["67",{"pageContent":"---\nlabel: \"Web Loaders\"\nhide_table_of_contents: true\n---\n\n# Web Loaders\n\nThese loaders are used to load web resources.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/index.mdx","loc":{"lines":{"from":1,"to":13}}}}],["68",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_class_name: node-only\n---\n\n# S3 File\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis covers how to load document objects from an s3 file object.\n\n## Setup\n\nTo run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.\n\nSee the docs [here](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured) for information on how to do that.\n\n## Usage\n\nOnce Unstructured is configured, you can use the S3 loader to load files and then convert them into a Document.\n\nYou can optionally provide a s3Config parameter to specify your bucket region, access key, and secret access key. If these are not provided, you will need to have them in your environment (e.g., by running `aws configure`).\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/s3.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/s3.mdx","loc":{"lines":{"from":1,"to":30}}}}],["69",{"pageContent":"---\nsidebar_position: 1\nsidebar_label: Cheerio\nhide_table_of_contents: true\n---\n\n# Webpages, with Cheerio\n\nThis example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.\n\nCheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.\n\nHowever, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the [PlaywrightWebBaseLoader](./web_playwright.md) or [PuppeteerWebBaseLoader](./web_puppeteer.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n```\n\n## Usage, with a custom selector\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\",\n  {\n    selector: \"p.athing\",\n  }\n);\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio.md","loc":{"lines":{"from":1,"to":47}}}}],["70",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_class_name: node-only\nsidebar_label: Playwright\n---\n\n# Webpages, with Playwright\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Playwright. One document will be created for each webpage.\n\nPlaywright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install playwright\n```\n\n## Usage\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\n/**\n * Loader uses `page.content()`\n * as default evaluate function\n **/\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```\n\n## Options\n\nHere's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:\n\n```typescript\ntype PlaywrightWebBaseLoaderOptions = {\n  launchOptions?: LaunchOptions;\n  gotoOptions?: PlaywrightGotoOptions;\n  evaluate?: PlaywrightEvaluate;\n};\n```\n\n1. `launchOptions`: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.\n\nBy passing these options to the `PlaywrightWebBaseLoader` constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":1,"to":83}}}}],["71",{"pageContent":"---\nsidebar_position: 2\nsidebar_label: Puppeteer\nhide_table_of_contents: true\nsidebar_class_name: node-only\n---\n\n# Webpages, with Puppeteer\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.\n\nPuppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install puppeteer\n```\n\n## Usage\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\n/**\n * Loader uses `page.evaluate(() => document.body.innerHTML)`\n * as default evaluate function\n **/\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```\n\n## Options\n\nHere's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:\n\n```typescript\ntype PuppeteerWebBaseLoaderOptions = {\n  launchOptions?: PuppeteerLaunchOptions;\n  gotoOptions?: PuppeteerGotoOptions;\n  evaluate?: (page: Page, browser: Browser) => Promise<string>;\n};\n```\n\n1. `launchOptions`: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.\n\nBy passing these options to the `PuppeteerWebBaseLoader` constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":1,"to":83}}}}],["72",{"pageContent":"---\nsidebar_label: Document Loaders\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Document Loaders\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/document-loaders)\n:::\n\nDocument loaders make it easy to create [Documents](../../schema/document.md) from a variety of sources. These documents can then be loaded onto [Vector Stores](../vector_stores/) to load documents from a source.\n\n```typescript\ninterface DocumentLoader {\n  load(): Promise<Document[]>;\n\n  loadAndSplit(textSplitter?: TextSplitter): Promise<Document[]>;\n}\n```\n\nDocument Loaders expose two methods, `load` and `loadAndSplit`. `load` will load the documents from the source and return them as an array of [Documents](../../schema/document.md). `loadAndSplit` will load the documents from the source, split them using the provided [TextSplitter](../text_splitters/index.mdx), and return them as an array of [Documents](../../schema/document.md).\n\n## All Document Loaders\n\n<DocCardList />\n\n## Advanced\n\nIf you want to implement your own Document Loader, you have a few options.\n\n### Subclassing `BaseDocumentLoader`\n\nYou can extend the `BaseDocumentLoader` class directly. The `BaseDocumentLoader` class provides a few convenience methods for loading documents from a variety of sources.\n\n```typescript\nabstract class BaseDocumentLoader implements DocumentLoader {\n  abstract load(): Promise<Document[]>;\n}\n```\n\n### Subclassing `TextLoader`\n\nIf you want to load documents from a text file, you can extend the `TextLoader` class. The `TextLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript\nabstract class TextLoader extends BaseDocumentLoader {\n  abstract parse(raw: string): Promise<string[]>;\n}\n```\n\n### Subclassing `BufferLoader`\n\nIf you want to load documents from a binary file, you can extend the `BufferLoader` class. The `BufferLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript\nabstract class BufferLoader extends BaseDocumentLoader {\n  abstract parse(\n    raw: Buffer,\n    metadata: Document[\"metadata\"]\n  ): Promise<Document[]>;\n}\n```\n","metadata":{"source":"docs/docs/modules/indexes/document_loaders/index.mdx","loc":{"lines":{"from":1,"to":66}}}}],["73",{"pageContent":"---\nsidebar_position: 4\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Indexes\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing)\n:::\n\nThis section deals with everything related to bringing your own data into LangChain, indexing it, and making it available for LLMs/Chat Models.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["74",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# ChatGPT Plugin Retriever\n\nThis example shows how to use the ChatGPT Retriever Plugin within LangChain.\n\nTo set up the ChatGPT Retriever Plugin, please follow instructions [here](https://github.com/openai/chatgpt-retrieval-plugin).\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/chatgpt-plugin.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/chatgpt-retriever-plugin.mdx","loc":{"lines":{"from":1,"to":17}}}}],["75",{"pageContent":"# Databerry Retriever\n\nThis example shows how to use the Databerry Retriever in a `RetrievalQAChain` to retrieve documents from a Databerry.ai datastore.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/databerry.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/databerry-retriever.mdx","loc":{"lines":{"from":1,"to":11}}}}],["76",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Retrievers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/retriever)\n:::\n\nA way of storing data such that it can be queried by a language model. The only interface this object must expose is a `getRelevantDocuments` method which takes in a string query and returns a list of Documents.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/index.mdx","loc":{"lines":{"from":1,"to":17}}}}],["77",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Metal Retriever\n\nThis example shows how to use the Metal Retriever in a `RetrievalQAChain` to retrieve documents from a Metal index.\n\n## Setup\n\n```bash npm2yarn\nnpm i @getmetal/metal-sdk\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/metal.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/metal-retriever.mdx","loc":{"lines":{"from":1,"to":21}}}}],["78",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Remote Retriever\n\nThis example shows how to use a Remote Retriever in a `RetrievalQAChain` to retrieve documents from a remote server.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/retrieval_qa_with_remote.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/remote-retriever.mdx","loc":{"lines":{"from":1,"to":15}}}}],["79",{"pageContent":"# Supabase Hybrid Search\n\nLangchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres `pgvector` extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore `addDocuments` function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The `getRelevantDocuments` function produces a list of documents that has duplicates removed and is sorted by relevance score.\n\n## Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```\n\n### Create a table and search functions in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to similarity search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create a function to keyword search for documents\ncreate function kw_match_documents(query_text text, match_count int)\nreturns table (id bigint, content text, metadata jsonb, similarity real)\nas $$\n\nbegin\nreturn query execute\nformat('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity\nfrom documents\nwhere to_tsvector(content) @@ plainto_tsquery($1)\norder by similarity desc\nlimit $2')\nusing query_text, match_count;\nend;\n$$ language plpgsql;\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/supabase_hybrid.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/supabase-hybrid.mdx","loc":{"lines":{"from":1,"to":78}}}}],["80",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Vector Store\n\nOnce you've created a [Vector Store](../vector_stores/), the way to use it as a Retriever is very simple:\n\n```typescript\nvectorStore = ...\nretriever = vectorStore.asRetriever()\n```\n","metadata":{"source":"docs/docs/modules/indexes/retrievers/vectorstore.md","loc":{"lines":{"from":1,"to":13}}}}],["81",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# CharacterTextSplitter\n\nBesides the `RecursiveCharacterTextSplitter`, there is also the more standard `CharacterTextSplitter`. This splits only on one type of character (defaults to `\"\\n\\n\"`). You can use it in the exact same way.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\n  separator: \" \",\n  chunkSize: 7,\n  chunkOverlap: 3,\n});\nconst output = await splitter.createDocuments([text]);\n```\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/examples/character.mdx","loc":{"lines":{"from":1,"to":21}}}}],["82",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Text Splitters: Examples\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/examples/index.mdx","loc":{"lines":{"from":1,"to":10}}}}],["83",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `MarkdownTextSplitter`\n\nIf your content is in Markdown format then `MarkdownTextSplitter`. This class will split your content into documents based on the Markdown headers. For example, if you have the following Markdown content:\n\n```markdown\n# Header 1\n\nThis is some content.\n\n## Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.\n```\n\nThen the `MarkdownTextSplitter` will split the content into three documents:\n\n```typescript\nimport { MarkdownTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `# Header 1\n\nThis is some content.\n\n## Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.`;\n\nconst splitter = new MarkdownTextSplitter();\n\nconst output = await splitter.createDocuments([text], {\n  metadata: \"something\",\n});\n/*\n[\n  {\n    \"pageContent\": \"# Header 1\\n\\nThis is some content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"## Header 2\\n\\nThis is some more content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"# Header 3\\n\\nThis is even more content.\",\n    \"metadata\": \"something\"\n  }\n]\n*/\n```\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/examples/markdown.mdx","loc":{"lines":{"from":1,"to":62}}}}],["84",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `RecursiveCharacterTextSplitter`\n\nThe recommended TextSplitter is the `RecursiveCharacterTextSplitter`. This will split documents recursively by different characters - starting with `\"\\n\\n\"`, then `\"\\n\"`, then `\" \"`. This is nice because it will try to keep all the semantically relevant content in the same place for as long as possible.\n\nImportant parameters to know here are `chunkSize` and `chunkOverlap`. `chunkSize` controls the max size (in terms of number of characters) of the final documents. `chunkOverlap` specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to `4000` and `200` respectively.\n\n```typescript\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst output = await splitter.createDocuments([text]);\n```\n\nYou'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n```\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/examples/recursive_character.mdx","loc":{"lines":{"from":1,"to":43}}}}],["85",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# TokenTextSplitter\n\nFinally, `TokenTextSplitter` splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\n\nTo utilize the `TokenTextSplitter`, first install the accompanying required library\n\n```bash npm2yarn\nnpm install -S @dqbd/tiktoken\n```\n\nThen, you can use it like so:\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\n\nconst splitter = new TokenTextSplitter({\n  encodingName: \"gpt2\",\n  chunkSize: 10,\n  chunkOverlap: 0,\n});\n\nconst output = await splitter.createDocuments([text]);\n```\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/examples/token.mdx","loc":{"lines":{"from":1,"to":31}}}}],["86",{"pageContent":"---\nsidebar_label: Text Splitters\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Text Splitters\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/text-splitters)\n:::\n\nLanguage Models are often limited by the amount of text that you can pass to them. Therefore, it is neccessary to split them up into smaller chunks. LangChain provides several utilities for doing so.\n\nUsing a Text Splitter can also help improve the results from vector store searches, as eg. smaller chunks may sometimes be more likely to match a query. Testing different chunk sizes (and chunk overlap) is a worthwhile exercise to tailor the results to your use case.\n\n```typescript\ninterface TextSplitter {\n  chunkSize: number;\n\n  chunkOverlap: number;\n\n  createDocuments(\n    texts: string[],\n    metadatas?: Record<string, any>[]\n  ): Promise<Document[]>;\n\n  splitDocuments(documents: Document[]): Promise<Document[]>;\n}\n```\n\nText Splitters expose two methods, `createDocuments` and `splitDocuments`. The former takes a list of raw text strings and returns a list of documents. The latter takes a list of documents and returns a list of documents. The difference is that `createDocuments` will split the raw text strings into chunks, while `splitDocuments` will split the documents into chunks.\n\n## All Text Splitters\n\n<DocCardList />\n\n## Advanced\n\nIf you want to implement your own custom Text Splitter, you only need to subclass TextSplitter and implement a single method `splitText`. The method takes a string and returns a list of strings. The returned strings will be used as the chunks.\n\n```typescript\nabstract class TextSplitter {\n  abstract splitText(text: string): Promise<string[]>;\n}\n```\n","metadata":{"source":"docs/docs/modules/indexes/text_splitters/index.mdx","loc":{"lines":{"from":1,"to":49}}}}],["87",{"pageContent":"---\nsidebar_label: \"Vector Stores\"\nsidebar_position: 3\n---\n\n# Getting Started: Vector Stores\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/vectorstore)\n:::\n\nA vector store is a particular type of database optimized for storing documents and their [embeddings](../../models/embeddings/), and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n\n```typescript\ninterface VectorStore {\n  /**\n   * Add more documents to an existing VectorStore\n   */\n  addDocuments(documents: Document[]): Promise<void>;\n\n  /**\n   * Search for the most similar documents to a query\n   */\n  similaritySearch(\n    query: string,\n    k?: number,\n    filter?: object | undefined\n  ): Promise<Document[]>;\n\n  /**\n   * Search for the most similar documents to a query,\n   * and return their similarity score\n   */\n  similaritySearchWithScore(\n    query: string,\n    k = 4,\n    filter: object | undefined = undefined\n  ): Promise<[object, number][]>;\n\n  /**\n   * Turn a VectorStore into a Retriever\n   */\n  asRetriever(k?: number): BaseRetriever;\n\n  /**\n   * Advanced: Add more documents to an existing VectorStore,\n   * when you already have their embeddings\n   */\n  addVectors(vectors: number[][], documents: Document[]): Promise<void>;\n\n  /**\n   * Advanced: Search for the most similar documents to a query,\n   * when you already have the embedding of the query\n   */\n  similaritySearchVectorWithScore(\n    query: number[],\n    k: number,\n    filter?: object\n  ): Promise<[Document, number][]>;\n}\n```\n\nYou can create a vector store from a list of [Documents](../../schema/document), or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.\n\n```typescript\nabstract class BaseVectorStore implements VectorStore {\n  static fromTexts(\n    texts: string[],\n    metadatas: object[] | object,\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n\n  static fromDocuments(\n    docs: Document[],\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n}\n```\n\n## Which one to pick?\n\nHere's a quick guide to help you pick the right vector store for your use case:\n\n- If you're after something that can just run inside your Node.js application, in-memory, without any other servers to stand up, then go for [HNSWLib](./integrations/hnswlib)\n- If you're looking for something that can run in-memory in browser-like environments, then go for [MemoryVectorStore](./integrations/memory)\n- If you come from Python and you were looking for something similar to FAISS, pick [HNSWLib](./integrations/hnswlib)\n- If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for [Chroma](./integrations/chroma)\n- If you're using Supabase already then look at the [Supabase](./integrations/supabase) vector store to use the same Postgres database for your embeddings too\n- If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for [Pinecone](./integrations/pinecone)\n\n## All Vector Stores\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/index.mdx","loc":{"lines":{"from":1,"to":98}}}}],["88",{"pageContent":"# Chroma\n\nChroma is an open-source Apache 2.0 embedding database.\n\nUse [chroma](https://github.com/chroma-core/chroma) with langchainjs.\n\n## Setup\n\n1. Run chroma inside of docker on your computer [docs](https://docs.trychroma.com/api-reference)\n2. Install the chroma js client.\n\n```bash npm2yarn\nnpm install -S chromadb\n```\n\n## Index and query docs\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Chroma.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n        Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n        Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n        it, waiting innocent victims to get lost in its fears complexity.\\\n        Then, when they wander and dazed into the center, he laughs and\\\n        laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);\n\n// or alternatively from docs\nconst vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel-escher-bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n\n## Query docs from existing collection\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Chroma.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/chroma.md","loc":{"lines":{"from":1,"to":66}}}}],["89",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# HNSWLib\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nHNSWLib is an in-memory vectorstore that can be saved to a file. It uses [HNSWLib](https://github.com/nmslib/hnswlib).\n\n## Setup\n\nYou can install it with\n\n```bash npm2yarn\nnpm install hnswlib-node\n```\n\n## Usage\n\n### Create a new index from texts\n\nimport ExampleTexts from \"@examples/indexes/vector_stores/hnswlib.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleTexts}</CodeBlock>\n\n### Create a new index from a loader\n\nimport ExampleLoader from \"@examples/indexes/vector_stores/hnswlib_fromdocs.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLoader}</CodeBlock>\n\n### Save an index to a file and load it again\n\nimport ExampleSave from \"@examples/indexes/vector_stores/hnswlib_saveload.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSave}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/hnswlib.mdx","loc":{"lines":{"from":1,"to":42}}}}],["90",{"pageContent":"---\nsidebar_label: Integrations\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Vector Stores: Integrations\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/index.mdx","loc":{"lines":{"from":1,"to":10}}}}],["91",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Memory\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# `MemoryVectorStore`\n\nMemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by [ml-distance](https://mljs.github.io/distance/modules/similarity.html).\n\n## Usage\n\n### Create a new index from texts\n\nimport ExampleTexts from \"@examples/indexes/vector_stores/memory.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleTexts}</CodeBlock>\n\n### Create a new index from a loader\n\nimport ExampleLoader from \"@examples/indexes/vector_stores/memory_fromdocs.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLoader}</CodeBlock>\n\n### Use a custom similarity metric\n\nimport ExampleCustom from \"@examples/indexes/vector_stores/memory_custom_similarity.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleCustom}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/memory.mdx","loc":{"lines":{"from":1,"to":32}}}}],["92",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# Milvus\n\n[Milvus](https://milvus.io/) is a vector database built for embeddings similarity search and AI applications.\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n## Setup\n\n1. Run Milvus instance with Docker on your computer [docs](https://milvus.io/docs/v2.1.x/install_standalone-docker.md)\n2. Install the Milvus Node.js SDK.\n\n```bash npm2yarn\nnpm install -S @zilliz/milvus2-sdk-node\n```\n\n3. Setup Env variables for Milvus before running the code\n\n```bash\nexport OPENAI_API_KEY=YOUR_OPEN_API_HERE\nexport MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530\n```\n\n## Index and query docs\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Milvus.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n            Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n            Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n            it, waiting innocent victims to get lost in its fears complexity.\\\n            Then, when they wander and dazed into the center, he laughs and\\\n            laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\n// or alternatively from docs\nconst vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel_escher_bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n\n## Query docs from existing collection\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Milvus.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":1,"to":79}}}}],["93",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# OpenSearch\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n[OpenSearch](https://opensearch.org/) is a fork of [Elasticsearch](https://www.elastic.co/elasticsearch/) that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors [here](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/).\n\nLangchain.js accepts [@opensearch-project/opensearch](https://opensearch.org/docs/latest/clients/javascript/index/) as the client for OpenSearch vectorstore.\n\n## Setup\n\n```bash npm2yarn\nnpm install -S @opensearch-project/opensearch\n```\n\nYou'll also need to have an OpenSearch instance running. You can use the [official Docker image](https://opensearch.org/docs/latest/opensearch/install/docker/) to get started. You can also find an example docker-compose file [here](https://github.com/hwchase17/langchainjs/blob/main/examples/src/indexes/vector_stores/opensearch/docker-compose.yml).\n\n## Index docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"opensearch is also a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent:\n      \"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications\",\n  }),\n];\n\nawait OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  client,\n  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`\n});\n```\n\n## Query docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {\n  client,\n});\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(JSON.stringify(results, null, 2));\n/* [\n    {\n      \"pageContent\": \"Hello world\",\n      \"metadata\": {\n        \"id\": 2\n      }\n    }\n  ] */\n\n/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is opensearch?\" });\n\nconsole.log(JSON.stringify(response, null, 2));\n/* \n  {\n    \"text\": \" Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.\",\n    \"sourceDocuments\": [\n      {\n        \"pageContent\": \"What's this?\",\n        \"metadata\": {\n          \"id\": 3\n        }\n      }\n    ]\n  } \n  */\n```\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":1,"to":113}}}}],["94",{"pageContent":"# Pinecone\n\nLangchain.js accepts [@pinecone-database/pinecone](https://docs.pinecone.io/docs/node-client) as the client for Pinecone vectorstore. Install the library with\n\n```bash npm2yarn\nnpm install -S dotenv langchain @pinecone-database/pinecone\n```\n\n## Index docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"pinecone is a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"pinecones are the woody fruiting body and of a pine tree\",\n  }),\n];\n\nawait PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  pineconeIndex,\n});\n```\n\n## Query docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"pinecone\", 1, {\n  foo: \"bar\",\n});\nconsole.log(results);\n/*\n[\n  Document {\n    pageContent: 'pinecone is a vector db',\n    metadata: { foo: 'bar' }\n  }\n]\n*/\n\n/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is pinecone?\" });\nconsole.log(response);\n/*\n{\n  text: ' A pinecone is the woody fruiting body of a pine tree.',\n  sourceDocuments: [\n    Document {\n      pageContent: 'pinecones are the woody fruiting body and of a pine tree',\n      metadata: [Object]\n    }\n  ]\n}\n*/\n```\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":1,"to":109}}}}],["95",{"pageContent":"# Prisma\n\nFor augmenting existing models in PostgreSQL database with vector search, Langchain supports using [Prisma](https://www.prisma.io/) together with PostgreSQL and [`pgvector`](https://github.com/pgvector/pgvector) Postgres extension.\n\n## Setup\n\n### Setup database instance with Supabase\n\nRefer to the [Prisma and Supabase integration guide](https://supabase.com/docs/guides/integrations/prisma) to setup a new database instance with Supabase and Prisma.\n\n### Install Prisma\n\n```bash npm2yarn\nnpm install prisma\n```\n\n### Setup `pgvector` self hosted instance with `docker-compose`\n\n`pgvector` provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.\n\n```yaml\nservices:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - db:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=\n      - POSTGRES_USER=\n      - POSTGRES_DB=\n\nvolumes:\n  db:\n```\n\n### Create a new schema\n\nAssuming you haven't created a schema yet, create a new model with a `vector` field of type `Unsupported(\"vector\")`:\n\n```prisma\nmodel Document {\n  id      String                 @id @default(cuid())\n  content String\n  vector  Unsupported(\"vector\")?\n}\n```\n\nAfterwards, create a new migration with `--create-only` to avoid running the migration directly.\n\n```bash npm2yarn\nnpm run prisma migrate dev --create-only\n```\n\nAdd the following line to the newly created migration to enable `pgvector` extension if it hasn't been enabled yet:\n\n```sql\nCREATE EXTENSION IF NOT EXISTS vector;\n```\n\nRun the migration afterwards:\n\n```bash npm2yarn\nnpm run prisma migrate dev\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/indexes/vector_stores/prisma_vectorstore/prisma.ts\";\nimport Schema from \"@examples/indexes/vector_stores/prisma_vectorstore/prisma/schema.prisma\";\n\nUse the `withModel` method to get proper type hints for `metadata` field:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\nThe sample above uses the following schema:\n\n<CodeBlock language=\"prisma\">{Schema}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/prisma.mdx","loc":{"lines":{"from":1,"to":81}}}}],["96",{"pageContent":"# Supabase\n\nLangchain supports using Supabase Postgres database as a vector store, using the `pgvector` postgres extension. Refer to the [Supabase blog post](https://supabase.com/blog/openai-embeddings-postgres-vector) for more information.\n\n## Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```\n\n### Create a table and search function in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/indexes/vector_stores/supabase.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/supabase.mdx","loc":{"lines":{"from":1,"to":62}}}}],["97",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Weaviate\n\nWeaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the `weaviate-ts-client` package, the official Typescript client for Weaviate.\n\nLangChain inserts vectors directly to Weaviate, and queries Weaviate for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Weaviate.\n\n## Setup\n\n```bash npm2yarn\nnpm install weaviate-ts-client graphql\n```\n\nYou'll need to run Weaviate either locally or on a server, see [the Weaviate documentation](https://weaviate.io/developers/weaviate/installation) for more information.\n\n## Usage, insert documents\n\nimport InsertExample from \"@examples/indexes/vector_stores/weaviate_fromTexts.ts\";\n\n<CodeBlock language=\"typescript\">{InsertExample}</CodeBlock>\n\n## Usage, query documents\n\nimport QueryExample from \"@examples/indexes/vector_stores/weaviate_search.ts\";\n\n<CodeBlock language=\"typescript\">{QueryExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/indexes/vector_stores/integrations/weaviate.mdx","loc":{"lines":{"from":1,"to":32}}}}],["98",{"pageContent":"# Buffer Memory\n\nBufferMemory is the simplest type of memory - it just remembers previous conversational back and forths directly.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\nYou can also load messages into a `BufferMemory` instance by creating and passing in a `ChatHistory` object.\nThis lets you easily pick up state from past conversations:\n\n```typescript\nimport { ChatMessageHistory } from \"langchain/memory\";\nimport { HumanChatMessage, AIChatMessage } from \"langchain/schema\";\n\nconst pastMessages = [\n  new HumanChatMessage(\"My name's Jonas\"),\n  new AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\n\nconst memory = new BufferMemory({\n  chatHistory: new ChatMessageHistory(pastMessages),\n});\n```\n","metadata":{"source":"docs/docs/modules/memory/examples/buffer_memory.md","loc":{"lines":{"from":1,"to":46}}}}],["99",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/memory.ts\";\n\n# Using Buffer Memory with Chat Models\n\nThis example covers how to use chat-specific memory classes with chat models.\nThe key thing to notice is that setting `returnMessages: true` makes the memory return a list of chat messages instead of a string.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/memory/examples/buffer_memory_chat.mdx","loc":{"lines":{"from":1,"to":14}}}}],["100",{"pageContent":"# Buffer Window Memory\n\nBufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size `k` to surface the last `k` back-and-forths to use as memory.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferWindowMemory({ k: 1 });\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n","metadata":{"source":"docs/docs/modules/memory/examples/buffer_window_memory.md","loc":{"lines":{"from":1,"to":29}}}}],["101",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Conversation Summary\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Conversation Summary Memory\n\nThe Conversation Summary Memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.\n\n## Usage, with an LLM\n\nimport TextExample from \"@examples/memory/summary_llm.ts\";\n\n<CodeBlock language=\"typescript\">{TextExample}</CodeBlock>\n\n## Usage, with a Chat Model\n\nimport ChatExample from \"@examples/memory/summary_chat.ts\";\n\n<CodeBlock language=\"typescript\">{ChatExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/memory/examples/conversation_summary.mdx","loc":{"lines":{"from":1,"to":23}}}}],["102",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Memory\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/memory/examples/index.mdx","loc":{"lines":{"from":1,"to":10}}}}],["103",{"pageContent":"# Motörhead Memory\n\n[Motörhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n\n## Setup\n\nSee instructions at [Motörhead](https://github.com/getmetal/motorhead) for running the server locally.\n\n## Usage\n\n```typescript\nimport { ConversationChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models\";\nimport { MotorheadMemory } from \"langchain/memory\";\n\nconst model = new ChatOpenAI({});\nconst memory = new MotorheadMemory({\n  sessionId: \"user-id\",\n  motorheadUrl: \"localhost:8080\",\n});\n\nawait memory.init(); // loads previous state from Motörhead 🤘\nconst context = memory.context\n  ? `\nHere's previous context: ${memory.context}`\n  : \"\";\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.${context}`\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory,\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n","metadata":{"source":"docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":1,"to":58}}}}],["104",{"pageContent":"---\nsidebar_label: Memory\nsidebar_position: 5\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Memory\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/memory)\n:::\n\nMemory is the concept of storing and retrieving data in the process of a conversation. There are two main methods, `loadMemoryVariables` and `saveContext`. The first method is used to retrieve data from memory (optionally using the current input values), and the second method is used to store data in memory.\n\n```typescript\nexport type InputValues = Record<string, any>;\n\nexport type OutputValues = Record<string, any>;\n\ninterface BaseMemory {\n  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}\n```\n\n:::note\nDo not share the same memory instance between two different chains, a memory instance represents the history of a single conversation\n:::\n\n:::note\nIf you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.\n:::\n\n## All Memory classes\n\n<DocCardList />\n\n## Advanced\n\nTo implement your own memory class you have two options:\n\n### Subclassing `BaseChatMemory`\n\nThis is the easiest way to implement your own memory class. You can subclass `BaseChatMemory`, which takes care of `saveContext` by saving inputs and outputs as [Chat Messages](../schema/chat-messages.md), and implement only the `loadMemoryVariables` method. This method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseChatMemory extends BaseMemory {\n  chatHistory: ChatMessageHistory;\n\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n}\n```\n\n### Subclassing `BaseMemory`\n\nIf you want to implement a more custom memory class, you can subclass `BaseMemory` and implement both `loadMemoryVariables` and `saveContext` methods. The `saveContext` method is responsible for storing the input and output values in memory. The `loadMemoryVariables` method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseMemory {\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  abstract saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}\n```\n","metadata":{"source":"docs/docs/modules/memory/index.mdx","loc":{"lines":{"from":1,"to":73}}}}],["105",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat.ts\";\nimport StreamingExample from \"@examples/models/chat/chat_streaming.ts\";\nimport TimeoutExample from \"@examples/models/chat/chat_timeout.ts\";\n\n# Additional Functionality: Chat Models\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with chat models:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Streaming\n\nSimilar to LLMs, you can stream responses from a chat model. This is useful for chatbots that need to respond to user input in real-time.\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({ maxRetries: 10 });\n```\n","metadata":{"source":"docs/docs/modules/models/chat/additional_functionality.mdx","loc":{"lines":{"from":1,"to":57}}}}],["106",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chat Models\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Chat Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/chat-model)\n:::\n\nLangChain provides a standard interface for using chat models. Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\n## Chat Messages\n\nA `ChatMessage` is what we refer to as the modular unit of information for a chat model.\nAt the moment, this consists of a `\"text\"` field, which refers to the content of the chat message.\n\nThere are currently four different classes of `ChatMessage` supported by LangChain:\n\n- `HumanChatMessage`: A chat message that is sent as if from a Human's point of view.\n- `AIChatMessage`: A chat message that is sent from the point of view of the AI system to which the Human is corresponding.\n- `SystemChatMessage`: A chat message that gives the AI system some information about the conversation. This is usually sent at the beginning of a conversation.\n- `ChatMessage`: A generic chat message, with not only a `\"text\"` field but also an arbitrary `\"role\"` field.\n\n> **_Note:_** Currently, the only chat-based model we support is `ChatOpenAI` (with gpt-4 and gpt-3.5-turbo), but anticipate adding more in the future.\n\nTo get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `ChatOpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/models/chat/index.mdx","loc":{"lines":{"from":1,"to":41}}}}],["107",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Integrations: Chat Models\n\nLangChain offers a number of Chat Models implementations that integrate with various model providers. These are:\n\n## `ChatOpenAI`\n\nimport OpenAI from \"@examples/models/chat/integration_openai.ts\";\n\n<CodeBlock language=\"typescript\">{OpenAI}</CodeBlock>\n\n## `ChatAnthropic`\n\nimport Anthropic from \"@examples/models/chat/integration_anthropic.ts\";\n\n<CodeBlock language=\"typescript\">{Anthropic}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/models/chat/integrations.mdx","loc":{"lines":{"from":1,"to":23}}}}],["108",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport TimeoutExample from \"@examples/models/embeddings/openai_timeout.ts\";\n\n# Additional Functionality: Embeddings\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an Embeddings model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst model = new OpenAIEmbeddings({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst model = new OpenAIEmbeddings({ maxRetries: 10 });\n```\n","metadata":{"source":"docs/docs/modules/models/embeddings/additional_functionality.mdx","loc":{"lines":{"from":1,"to":43}}}}],["109",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Embeddings\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Embeddings\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/text-embedding-model)\n:::\n\nEmbeddings can be used to create a numerical representation of textual data. This numerical representation is useful because it can be used to find similar documents.\n\nBelow is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a `embedQuery` and `embedDocuments` method.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n/* Create instance */\nconst embeddings = new OpenAIEmbeddings();\n\n/* Embed queries */\nconst res = await embeddings.embedQuery(\"Hello world\");\n/*\n[\n   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,\n    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,\n    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,\n   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,\n   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,\n  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,\n   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,\n   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,\n    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,\n    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,\n    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,\n    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,\n    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,\n      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,\n    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,\n   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,\n    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,\n   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,\n    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,\n  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,\n  ... 1436 more items\n]\n*/\n\n/* Embed documents */\nconst documentRes =","metadata":{"source":"docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":1,"to":53}}}}],["110",{"pageContent":" await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\n/*\n[\n  [\n    -0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,\n      0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,\n      0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,\n      0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,\n      -0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,\n    -0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,\n      0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,\n      -0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,\n      0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,\n      0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,\n      0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,\n      0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,\n      0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,\n      0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,\n      0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,\n      0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,\n        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,\n      0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,\n      0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,\n    -0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,\n    ... 1436 more items\n  ],\n  [\n      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,\n      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,\n      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,\n      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,\n     ","metadata":{"source":"docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":53,"to":84}}}}],["111",{"pageContent":" -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,\n      -0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,\n        0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,\n      -0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,\n      0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,\n      0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,\n        0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,\n      0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,\n      0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,\n      -0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,\n      0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,\n      0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,\n      0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,\n    -0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,\n    0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,\n      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,\n    ... 1436 more items\n  ]\n]\n*/\n```\n\n## Dig deeper\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/models/embeddings/index.mdx","loc":{"lines":{"from":84,"to":109}}}}],["112",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: Embeddings\n\nLangChain offers a number of Embeddings implementations that integrate with various model providers. These are:\n\n## `OpenAIEmbeddings`\n\nThe `OpenAIEmbeddings` class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing `stripNewLines: false` to the constructor.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\n```\n\n## `CohereEmbeddings`\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nconst embeddings = new CohereEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.COHERE_API_KEY\n});\n```\n","metadata":{"source":"docs/docs/modules/models/embeddings/integrations.mdx","loc":{"lines":{"from":1,"to":35}}}}],["113",{"pageContent":"---\nsidebar_position: 2\nhide_table_of_contents: true\nsidebar_label: Models\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/)\n:::\n\nModels are a core component of LangChain. LangChain is not a provider of models, but rather provides a standard interface through which you can interact with a variety of language models.\nLangChain provides support for both text-based Large Language Models (LLMs), Chat Models, and Text Embedding models.\n\nLLMs use a text-based input and output, while Chat Models use a message-based input and output.\n\n> **_Note:_** Chat model APIs are fairly new, so we are still figuring out the correct abstractions. If you have any feedback, please let us know!\n\n## All Models\n\n<DocCardList />\n\n## Advanced\n\n_This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section._\n\nBoth LLMs and Chat Models are built on top of the `BaseLanguageModel` class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.\n\nThe `BaseLanguageModel` class has two abstract methods: `generatePrompt` and `getNumTokens`, which are implemented by `BaseChatModel` and `BaseLLM` respectively.\n\n`BaseLLM` is a subclass of `BaseLanguageModel` that provides a common interface for LLMs while `BaseChatModel` is a subclass of `BaseLanguageModel` that provides a common interface for chat models.\n","metadata":{"source":"docs/docs/modules/models/index.mdx","loc":{"lines":{"from":1,"to":35}}}}],["114",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm.ts\";\nimport DebuggingExample from \"@examples/models/llm/llm_debugging.ts\";\nimport StreamingExample from \"@examples/models/llm/llm_streaming.ts\";\nimport TimeoutExample from \"@examples/models/llm/llm_timeout.ts\";\n\n# Additional Functionality: LLMs\n\nWe offer a number of additional features for LLMs. In most of the examples below, we'll be using the `OpenAI` LLM. However, all of these features are available for all LLMs.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with LLMs:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Streaming Responses\n\nSome LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.\nLangChain currently provides streaming for the `OpenAI` LLM:\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>\n\n## Caching\n\nLangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n\n1. It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n2. It can speed up your application by reducing the number of API calls you make to the LLM provider.\n\n### Caching in-memory\n\nThe default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.\n\nTo enable it you can pass `cache: true` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ cache: true });\n```\n\n### Caching with Redis\n\nLangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the `redis` package:\n\n```bash npm2yarn\nnpm install redis\n```\n\nThen, you can pass a `cache` option when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { RedisCache } from \"langchain/cache/redis\";\nimport { createClient } from \"redis\";\n\n// See https://github.com/redis/node-redis for connection options\nconst client = createClient();\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n```\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ maxRetries: 10","metadata":{"source":"docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":1,"to":98}}}}],["115",{"pageContent":" });\n```\n\n## Logging for Debugging\n\nEspecially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a chain. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can use the LLMCallbackManager to write yourself custom logging (or anything else you want to do) as the model goes through the steps:\n\n<CodeBlock language=\"typescript\">{DebuggingExample}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/models/llms/additional_functionality.mdx","loc":{"lines":{"from":98,"to":106}}}}],["116",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLMs\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: LLMs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/language-model)\n:::\n\nLangChain provides a standard interface for using a variety of LLMs.\n\nTo get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `OpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/models/llms/index.mdx","loc":{"lines":{"from":1,"to":25}}}}],["117",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: LLMs\n\nLangChain offers a number of LLM implementations that integrate with various model providers. These are:\n\n## `OpenAI`\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## `HuggingFaceInference`\n\n```bash npm2yarn\nnpm install @huggingface/inference\n```\n\n```typescript\nimport { HuggingFaceInference } from \"langchain/llms/hf\";\n\nconst model = new HuggingFaceInference({\n  model: \"gpt2\",\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY\n});\nconst res = await model.call(\"1 + 1 =\");\nconsole.log({ res });\n```\n\n## `Cohere`\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { Cohere } from \"langchain/llms/cohere\";\n\nconst model = new Cohere({\n  maxTokens: 20,\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.COHERE_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## `Replicate`\n\n```bash npm2yarn\nnpm install replicate\n```\n\n```typescript\nimport { Replicate } from \"langchain/llms/replicate\";\n\nconst model = new Replicate({\n  model:\n    \"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.REPLICATE_API_KEY\n});\nconst res = await modelA.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## Additional LLM Implementations\n\n### `PromptLayerOpenAI`\n\nLangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:\n\n1. Create a PromptLayer account here: [https://promptlayer.com](https://promptlayer.com).\n2. Create an API token and pass it either as `promptLayerApiKey` argument in the `PromptLayerOpenAI` constructor or in the `PROMPTLAYER_API_KEY` environment variable.\n\n```typescript\nimport { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nconst model = new PromptLayerOpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  promptLayerApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY\n});\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```\n\nThe request and the response will be logged in the [PromptLayer dashboard](https://promptlayer.com/home).\n\n> **_Note:_** In streaming mode PromptLayer will not log the response.\n","metadata":{"source":"docs/docs/modules/models/llms/integrations.mdx","loc":{"lines":{"from":1,"to":106}}}}],["118",{"pageContent":"---\nsidebar_label: Example Selectors\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Example Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/example-selectors)\n:::\n\nIf you have a large number of examples, you may need to programmatically select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so. The base interface is defined as below.\n\n```typescript\nclass BaseExampleSelector {\n  addExample(example: Example): Promise<void | string>;\n\n  selectExamples(input_variables: Example): Promise<Example[]>;\n}\n```\n\nIt needs to expose a `selectExamples` - this takes in the input variables and then returns a list of examples method - and an `addExample` method, which saves an example for later selection. It is up to each specific implementation as to how those examples are saved and selected. Let’s take a look at some below.\n\n## Select by Length\n\nThis `ExampleSelector` selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n\nimport ExampleLength from \"@examples/prompts/length_based_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLength}</CodeBlock>\n\n## Select by Similarity\n\nThe `SemanticSimilarityExampleSelector` selects examples based on which examples are most similar to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n\nimport ExampleSimilarity from \"@examples/prompts/semantic_similarity_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSimilarity}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/prompts/example_selectors/index.mdx","loc":{"lines":{"from":1,"to":41}}}}],["119",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_label: Prompts\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompts\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts)\n:::\n\nLangChain provides several utilities to help manage prompts for language models, including chat models.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/prompts/index.mdx","loc":{"lines":{"from":1,"to":18}}}}],["120",{"pageContent":"---\nsidebar_label: Output Parsers\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Output Parsers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/output-parser)\n:::\n\nLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n\n- `getFormatInstructions(): str` A method which returns a string containing instructions for how the output of a language model should be formatted.\n- `parse(raw: string): any` A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n\nAnd then one optional one:\n\n- `parseWithPrompt(text: string, prompt: BasePromptValue): any`: A method which takes in a string (assumed to be the response from a language model) and a formatted prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n\nBelow we go over some examples of output parsers.\n\n## Structured Output Parser\n\nThis output parser can be used when you want to return multiple fields.\n\nimport Structured from \"@examples/prompts/structured_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Structured}</CodeBlock>\n\n## Structured Output Parser with Zod Schema\n\nThis output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. `z.date()` is not allowed, but `z.coerce.date()` is.\n\nimport StructuredZod from \"@examples/prompts/structured_parser_zod.ts\";\n\n<CodeBlock language=\"typescript\">{StructuredZod}</CodeBlock>\n\n## Output Fixing Parser\n\nThis output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n\nimport Fix from \"@examples/prompts/fix_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Fix}</CodeBlock>\n\n## Comma-separated List Parser\n\nThis output parser can be used when you want to return a list of items.\n\nimport Comma from \"@examples/prompts/comma_list_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Comma}</CodeBlock>\n\n## Combining Output Parsers\n\nOutput parsers can be combined using `CombiningOutputParser`. This output parser takes in a list of output parsers, and will ask for (and parse) a combined output that contains all the fields of all the parsers.\n\nimport Combining from \"@examples/prompts/combining_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Combining}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/prompts/output_parsers/index.mdx","loc":{"lines":{"from":1,"to":66}}}}],["121",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport PromptValue from \"@examples/prompts/prompt_value.ts\";\nimport PartialValue from \"@examples/prompts/partial.ts\";\nimport FewShot from \"@examples/prompts/few_shot.ts\";\n\n# Additional Functionality: Prompt Templates\n\nWe offer a number of extra features for prompt templates, as shown below:\n\n## Prompt Values\n\nA `PromptValue` is an object returned by the `formatPromptValue` of a `PromptTemplate`. It can be converted to a string or list of `ChatMessage` objects.\n\n<CodeBlock language=\"typescript\">{PromptValue}</CodeBlock>\n\n## Partial Values\n\nLike other methods, it can make sense to \"partial\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n\nLangChain supports this in two ways:\n\n1. Partial formatting with string values.\n2. Partial formatting with functions that return string values.\n\nThese two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.\n\n<CodeBlock language=\"typescript\">{PartialValue}</CodeBlock>\n\n## Few-Shot Prompt Templates\n\nA few-shot prompt template is a prompt template you can build with examples.\n\n<CodeBlock language=\"typescript\">{FewShot}</CodeBlock>\n","metadata":{"source":"docs/docs/modules/prompts/prompt_templates/additional_functionality.mdx","loc":{"lines":{"from":1,"to":38}}}}],["122",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Templates\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/prompts/prompts.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompt Templates\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts/prompt-template)\n:::\n\nA `PromptTemplate` allows you to make use of templating to generate a prompt. This is useful for when you want to use the same prompt outline in multiple places, but with certain values changed.\nPrompt templates are supported for both LLMs and chat models, as shown below:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/prompts/prompt_templates/index.mdx","loc":{"lines":{"from":1,"to":25}}}}],["123",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Chat Messages\n\nThe primary interface through which end users interact with LLMs is a chat interface. For this reason, some model providers have started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user (or role). Right now the supported users are System, Human, and AI.\n\n## SystemChatMessage\n\nA chat message representing information that should be instructions to the AI system.\n\n```typescript\nimport { SystemChatMessage } from \"langchain/schema\";\n\nnew SystemChatMessage(\"You are a nice assistant\");\n```\n\n## HumanChatMessage\n\nA chat message representing information coming from a human interacting with the AI system.\n\n```typescript\nimport { HumanChatMessage } from \"langchain/schema\";\n\nnew HumanChatMessage(\"Hello, how are you?\");\n```\n\n## AIChatMessage\n\nA chat message representing information coming from the AI system.\n\n```typescript\nimport { AIChatMessage } from \"langchain/schema\";\n\nnew AIChatMessage(\"I am doing well, thank you!\");\n```\n","metadata":{"source":"docs/docs/modules/schema/chat-messages.md","loc":{"lines":{"from":1,"to":39}}}}],["124",{"pageContent":"# Document\n\nLanguage models only know information about what they were trained on. In order to get them answer questions or summarize other information you have to pass it to the language model. Therefore, it is very important to have a concept of a document.\n\nA document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).\n\n```typescript\ninterface Document {\n  pageContent: string;\n  metadata: Record<string, any>;\n}\n```\n\n## Creating a Document\n\nYou can create a document object rather easily in LangChain with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\" });\n```\n\nYou can create one with metadata with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\", metadata: { source: \"1\" } });\n```\n\nAlso check out [Document Loaders](../indexes/document_loaders/) for a way to load documents from a variety of sources.\n","metadata":{"source":"docs/docs/modules/schema/document.md","loc":{"lines":{"from":1,"to":33}}}}],["125",{"pageContent":"---\n---\n\n# Examples\n\nExamples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.\n\n```typescript\ntype Example = Record<string, string>;\n```\n\n## Creating an Example\n\nYou can create an Example like this:\n\n```typescript\nconst example = {\n  input: \"foo\",\n  output: \"bar\",\n};\n```\n","metadata":{"source":"docs/docs/modules/schema/example.md","loc":{"lines":{"from":1,"to":22}}}}],["126",{"pageContent":"---\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Schema\n\nThis section speaks about interfaces that are used throughout the rest of the library.\n\n<DocCardList />\n","metadata":{"source":"docs/docs/modules/schema/index.mdx","loc":{"lines":{"from":1,"to":12}}}}],["127",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Events / Callbacks\n\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, [monitoring](./tracing), [streaming](../modules/models/llms/additional_functionality#streaming-responses), and other tasks.\n\nYou can subscribe to these events by using the `callbacks` argument available throughout the API. This method accepts a list of handler objects, which are expected to implement one or more of the methods described in the [API docs](../api/callbacks/interfaces/CallbackHandlerMethods).\n\n## Using an existing handler\n\nLangChain provides a few built-in handlers that you can use to get started. These are available in the `langchain/callbacks` module. The most basic handler is the `ConsoleCallbackHandler`, which simply logs all events to the console. In the future we will add more default handlers to the library. Note that when the `verbose` flag on the object is set to `true`, the `ConsoleCallbackHandler` will be invoked even without being explicitly passed in.\n\nimport ConsoleExample from \"@examples/callbacks/console_handler.ts\";\n\n<CodeBlock language=\"typescript\">{ConsoleExample}</CodeBlock>\n\n## Creating a one-off handler\n\nYou can create a one-off handler inline by passing a plain object to the `callbacks` argument. This object should implement the [`CallbackHandlerMethods`](../api/callbacks/interfaces/CallbackHandlerMethods) interface. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.\n\nimport StreamingExample from \"@examples/models/llm/llm_streaming.ts\";\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>\n\n## Using multiple handlers\n\nWe offer a method on the `CallbackManager` class that allows you to create a one-off handler. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.\n\nThis is a more complete example that passes a `CallbackManager` to a ChatModel, and LLMChain, a Tool, and an Agent.\n\nimport AgentExample from \"@examples/agents/streaming.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>\n\n## Creating a custom handler\n\nYou can also create your own handler by implementing the `CallbackHandler` interface. This is useful if you want to do something more complex than just logging to the console, eg. send the events to a logging service. As an example here is the implementation of the `ConsoleCallbackHandler`:\n\n```typescript\nimport { BaseCallbackHandler } from \"langchain/callbacks\";\n\nexport class MyCallbackHandler extends BaseCallbackHandler {\n  async handleChainStart(chain: { name: string }) {\n    console.log(`Entering new ${chain.name} chain...`);\n  }\n\n  async handleChainEnd(_output: ChainValues) {\n    console.log(\"Finished chain.\");\n  }\n\n  async handleAgentAction(action: AgentAction) {\n    console.log(action.log);\n  }\n\n  async handleToolEnd(output: string) {\n    console.log(output);\n  }\n\n  async handleText(text: string) {\n    console.log(text);\n  }\n\n  async handleAgentEnd(action: AgentFinish) {\n    console.log(action.log);\n  }\n}\n```\n\nYou could then use it as described in the [section](#using-an-existing-handler) above.\n","metadata":{"source":"docs/docs/production/callbacks.mdx","loc":{"lines":{"from":1,"to":70}}}}],["128",{"pageContent":"# Deployment\n\nYou've built your LangChain app and now you're looking to deploy it to production? You've come to the right place. This guide will walk you through the options you have for deploying your app, and the considerations you should make when doing so.\n\n## Overview\n\nLangChain is a library for building applications that use language models. It is not a web framework, and does not provide any built-in functionality for serving your app over the web. Instead, it provides a set of tools that you can integrate in your API or backend server.\n\nThere are a couple of high-level options for deploying your app:\n\n- Deploying to a VM or container\n  - Persistent filesystem means you can save and load files from disk\n  - Always-running process means you can cache some things in memory\n  - You can support long-running requests, such as WebSockets\n- Deploying to a serverless environment\n  - No persistent filesystem means you can load files from disk, but not save them for later\n  - Cold start means you can't cache things in memory and expect them to be cached between requests\n  - Function timeouts mean you can't support long-running requests, such as WebSockets\n\nSome other considerations include:\n\n- Do you deploy your backend and frontend together, or separately?\n- Do you deploy your backend co-located with your database, or separately?\n\nAs you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.\n\n## Deployment Options\n\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.\n\n### Deploying to Fly.io\n\n[Fly.io](https://fly.io) is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.\n\nSee [our Fly.io template](https://github.com/hwchase17/langchain-template-node-fly) for an example of how to deploy your app to Fly.io.\n","metadata":{"source":"docs/docs/production/deployment.md","loc":{"lines":{"from":1,"to":36}}}}],["129",{"pageContent":"# Tracing\n\nSimilar to the Python `langchain` package, JS `langchain` also supports tracing.\n\nYou can view an overview of tracing [here.](https://langchain.readthedocs.io/en/latest/tracing.html)\nTo spin up the tracing backend, run `docker compose up` (or `docker-compose up` if on using an older version of `docker`) in the `langchain` directory.\nYou can also use the `langchain-server` command if you have the python `langchain` package installed.\n\nHere's an example of how to use tracing in `langchain.js`. All that needs to be done is setting the `LANGCHAIN_TRACING` environment variable to `true`.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```\n\n## Concurrency\n\nTracing works with concurrency out of the box.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output} ${resultA.__run.runId}`);\n  console.log(`Got output ${resultB.output} ${resultB.__run.runId}`);\n  console.log(`Got output ${resultC.output} ${resultC.__run.runId}`);\n\n  /*\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. b8fb98aa-07a5-45bd-b593-e8d7376b05ca\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. c8d916d5-ca1d-4702-8dd7-cab5e438578b\n    Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. bf5fe04f-ef29-4e55-8ce1-e4aa974f9484\n    */\n};\n```\n","metadata":{"source":"docs/docs/production/tracing.md","loc":{"lines":{"from":1,"to":98}}}}],["130",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\n# Interacting with APIs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/apis)\n:::\n\nLots of data and information is stored behind APIs.\nThis page covers all resources available in LangChain for working with APIs.\n\n## Chains\n\nIf you are just getting started, and you have relatively apis, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\nTODO: add an API chain and then add an example here.\n\n## Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger and more complex schemas.\n\n- [OpenAPI Agent](../modules/agents/toolkits/examples/openapi.md)\n","metadata":{"source":"docs/docs/use_cases/api.mdx","loc":{"lines":{"from":1,"to":30}}}}],["131",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Personal Assistants\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/personal-assistants)\n:::\n\nWe use \"personal assistant\" here in a very broad sense.\nPersonal assistants have a few characteristics:\n\n- They can interact with the outside world\n- They have knowledge of your data\n- They remember your interactions\n\nReally all of the functionality in LangChain is relevant for building a personal assistant.\nHighlighting specific parts:\n\n- [Agent Documentation](../modules/agents/index.mdx) (for interacting with the outside world)\n- [Index Documentation](../modules/indexes/index.mdx) (for giving them knowledge of your data)\n- [Memory](../modules/memory/index.mdx) (for helping them remember interactions)\n","metadata":{"source":"docs/docs/use_cases/personal_assistants.mdx","loc":{"lines":{"from":1,"to":25}}}}],["132",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\n# Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-docs)\n:::\n\nQuestion answering in this context refers to question answering over your document data.\nThere are a few different types of question answering:\n\n- [Retrieval Question Answering](../modules/chains/index_related_chains/retrieval_qa): Use this to ingest documents, index them into a vectorstore, and then be able to ask questions about it.\n- [Chat Retrieval](../modules/chains/index_related_chains/conversational_retrieval): Similar to above in that you ingest and index documents, but this lets you have more a conversation (ask follow up questions, etc) rather than just asking one-off questions.\n\n## Indexing\n\nFor question answering over many documents, you almost always want to create an index over the data.\nThis can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).\n\nTherefor, it is really important to understand how to create indexes, and so you should familiarize yourself with all the documentation related to that.\n\n- [Indexes](../modules/indexes/index.mdx)\n\n## Chains\n\nAfter you create an index, you can then use it in a chain.\nYou can just do normal question answering over it, or you can use it a conversational way.\nFor an overview of these chains (and more) see the below documentation.\n\n- [Index related chains](../modules/chains/index_related_chains/index.mdx)\n\n## Agents\n\nIf you want to be able to answer more complex, multi-hop questions you should look into combining your indexes with an agent.\nFor an example of how to do that, please see the below.\n\n- [Vectorstore Agent](../modules/agents/toolkits/examples/vectorstore.md)\n","metadata":{"source":"docs/docs/use_cases/question_answering.mdx","loc":{"lines":{"from":1,"to":41}}}}],["133",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 6\n---\n\n# Summarization\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/summarization)\n:::\n\nA common use case is wanting to summarize long documents.\nThis naturally runs into the context window limitations.\nUnlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything).\nSo what do you do then?\n\nTo get started, we would recommend checking out the summarization chain which attacks this problem in a recursive manner.\n\n- [Summarization Chain](../modules/chains/other_chains/summarization)\n","metadata":{"source":"docs/docs/use_cases/summarization.mdx","loc":{"lines":{"from":1,"to":20}}}}],["134",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\n# Tabular Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-tabular)\n:::\n\nLots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.\nThis page covers all resources available in LangChain for working with data in this format.\n\n## Chains\n\nIf you are just getting started, and you have relatively small/simple tabular data, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\n- [SQL Database Chain](../modules/chains/other_chains/sql)\n\n## Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger databases and more complex schemas.\n\n- [SQL Agent](../modules/agents/toolkits/examples/sql.mdx)\n","metadata":{"source":"docs/docs/use_cases/tabular.mdx","loc":{"lines":{"from":1,"to":30}}}}]]