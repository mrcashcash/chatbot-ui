[{"pageContent":"# langchain-examples\n\nThis folder contains examples of how to use LangChain.\n\n## Run an example\n\nWhat you'll usually want to do.\n\nFirst, build langchain. From the repository root, run:\n\n```sh\nyarn\nyarn build\n```\n\nMost examples require API keys. Run `cp .env.example .env`, then edit `.env` with your API keys.\n\nThen from the `examples/` directory, run:\n\n`yarn run start <path to example>`\n\neg.\n\n`yarn run start ./src/prompts/few_shot.ts`\n\n## Run an example with the transpiled JS\n\nYou shouldn't need to do this, but if you want to run an example with the transpiled JS, you can do so with:\n\n`yarn run start:dist <path to example>`\n\neg.\n\n`yarn run start:dist ./dist/prompts/few_shot.js`\n","metadata":{"source":"examples/src/README.md"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport {\n  RequestsGetTool,\n  RequestsPostTool,\n  AIPluginTool,\n} from \"langchain/tools\";\n\nexport const run = async () => {\n  const tools = [\n    new RequestsGetTool(),\n    new RequestsPostTool(),\n    await AIPluginTool.fromPluginUrl(\n      \"https://www.klarna.com/.well-known/ai-plugin.json\"\n    ),\n  ];\n  const agent = await initializeAgentExecutorWithOptions(\n    tools,\n    new ChatOpenAI({ temperature: 0 }),\n    { agentType: \"chat-zero-shot-react-description\", verbose: true }\n  );\n\n  const result = await agent.call({\n    input: \"what t shirts are available in klarna?\",\n  });\n\n  console.log({ result });\n};\n","metadata":{"source":"examples/src/agents/aiplugin-tool.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  // Passing \"chat-conversational-react-description\" as the agent type\n  // automatically creates and uses BufferMemory with the executor.\n  // If you would like to override this, you can pass in a custom\n  // memory option, but the memoryKey set on it must be \"chat_history\".\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-conversational-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input0 = \"hi, i am bob\";\n\n  const result0 = await executor.call({ input: input0 });\n\n  console.log(`Got output ${result0.output}`);\n\n  const input1 = \"whats my name?\";\n\n  const result1 = await executor.call({ input: input1 });\n\n  console.log(`Got output ${result1.output}`);\n\n  const input2 = \"whats the weather in pomfret?\";\n\n  const result2 = await executor.call({ input: input2 });\n\n  console.log(`Got output ${result2.output}`);\n};\n","metadata":{"source":"examples/src/agents/chat_convo_with_tracing.ts"}},{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-zero-shot-react-description\",\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n","metadata":{"source":"examples/src/agents/chat_mrkl.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-zero-shot-react-description\",\n    returnIntermediateSteps: true,\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n","metadata":{"source":"examples/src/agents/chat_mrkl_with_tracing.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output} ${resultA.__run.runId}`);\n  console.log(`Got output ${resultB.output} ${resultB.__run.runId}`);\n  console.log(`Got output ${resultC.output} ${resultC.__run.runId}`);\n};\n","metadata":{"source":"examples/src/agents/concurrent_mrkl.ts"}},{"pageContent":"import { AgentExecutor, ZeroShotAgent } from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const prefix = `Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:`;\n  const suffix = `Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\n\nQuestion: {input}\n{agent_scratchpad}`;\n\n  const createPromptArgs = {\n    suffix,\n    prefix,\n    inputVariables: [\"input\", \"agent_scratchpad\"],\n  };\n\n  const prompt = ZeroShotAgent.createPrompt(tools, createPromptArgs);\n\n  const llmChain = new LLMChain({ llm: model, prompt });\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: [\"search\", \"calculator\"],\n  });\n  const agentExecutor = AgentExecutor.fromAgentAndTools({ agent, tools });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await agentExecutor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/custom_agent.ts"}},{"pageContent":"import {\n  LLMSingleActionAgent,\n  AgentActionOutputParser,\n  AgentExecutor,\n} from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport {\n  BasePromptTemplate,\n  BaseStringPromptTemplate,\n  SerializedBasePromptTemplate,\n  renderTemplate,\n} from \"langchain/prompts\";\nimport {\n  InputValues,\n  PartialValues,\n  AgentStep,\n  AgentAction,\n  AgentFinish,\n} from \"langchain/schema\";\nimport { SerpAPI, Tool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nconst formatInstructions = (toolNames: string) => `Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [${toolNames}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\nconst SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;\n\nclass CustomPromptTemplate extends BaseStringPromptTemplate {\n  tools: Tool[];\n\n  constructor(args: { tools: Tool[]; inputVariables: string[] }) {\n    super({ inputVariables: args.inputVariables });\n    this.tools = args.tools;\n  }\n\n  _getPromptType(): string {\n    throw new Error(\"Not implemented\");\n  }\n\n  format(input: InputValues): Promise<string> {\n    /** Construct the final template */\n    const toolStrings = this.tools\n      .map((tool) => `${tool.name}: ${tool.description}`)\n      .join(\"\\n\");\n    const toolNames = this.tools.map((tool) => tool.name).join(\"\\n\");\n    const instructions = formatInstructions(toolNames);\n    const template = [PREFIX, toolStrings, instructions, SUFFIX].join(\"\\n\\n\");\n    /** Construct the agent_scratchpad */\n    const intermediateSteps = input.intermediate_steps as AgentStep[];\n    const agentScratchpad = intermediateSteps.reduce(\n      (thoughts, { action, observation }) =>\n        thoughts +\n        [action.log, `\\nObservation: ${observation}`, \"Thought:\"].join(\"\\n\"),\n      \"\"\n    );\n    const newInput = { agent_scratchpad: agentScratchpad, ...input };\n    /** Format the template. */\n    return Promise.resolve(renderTemplate(template, \"f-string\", newInput));\n  }\n\n  partial(_values: PartialValues): Promise<BasePromptTemplate> {\n    throw new Error(\"Not implemented\");\n  }\n\n  serialize(): SerializedBasePromptTemplate {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nclass CustomOutputParser extends AgentActionOutputParser {\n  async parse(text: string): Promise<AgentAction | AgentFinish> {\n    if (text.includes(\"Final Answer:\")) {\n      const parts = text.split(\"Final Answer:\");\n      const input = parts[parts.length - 1].trim();\n      const finalAnswers = { output: input };\n      return { log: text, returnValues: finalAnswers };\n    }\n\n    const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n    if (!match) {\n      throw new Error(`Could not parse LLM output: ${text}`);\n    }\n\n    return {\n      tool: match[1].trim(),\n      toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n      log: text,\n    };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const llmChain = new LLMChain({\n    prompt: new CustomPromptTemplate({\n      tools,\n      inputVariables: [\"input\", \"agent_scratchpad\"],\n    }),\n    llm: model,\n  });\n\n  const agent = new LLMSingleActionAgent({\n    llmChain,\n    outputParser: new CustomOutputParser(),\n    stop: [\"\\nObservation\"],\n  });\n  const executor = new AgentExecutor({\n    agent,\n    tools,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/custom_llm_agent.ts"}},{"pageContent":"import {\n  AgentActionOutputParser,\n  AgentExecutor,\n  LLMSingleActionAgent,\n} from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  BaseChatPromptTemplate,\n  BasePromptTemplate,\n  SerializedBasePromptTemplate,\n  renderTemplate,\n} from \"langchain/prompts\";\nimport {\n  AgentAction,\n  AgentFinish,\n  AgentStep,\n  BaseChatMessage,\n  HumanChatMessage,\n  InputValues,\n  PartialValues,\n} from \"langchain/schema\";\nimport { SerpAPI, Tool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nconst formatInstructions = (toolNames: string) => `Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [${toolNames}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\nconst SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;\n\nclass CustomPromptTemplate extends BaseChatPromptTemplate {\n  tools: Tool[];\n\n  constructor(args: { tools: Tool[]; inputVariables: string[] }) {\n    super({ inputVariables: args.inputVariables });\n    this.tools = args.tools;\n  }\n\n  _getPromptType(): string {\n    throw new Error(\"Not implemented\");\n  }\n\n  async formatMessages(values: InputValues): Promise<BaseChatMessage[]> {\n    /** Construct the final template */\n    const toolStrings = this.tools\n      .map((tool) => `${tool.name}: ${tool.description}`)\n      .join(\"\\n\");\n    const toolNames = this.tools.map((tool) => tool.name).join(\"\\n\");\n    const instructions = formatInstructions(toolNames);\n    const template = [PREFIX, toolStrings, instructions, SUFFIX].join(\"\\n\\n\");\n    /** Construct the agent_scratchpad */\n    const intermediateSteps = values.intermediate_steps as AgentStep[];\n    const agentScratchpad = intermediateSteps.reduce(\n      (thoughts, { action, observation }) =>\n        thoughts +\n        [action.log, `\\nObservation: ${observation}`, \"Thought:\"].join(\"\\n\"),\n      \"\"\n    );\n    const newInput = { agent_scratchpad: agentScratchpad, ...values };\n    /** Format the template. */\n    const formatted = renderTemplate(template, \"f-string\", newInput);\n    return [new HumanChatMessage(formatted)];\n  }\n\n  partial(_values: PartialValues): Promise<BasePromptTemplate> {\n    throw new Error(\"Not implemented\");\n  }\n\n  serialize(): SerializedBasePromptTemplate {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nclass CustomOutputParser extends AgentActionOutputParser {\n  async parse(text: string): Promise<AgentAction | AgentFinish> {\n    if (text.includes(\"Final Answer:\")) {\n      const parts = text.split(\"Final Answer:\");\n      const input = parts[parts.length - 1].trim();\n      const finalAnswers = { output: input };\n      return { log: text, returnValues: finalAnswers };\n    }\n\n    const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n    if (!match) {\n      throw new Error(`Could not parse LLM output: ${text}`);\n    }\n\n    return {\n      tool: match[1].trim(),\n      toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n      log: text,\n    };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const llmChain = new LLMChain({\n    prompt: new CustomPromptTemplate({\n      tools,\n      inputVariables: [\"input\", \"agent_scratchpad\"],\n    }),\n    llm: model,\n  });\n\n  const agent = new LLMSingleActionAgent({\n    llmChain,\n    outputParser: new CustomOutputParser(),\n    stop: [\"\\nObservation\"],\n  });\n  const executor = new AgentExecutor({\n    agent,\n    tools,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\nrun();\n","metadata":{"source":"examples/src/agents/custom_llm_agent_chat.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { DynamicTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new DynamicTool({\n      name: \"FOO\",\n      description:\n        \"call this to get the value of foo. input should be an empty string.\",\n      func: () =>\n        new Promise((resolve) => {\n          resolve(\"foo\");\n        }),\n    }),\n    new DynamicTool({\n      name: \"BAR\",\n      description:\n        \"call this to get the value of bar. input should be an empty string.\",\n      func: () =>\n        new Promise((resolve) => {\n          resolve(\"baz1\");\n        }),\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the value of foo?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/custom_tool.ts"}},{"pageContent":"import * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n","metadata":{"source":"examples/src/agents/json.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { AgentExecutor } from \"langchain/agents\";\nimport { loadAgent } from \"langchain/agents/load\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const agent = await loadAgent(\n    \"lc://agents/zero-shot-react-description/agent.json\",\n    { llm: model, tools }\n  );\n  console.log(\"Loaded agent from Langchain hub\");\n\n  const executor = AgentExecutor.fromAgentAndTools({\n    agent,\n    tools,\n    returnIntermediateSteps: true,\n  });\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/load_from_hub.ts"}},{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/mrkl.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { WebBrowser } from \"langchain/tools/webbrowser\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const embeddings = new OpenAIEmbeddings();\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n    new WebBrowser({ model, embeddings }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the word of the day on merriam webster. What is the top result on google for that word`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  /*\n  Entering new agent_executor chain...\n  I need to find the word of the day on Merriam Webster and then search for it on Google\n  Action: web-browser\n  Action Input: \"https://www.merriam-webster.com/word-of-the-day\", \"\"\n\n\n  Summary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \"lackadaisical\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\n\n  Relevant Links: \n  - [Test Your Vocabulary](https://www.merriam-webster.com/games)\n  - [Thesaurus](https://www.merriam-webster.com/thesaurus)\n  - [Word Finder](https://www.merriam-webster.com/wordfinder)\n  - [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\n  - [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=\n  I now need to search for the word of the day on Google\n  Action: search\n  Action Input: \"lackadaisical\"\n  lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...\n  Finished chain.\n  */\n\n  console.log(`Got output ${JSON.stringify(result, null, 2)}`);\n  /*\n  Got output {\n    \"output\": \"The word of the day on Merriam Webster is \\\"lackadaisical\\\", which implies a carefree indifference marked by half-hearted efforts.\",\n    \"intermediateSteps\": [\n      {\n        \"action\": {\n          \"tool\": \"web-browser\",\n          \"toolInput\": \"https://www.merriam-webster.com/word-of-the-day\\\", \",\n          \"log\": \" I need to find the word of the day on Merriam Webster and then search for it on Google\\nAction: web-browser\\nAction Input: \\\"https://www.merriam-webster.com/word-of-the-day\\\", \\\"\\\"\"\n        },\n        \"observation\": \"\\n\\nSummary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \\\"lackadaisical\\\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\\n\\nRelevant Links: \\n- [Test Your Vocabulary](https://www.merriam-webster.com/games)\\n- [Thesaurus](https://www.merriam-webster.com/thesaurus)\\n- [Word Finder](https://www.merriam-webster.com/wordfinder)\\n- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\\n- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=\"\n      },\n      {\n        \"action\": {\n          \"tool\": \"search\",\n          \"toolInput\": \"lackadaisical\",\n          \"log\": \" I now need to search for the word of the day on Google\\nAction: search\\nAction Input: \\\"lackadaisical\\\"\"\n        },\n        \"observation\": \"lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...\"\n      }\n    ]\n  }\n  */\n};\n","metadata":{"source":"examples/src/agents/mrkl_browser.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/mrkl_with_tracing.ts"}},{"pageContent":"import * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);\n\n  const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n","metadata":{"source":"examples/src/agents/openapi.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { createSqlAgent, SqlToolkit } from \"langchain/agents\";\nimport { DataSource } from \"typeorm\";\n\n/** This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nexport const run = async () => {\n  const datasource = new DataSource({\n    type: \"sqlite\",\n    database: \"Chinook.db\",\n  });\n  const db = await SqlDatabase.fromDataSourceParams({\n    appDataSource: datasource,\n  });\n  const toolkit = new SqlToolkit(db);\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createSqlAgent(model, toolkit);\n\n  const input = `List the total sales per country. Which country's customers spent the most?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n\n  await datasource.destroy();\n};\n","metadata":{"source":"examples/src/agents/sql.ts"}},{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { AgentExecutor, ZeroShotAgent } from \"langchain/agents\";\nimport { BaseCallbackHandler } from \"langchain/callbacks\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { AgentAction } from \"langchain/schema\";\n\nexport const run = async () => {\n  // You can implement your own callback handler by extending BaseCallbackHandler\n  class CustomHandler extends BaseCallbackHandler {\n    name = \"custom_handler\";\n\n    handleLLMNewToken(token: string) {\n      console.log(\"token\", { token });\n    }\n\n    handleLLMStart(llm: { name: string }, _prompts: string[]) {\n      console.log(\"handleLLMStart\", { llm });\n    }\n\n    handleChainStart(chain: { name: string }) {\n      console.log(\"handleChainStart\", { chain });\n    }\n\n    handleAgentAction(action: AgentAction) {\n      console.log(\"handleAgentAction\", action);\n    }\n\n    handleToolStart(tool: { name: string }) {\n      console.log(\"handleToolStart\", { tool });\n    }\n  }\n\n  const handler1 = new CustomHandler();\n\n  // Additionally, you can use the `fromMethods` method to create a callback handler\n  const handler2 = BaseCallbackHandler.fromMethods({\n    handleLLMStart(llm, _prompts: string[]) {\n      console.log(\"handleLLMStart: I'm the second handler!!\", { llm });\n    },\n    handleChainStart(chain) {\n      console.log(\"handleChainStart: I'm the second handler!!\", { chain });\n    },\n    handleAgentAction(action) {\n      console.log(\"handleAgentAction\", action);\n    },\n    handleToolStart(tool) {\n      console.log(\"handleToolStart\", { tool });\n    },\n  });\n\n  // You can restrict callbacks to a particular object by passing it upon creation\n  const model = new ChatOpenAI({\n    temperature: 0,\n    callbacks: [handler2], // this will issue handler2 callbacks related to this model\n    streaming: true, // needed to enable streaming, which enables handleLLMNewToken\n  });\n\n  const tools = [new Calculator()];\n  const agentPrompt = ZeroShotAgent.createPrompt(tools);\n\n  const llmChain = new LLMChain({\n    llm: model,\n    prompt: agentPrompt,\n    callbacks: [handler2], // this will issue handler2 callbacks related to this chain\n  });\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: [\"search\"],\n  });\n\n  const agentExecutor = AgentExecutor.fromAgentAndTools({\n    agent,\n    tools,\n  });\n\n  /*\n   * When we pass the callback handler to the agent executor, it will be used for all\n   * callbacks related to the agent and all the objects involved in the agent's\n   * execution, in this case, the Tool, LLMChain, and LLM.\n   *\n   * The `handler2` callback handler will only be used for callbacks related to the\n   * LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.\n   */\n  const result = await agentExecutor.call(\n    {\n      input: \"What is 2 to the power of 8\",\n    },\n    [handler1]\n  ); // this is needed to see handleAgentAction\n  /*\n  handleChainStart { chain: { name: 'agent_executor' } }\n  handleChainStart { chain: { name: 'llm_chain' } }\n  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }\n  handleLLMStart { llm: { name: 'openai' } }\n  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }\n  token { token: '' }\n  token { token: 'I' }\n  token { token: ' can' }\n  token { token: ' use' }\n  token { token: ' the' }\n  token { token: ' calculator' }\n  token { token: ' tool' }\n  token { token: ' to' }\n  token { token: ' solve' }\n  token { token: ' this' }\n  token { token: '.\\n' }\n  token { token: 'Action' }\n  token { token: ':' }\n  token { token: ' calculator' }\n  token { token: '\\n' }\n  token { token: 'Action' }\n  token { token: ' Input' }\n  token { token: ':' }\n  token { token: ' ' }\n  token { token: '2' }\n  token { token: '^' }\n  token { token: '8' }\n  token { token: '' }\n  handleAgentAction {\n    tool: 'calculator',\n    toolInput: '2^8',\n    log: 'I can use the calculator tool to solve this.\\n' +\n      'Action: calculator\\n' +\n      'Action Input: 2^8'\n  }\n  handleToolStart { tool: { name: 'calculator' } }\n  handleChainStart { chain: { name: 'llm_chain' } }\n  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }\n  handleLLMStart { llm: { name: 'openai' } }\n  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }\n  token { token: '' }\n  token { token: 'That' }\n  token { token: ' was' }\n  token { token: ' easy' }\n  token { token: '!\\n' }\n  token { token: 'Final' }\n  token { token: ' Answer' }\n  token { token: ':' }\n  token { token: ' ' }\n  token { token: '256' }\n  token { token: '' }\n  */\n\n  console.log(result);\n  /*\n  {\n    output: '256',\n    __run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }\n  }\n  */\n};\n","metadata":{"source":"examples/src/agents/streaming.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  console.log(docs);\n\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n  /* Create the agent */\n  const vectorStoreInfo: VectorStoreInfo = {\n    name: \"state_of_union_address\",\n    description: \"the most recent state of the Union address\",\n    vectorStore,\n  };\n\n  const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\n  const agent = createVectorStoreAgent(model, toolkit);\n\n  const input =\n    \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\n  console.log(`Executing: ${input}`);\n\n  const result = await agent.call({ input });\n  console.log(`Got output ${result.output}`);\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n","metadata":{"source":"examples/src/agents/vectorstore.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport {\n  initializeAgentExecutorWithOptions,\n  ZapierToolKit,\n} from \"langchain/agents\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const zapier = new ZapierNLAWrapper();\n  const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\n  const executor = await initializeAgentExecutorWithOptions(\n    toolkit.tools,\n    model,\n    {\n      agentType: \"zero-shot-react-description\",\n      verbose: true,\n    }\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n","metadata":{"source":"examples/src/agents/zapier_mrkl.ts"}},{"pageContent":"import { ConsoleCallbackHandler } from \"langchain/callbacks\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const handler = new ConsoleCallbackHandler();\n  const llm = new OpenAI({ temperature: 0, callbacks: [handler] });\n  const prompt = PromptTemplate.fromTemplate(\"1 + {number} =\");\n  const chain = new LLMChain({ prompt, llm, callbacks: [handler] });\n\n  await chain.call({ number: 2 });\n  /*\n  Entering new llm_chain chain...\n  Finished chain.\n  */\n};\n","metadata":{"source":"examples/src/callbacks/console_handler.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain, AnalyzeDocumentChain } from \"langchain/chains\";\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  // In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  const model = new OpenAI({ temperature: 0 });\n  const combineDocsChain = loadSummarizationChain(model);\n  const chain = new AnalyzeDocumentChain({\n    combineDocumentsChain: combineDocsChain,\n  });\n  const res = await chain.call({\n    input_document: text,\n  });\n  console.log({ res });\n  /*\n  {\n    res: {\n      text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.\n      He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.\n      The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/chains/analyze_document_chain_summarize.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// to run this first run chroma's docker-container with `docker-compose up -d --build`\n\nexport const run = async () => {\n  /* Initialize the LLM to use to answer the question */\n  const model = new OpenAI();\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n    collectionName: \"state_of_the_union\",\n  });\n  /* Create the chain */\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever()\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question, chat_history: [] });\n  console.log(res);\n  /* Ask it a follow up question */\n  const chatHistory = question + res.text;\n  const followUpRes = await chain.call({\n    question: \"Was that nice?\",\n    chat_history: chatHistory,\n  });\n  console.log(followUpRes);\n};\n","metadata":{"source":"examples/src/chains/chat_vector_db_chroma.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nexport const run = async () => {\n  const model = new OpenAI({});\n  const chain = new ConversationChain({ llm: model });\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1 });\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2 });\n};\n","metadata":{"source":"examples/src/chains/conversation_chain.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  /* Initialize the LLM to use to answer the question */\n  const model = new OpenAI({});\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n  /* Create the chain */\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever()\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question, chat_history: [] });\n  console.log(res);\n  /* Ask it a follow up question */\n  const chatHistory = question + res.text;\n  const followUpRes = await chain.call({\n    question: \"Was that nice?\",\n    chat_history: chatHistory,\n  });\n  console.log(followUpRes);\n};\n","metadata":{"source":"examples/src/chains/conversational_qa.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nexport const run = async () => {\n  // We can construct an LLMChain from a PromptTemplate and an LLM.\n  const model = new OpenAI({ temperature: 0 });\n  const template = \"What is a good name for a company that makes {product}?\";\n  const prompt = new PromptTemplate({ template, inputVariables: [\"product\"] });\n  const chainA = new LLMChain({ llm: model, prompt });\n  const resA = await chainA.call({ product: \"colorful socks\" });\n  // The result is an object with a `text` property.\n  console.log({ resA });\n  // { resA: { text: '\\n\\nSocktastic!' } }\n\n  // Since the LLMChain is a single-input, single-output chain, we can also call it with `run`.\n  // This takes in a string and returns the `text` property.\n  const resA2 = await chainA.run(\"colorful socks\");\n  console.log({ resA2 });\n  // { resA2: '\\n\\nSocktastic!' }\n\n  // We can also construct an LLMChain from a ChatPromptTemplate and a chat model.\n  const chat = new ChatOpenAI({ temperature: 0 });\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n  const chainB = new LLMChain({\n    prompt: chatPrompt,\n    llm: chat,\n  });\n  const resB = await chainB.call({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  console.log({ resB });\n  // { resB: { text: \"J'adore la programmation.\" } }\n};\n","metadata":{"source":"examples/src/chains/llm_chain.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\nexport const run = async () => {\n  const model = new OpenAI({\n    temperature: 0.9,\n    streaming: true,\n    callbacks: [\n      {\n        handleLLMNewToken(token: string) {\n          console.log({ token });\n        },\n      },\n    ],\n  });\n\n  const template = \"What is a good name for a company that makes {product}?\";\n  const prompt = new PromptTemplate({ template, inputVariables: [\"product\"] });\n  const chain = new LLMChain({ llm: model, prompt });\n  const res = await chain.call({ product: \"colorful socks\" });\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/chains/llm_chain_stream.ts"}},{"pageContent":"import { loadChain } from \"langchain/chains/load\";\n\nexport const run = async () => {\n  const chain = await loadChain(\"lc://chains/hello-world/chain.json\");\n  const res = chain.call({ topic: \"foo\" });\n  console.log(res);\n};\n","metadata":{"source":"examples/src/chains/load_from_hub.ts"}},{"pageContent":"import { loadQARefineChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Create the models and chain\n  const embeddings = new OpenAIEmbeddings();\n  const model = new OpenAI({ temperature: 0 });\n  const chain = loadQARefineChain(model);\n\n  // Load the documents and create the vector store\n  const loader = new TextLoader(\"./state_of_the_union.txt\");\n  const docs = await loader.loadAndSplit();\n  const store = await MemoryVectorStore.fromDocuments(docs, embeddings);\n\n  // Select the relevant documents\n  const question = \"What did the president say about Justice Breyer\";\n  const relevantDocs = await store.similaritySearch(question);\n\n  // Call the chain\n  const res = await chain.call({\n    input_documents: relevantDocs,\n    question,\n  });\n\n  console.log(res);\n  /*\n  {\n    output_text: '\\n' +\n      '\\n' +\n      \"The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act.\"\n  }\n  */\n}\n","metadata":{"source":"examples/src/chains/qa_refine.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadQAStuffChain, loadQAMapReduceChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\nexport const run = async () => {\n  // This first example uses the `StuffDocumentsChain`.\n  const llmA = new OpenAI({});\n  const chainA = loadQAStuffChain(llmA);\n  const docs = [\n    new Document({ pageContent: \"Harrison went to Harvard.\" }),\n    new Document({ pageContent: \"Ankush went to Princeton.\" }),\n  ];\n  const resA = await chainA.call({\n    input_documents: docs,\n    question: \"Where did Harrison go to college?\",\n  });\n  console.log({ resA });\n  // { resA: { text: ' Harrison went to Harvard.' } }\n\n  // This second example uses the `MapReduceChain`.\n  // Optionally limit the number of concurrent requests to the language model.\n  const llmB = new OpenAI({ maxConcurrency: 10 });\n  const chainB = loadQAMapReduceChain(llmB);\n  const resB = await chainB.call({\n    input_documents: docs,\n    question: \"Where did Harrison go to college?\",\n  });\n  console.log({ resB });\n  // { resB: { text: ' Harrison went to Harvard.' } }\n};\n","metadata":{"source":"examples/src/chains/question_answering.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadQAMapReduceChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const chain = loadQAMapReduceChain(model);\n  const docs = [\n    new Document({ pageContent: \"harrison went to harvard\" }),\n    new Document({ pageContent: \"ankush went to princeton\" }),\n  ];\n  const res = await chain.call({\n    input_documents: docs,\n    question: \"Where did harrison go to college\",\n  });\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/chains/question_answering_map_reduce.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  // Initialize the LLM to use to answer the question.\n  const model = new OpenAI({});\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n\n  // Create a vector store from the documents.\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n  // Create a chain that uses the OpenAI LLM and HNSWLib vector store.\n  const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());\n  const res = await chain.call({\n    query: \"What did the president say about Justice Breyer?\",\n  });\n  console.log({ res });\n  /*\n  {\n    res: {\n      text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,\n      and retiring Justice of the United States Supreme Court and thanked him for his service.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/chains/retrieval_qa.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain, loadQARefineChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Create a chain that uses a Refine chain and HNSWLib vector store.\nconst chain = new RetrievalQAChain({\n  combineDocumentsChain: loadQARefineChain(model),\n  retriever: vectorStore.asRetriever(),\n});\nconst res = await chain.call({\n  query: \"What did the president say about Justice Breyer?\",\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    output_text: '\\n' +\n      '\\n' +\n      \"The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery.\"\n  }\n}\n*/\n","metadata":{"source":"examples/src/chains/retrieval_qa_custom.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { RemoteLangChainRetriever } from \"langchain/retrievers/remote\";\n\nexport const run = async () => {\n  // Initialize the LLM to use to answer the question.\n  const model = new OpenAI({});\n\n  // Initialize the remote retriever.\n  const retriever = new RemoteLangChainRetriever({\n    url: \"http://0.0.0.0:8080/retrieve\", // Replace with your own URL.\n    auth: { bearer: \"foo\" }, // Replace with your own auth.\n    inputKey: \"message\",\n    responseKey: \"response\",\n  });\n\n  // Create a chain that uses the OpenAI LLM and remote retriever.\n  const chain = RetrievalQAChain.fromLLM(model, retriever);\n\n  // Call the chain with a query.\n  const res = await chain.call({\n    query: \"What did the president say about Justice Breyer?\",\n  });\n  console.log({ res });\n  /*\n  {\n    res: {\n      text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,\n      and retiring Justice of the United States Supreme Court and thanked him for his service.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/chains/retrieval_qa_with_remote.ts"}},{"pageContent":"import { SimpleSequentialChain, LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n// This is an LLMChain to write a synopsis given a title of a play.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n \n  Title: {title}\n  Playwright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\n  template,\n  inputVariables: [\"title\"],\n});\nconst synopsisChain = new LLMChain({ llm, prompt: promptTemplate });\n\n// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n \n  Play Synopsis:\n  {synopsis}\n  Review from a New York Times play critic of the above play:`;\nconst reviewPromptTemplate = new PromptTemplate({\n  template: reviewTemplate,\n  inputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\n  llm: reviewLLM,\n  prompt: reviewPromptTemplate,\n});\n\nconst overallChain = new SimpleSequentialChain({\n  chains: [synopsisChain, reviewChain],\n  verbose: true,\n});\nconst review = await overallChain.run(\"Tragedy at sunset on the beach\");\nconsole.log(review);\n/*\n    variable review contains the generated play review based on the input title and synopsis generated in the first step:\n\n    \"Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man. \n    The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears. \n    Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience.\"\n*/\n","metadata":{"source":"examples/src/chains/simple_sequential_chain.ts"}},{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains\";\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nexport const run = async () => {\n  const datasource = new DataSource({\n    type: \"sqlite\",\n    database: \"Chinook.db\",\n  });\n\n  const db = await SqlDatabase.fromDataSourceParams({\n    appDataSource: datasource,\n  });\n\n  const chain = new SqlDatabaseChain({\n    llm: new OpenAI({ temperature: 0 }),\n    database: db,\n  });\n\n  const res = await chain.run(\"How many tracks are there?\");\n  console.log(res);\n  // There are 3503 tracks.\n};\n","metadata":{"source":"examples/src/chains/sql_db.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\nexport const run = async () => {\n  const model = new OpenAI({});\n  const chain = loadSummarizationChain(model, { type: \"stuff\" });\n  const docs = [\n    new Document({ pageContent: \"harrison went to harvard\" }),\n    new Document({ pageContent: \"ankush went to princeton\" }),\n  ];\n  const res = await chain.call({\n    input_documents: docs,\n  });\n  console.log(res);\n};\n","metadata":{"source":"examples/src/chains/summarization.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  // In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  const model = new OpenAI({ temperature: 0 });\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n\n  // This convenience function creates a document chain prompted to summarize a set of documents.\n  const chain = loadSummarizationChain(model);\n  const res = await chain.call({\n    input_documents: docs,\n  });\n  console.log({ res });\n  /*\n  {\n    res: {\n      text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.\n      He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.\n      The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/chains/summarization_map_reduce.ts"}},{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ZeroShotAgent, AgentExecutor } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport {\n  ChatPromptTemplate,\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n  ];\n\n  const prompt = ZeroShotAgent.createPrompt(tools, {\n    prefix: `Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:`,\n    suffix: `Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"`,\n  });\n\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    new SystemMessagePromptTemplate(prompt),\n    HumanMessagePromptTemplate.fromTemplate(`{input}\n\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\n{agent_scratchpad}`),\n  ]);\n\n  const chat = new ChatOpenAI({});\n\n  const llmChain = new LLMChain({\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: tools.map((tool) => tool.name),\n  });\n\n  const executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\n  const response = await executor.run(\n    \"How many people live in canada as of 2023?\"\n  );\n\n  console.log(response);\n};\n","metadata":{"source":"examples/src/chat/agent.ts"}},{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ temperature: 0 });\n\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  const chain = new LLMChain({\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const response = await chain.call({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n\n  console.log(response);\n};\n","metadata":{"source":"examples/src/chat/llm_chain.ts"}},{"pageContent":"import { ConversationChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n  MessagesPlaceholder,\n} from \"langchain/prompts\";\nimport { BufferMemory } from \"langchain/memory\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ temperature: 0 });\n\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n    ),\n    new MessagesPlaceholder(\"history\"),\n    HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n  ]);\n\n  const chain = new ConversationChain({\n    memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const response = await chain.call({\n    input: \"hi! whats up?\",\n  });\n\n  console.log(response);\n};\n","metadata":{"source":"examples/src/chat/memory.ts"}},{"pageContent":"import { AgentExecutor, ChatAgent } from \"langchain/agents\";\nimport { ConversationChain, LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  MessagesPlaceholder,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { HumanChatMessage, SystemChatMessage } from \"langchain/schema\";\nimport { SerpAPI } from \"langchain/tools\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ temperature: 0 });\n\n  // Sending one message to the chat model, receiving one message back\n\n  let response = await chat.call([\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love programming.\"\n    ),\n  ]);\n\n  console.log(response);\n\n  // Sending an input made up of two messages to the chat model\n\n  response = await chat.call([\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\"Translate: I love programming.\"),\n  ]);\n\n  console.log(response);\n\n  // Sending two separate prompts in parallel, receiving two responses back\n\n  const responseA = await chat.generate([\n    [\n      new SystemChatMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanChatMessage(\n        \"Translate this sentence from English to French. I love programming.\"\n      ),\n    ],\n    [\n      new SystemChatMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanChatMessage(\n        \"Translate this sentence from English to French. I love artificial intelligence.\"\n      ),\n    ],\n  ]);\n\n  console.log(responseA);\n\n  // Using ChatPromptTemplate to encapsulate the reusable parts of the prompt\n\n  const translatePrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  const responseB = await chat.callPrompt(\n    await translatePrompt.formatPromptValue({\n      input_language: \"English\",\n      output_language: \"French\",\n      text: \"I love programming.\",\n    })\n  );\n\n  console.log(responseB);\n\n  // This pattern of asking for the completion of a formatted prompt is quite\n  // common, so we introduce the next piece of the puzzle: LLMChain\n\n  const translateChain = new LLMChain({\n    prompt: translatePrompt,\n    llm: chat,\n  });\n\n  const responseC = await translateChain.call({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n\n  console.log(responseC);\n\n  // Next up, stateful chains that remember the conversation history\n\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n    ),\n    new MessagesPlaceholder(\"history\"),\n    HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n  ]);\n\n  const chain = new ConversationChain({\n    memory: new BufferMemory({ returnMessages: true }),\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const responseE = await chain.call({\n    input: \"hi from London, how are you doing today\",\n  });\n\n  console.log(responseE);\n\n  const responseF = await chain.call({\n    input: \"Do you know where I am?\",\n  });\n\n  console.log(responseF);\n\n  // Finally, we introduce Tools and Agents, which extend the model with\n  // other abilities, such as search, or a calculator\n\n  // Define the list of tools the agent can use\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n  ];\n  // Create the agent from the chat model and the tools\n  const agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n  // Create an executor, which calls to the agent until an answer is found\n  const executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\n  const responseG = await executor.run(\n    \"How many people live in canada as of 2023?\"\n  );\n\n  console.log(responseG);\n};\n","metadata":{"source":"examples/src/chat/overview.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI(\n    { temperature: 0 },\n    {\n      basePath: \"https://oai.hconeai.com/v1\",\n    }\n  );\n  const res = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log(res);\n};\n","metadata":{"source":"examples/src/customParameters/differentBaseUrl.ts"}},{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nexport const run = async () => {\n  const loader = new CheerioWebBaseLoader(\n    \"https://news.ycombinator.com/item?id=34817881\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/cheerio_web.ts"}},{"pageContent":"import { CollegeConfidentialLoader } from \"langchain/document_loaders/web/college_confidential\";\n\nexport const run = async () => {\n  const loader = new CollegeConfidentialLoader(\n    \"https://www.collegeconfidential.com/colleges/brown-university/\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/college_confidential.ts"}},{"pageContent":"1\n00:00:17,580 --> 00:00:21,920\n<i>Corruption discovered\nat the core of the Banking Clan!</i>\n\n2\n00:00:21,950 --> 00:00:24,620\n<i>Reunited, Rush Clovis\nand Senator Amidala</i>\n\n3\n00:00:24,660 --> 00:00:27,830\n<i>discover the full extent\nof the deception.</i>\n\n4\n00:00:27,870 --> 00:00:30,960\n<i>Anakin Skywalker is sent to the rescue!</i>\n\n5\n00:00:31,000 --> 00:00:35,050\n<i>He refuses to trust Clovis and\nasks Padm not to work with him.</i>\n\n6\n00:00:35,090 --> 00:00:39,050\n<i>Determined to save the banks,\nshe refuses her husband's request,</i>\n\n7\n00:00:39,090 --> 00:00:42,800\n<i>throwing their\nrelationship into turmoil.</i>\n\n8\n00:00:42,840 --> 00:00:45,890\n<i>Voted for by both the\nSeparatists and the Republic,</i>\n\n9\n00:00:45,930 --> 00:00:50,260\n<i>Rush Clovis is elected new leader\nof the Galactic Banking Clan.</i>\n\n10\n00:00:50,310 --> 00:00:53,320\n<i>Now, all attention is focused on Scipio</i>\n\n11\n00:00:53,350 --> 00:00:56,350\n<i>as the important\ntransfer of power begins.</i>\n\n12\n00:01:20,410 --> 00:01:24,330\nWelcome back to Scipio, Rush Clovis.\n\n13\n00:01:24,370 --> 00:01:27,240\nOur Separatist government\nhas great hopes for you.\n\n14\n00:01:27,290 --> 00:01:30,080\nThank you, Senator.\n\n15\n00:01:30,120 --> 00:01:31,750\nOnly you and Senator Amidala\n\n16\n00:01:31,790 --> 00:01:34,330\nwill be allowed to monitor\nthe exchange proceedings.\n\n17\n00:01:34,380 --> 00:01:36,050\nNo forces on either side\n\n18\n00:01:36,080 --> 00:01:38,540\nwill be allowed\ninto the Neutral Zone.\n\n19\n00:01:38,590 --> 00:01:40,750\nSenator Amidala,\nwe will be right here\n\n20\n00:01:40,800 --> 00:01:41,850\nif you should need us.\n\n21\n00:01:41,880 --> 00:01:43,210\nThank you, Commander.\n\n22\n00:02:06,600 --> 00:02:09,190\nIt is with great disappointment\n\n23\n00:02:09,230 --> 00:02:13,020\nthat I implement\nthe following verdict.\n\n24\n00:02:13,070 --> 00:02:15,490\nBy decree of the Muun people,\n\n25\n00:02:15,530 --> 00:02:18,570\nthe five representatives\nstanding before me\n\n26\n00:02:18,610 --> 00:02:21,280\nare found guilty\nof embezzlement.\n\n27\n00:02:21,320 --> 00:02:24,450\nThey shall be imprisoned\nforthwith,\n\n28\n00:02:24,490 --> 00:02:27,660\nand control of the banks\nshall transfer immediately\n\n29\n00:02:27,700 --> 00:02:29,580\nto Rush Clovis\n\n30\n00:02:29,620 --> 00:02:33,080\nunder the guidance\nof the Muun government.\n\n31\n00:02:41,210 --> 00:02:43,250\nWe are grateful to you, Clovis,\n\n32\n00:02:43,290 --> 00:02:46,630\nfor everything you have done\nfor the Muun people.\n\n33\n00:02:46,670 --> 00:02:48,340\nTo have lost the banks\n\n34\n00:02:48,380 --> 00:02:51,010\nwould have been\nan historic disaster.\n\n35\n00:02:51,050 --> 00:02:52,510\nI would like you to know\n\n36\n00:02:52,550 --> 00:02:54,840\nI have no interest\nin controlling the banks.\n\n37\n00:02:54,880 --> 00:02:57,930\nI am simply here\nto reestablish order.\n\n38\n00:03:01,890 --> 00:03:06,060\nDo you think our friend\nis up to the task?\n\n39\n00:03:06,100 --> 00:03:07,850\nThere are few men\nI have met in my career\n\n40\n00:03:07,890 --> 00:03:10,680\nwho are more dedicated\nto a cause than Clovis.\n\n41\n00:03:10,730 --> 00:03:12,850\nOnce he decides\nwhat he is fighting for,\n\n42\n00:03:12,890 --> 00:03:15,360\nlittle will stop him\nfrom achieving it.\n\n43\n00:03:15,400 --> 00:03:17,520\nLet us hope you are right\n\n44\n00:03:17,570 --> 00:03:19,910\nfor all our sakes.\n\n45\n00:03:39,330 --> 00:03:41,540\nAh, Clovis.\n\n46\n00:03:41,580 --> 00:03:44,160\nHow are you liking\nyour new office?\n\n47\n00:03:44,210 --> 00:03:48,040\nI must say, you look very\ncomfortable behind that desk.\n\n48\n00:03:48,080 --> 00:03:51,080\nCount Dooku,\nwhat do I owe the pleasure?\n\n49\n00:03:51,130 --> 00:03:53,420\nCome, come, my boy.\n\n50\n00:03:53,460 --> 00:03:55,920\nYou don't think I'd let\nsuch an important day pass\n\n51\n00:03:55,960 --> 00:03:58,630\nwithout wishing you\nthe best of luck.\n\n52\n00:03:58,680 --> 00:04:01,930\nThank you, but luck\nhas nothing to do with it.\n\n53\n00:04:01,970 --> 00:04:04,260\nThe transfer has occurred\nwithout a hitch.\n\n54\n00:04:04,300 --> 00:04:06,010\nWell, of course it has.\n\n55\n00:04:06,050 --> 00:04:09,430\nThe Separatists are fully\nbehind your appointment.\n\n56\n00:04:09,470 --> 00:04:14,430\nAfter all, aren't we the ones\nwho put you there?\n\n57\n00:04:14,480 --> 00:04:16,100\nFor your support, I am grateful,\n\n58\n00:04:16,140 --> 00:04:17,480\nbut I now must lead\n\n59\n00:04:17,520 --> 00:04:21,270\nwithout allegiance\ntowards either side.\n\n60\n00:04:22,690 --> 00:04:24,570\nIs that so?\n\n61\n00:04:24,610 --> 00:04:28,030\nQuite the idealist you have become\nin so short a time.\n\n62\n00:04:28,070 --> 00:04:30,490\nWhat do you want, Dooku?\n\n63\n00:04:30,530 --> 00:04:32,700\nTo collect on my investment.\n\n64\n00:04:32,740 --> 00:04:34,620\nHow do you think the Republic\nwould like to know\n\n65\n00:04:34,660 --> 00:04:37,250\nthat it was I\nwho supplied Rush Clovis\n\n66\n00:04:37,280 --> 00:04:38,950\nwith all the information\nhe needed\n\n67\n00:04:38,990 --> 00:04:41,030\nto topple the leaders of the bank?\n\n68\n00:04:41,080 --> 00:04:42,910\nI will tell them myself.\n\n69\n00:04:42,950 --> 00:04:44,700\nOh, but you can't.\n\n70\n00:04:44,750 --> 00:04:46,800\nI put you in power.\n\n71\n00:04:46,830 --> 00:04:49,290\nYou belong to me,\n\n72\n00:04:49,330 --> 00:04:51,120\nand if you want to stay in control,\n\n73\n00:04:51,170 --> 00:04:52,840\nyou will do as I say.\n\n74\n00:04:52,880 --> 00:04:56,050\nThe banks will remain unbiased.\n\n75\n00:04:56,090 --> 00:04:57,850\nThen I'm afraid the Separatists\n\n76\n00:04:57,880 --> 00:05:01,260\nwill be unable to pay\nthe interest on our loans.\n\n77\n00:05:01,300 --> 00:05:03,300\nBut the banks will collapse,\nand then...\n\n78\n00:05:03,340 --> 00:05:06,840\nNot if you raise\ninterest rates on the Republic.\n\n79\n00:05:06,880 --> 00:05:07,970\nWhat?\n\n80\n00:05:08,010 --> 00:05:09,880\nYou know I can't do that.\n\n81\n00:05:09,930 --> 00:05:12,600\nOh, but you can, and you will,\n\n82\n00:05:12,640 --> 00:05:15,430\nor everything that you\nfought so hard for\n\n83\n00:05:15,470 --> 00:05:17,350\nwill be destroyed.\n\n84\n00:05:31,110 --> 00:05:33,860\nBy the new order\nof the Traxus Division\n\n85\n00:05:33,900 --> 00:05:36,240\nand in an attempt\nto stabilize the banks,\n\n86\n00:05:36,280 --> 00:05:39,450\nit is essential that interest\nrates on loans to the Republic\n\n87\n00:05:39,490 --> 00:05:41,910\nbe raised immediately.\n\n88\n00:05:41,950 --> 00:05:43,490\nWhat?\n\n89\n00:05:43,530 --> 00:05:44,950\nBut you can't do that!\n\n90\n00:05:44,990 --> 00:05:46,700\nClovis.\n\n91\n00:05:46,740 --> 00:05:47,910\nClovis!\n\n92\n00:05:47,950 --> 00:05:49,950\nWhat are you doing?\n\n93\n00:06:03,960 --> 00:06:05,670\nThis is an outrage!\n\n94\n00:06:05,710 --> 00:06:07,920\nWe warned you this would happen!\n\n95\n00:06:07,960 --> 00:06:10,260\nAnd what of the Separatists?\n\n96\n00:06:10,300 --> 00:06:11,760\nFrom the little information.\n\n97\n00:06:11,800 --> 00:06:14,550\nSenator Amidala\nhas been able to establish,\n\n98\n00:06:14,590 --> 00:06:18,430\nthere will be no raise\non their current loan.\n\n99\n00:06:18,470 --> 00:06:22,060\nI knew from the beginning\nthat Clovis would do this.\n\n100\n00:06:28,980 --> 00:06:31,270\nHmm, correct you might have been\n\n101\n00:06:31,310 --> 00:06:32,690\nabout Clovis.\n\n102\n00:06:32,730 --> 00:06:34,150\nIt's incredibly foolish\n\n103\n00:06:34,190 --> 00:06:36,360\nfor to make a move like this\nso early.\n\n104\n00:06:36,400 --> 00:06:39,440\nHe will turn the whole Republic\nagainst him.\n\n105\n00:06:39,480 --> 00:06:42,570\nNot clear to us\nare his objectives.\n\n106\n00:06:42,610 --> 00:06:44,820\nWant this he might.\n\n107\n00:06:44,860 --> 00:06:46,820\nSomething's wrong.\n\n108\n00:06:46,860 --> 00:06:48,450\nThis doesn't make sense.\n\n109\n00:06:48,490 --> 00:06:51,950\nI would like\nto call for restraint\n\n110\n00:06:51,990 --> 00:06:55,740\nand allow us time\nto analyze the situation.\n\n111\n00:07:12,630 --> 00:07:14,760\nYou may begin your attack.\n\n112\n00:07:14,800 --> 00:07:17,420\nIt is time to make Rush Clovis\n\n113\n00:07:17,470 --> 00:07:19,800\nlook like a powerful Separatist.\n\n114\n00:07:19,840 --> 00:07:21,840\nRight away, sir.\n\n115\n00:07:28,390 --> 00:07:29,930\nIt looks like\nan invasion fleet, sir.\n\n116\n00:07:29,970 --> 00:07:31,970\nWe're caught out here\nin the open.\n\n117\n00:07:36,650 --> 00:07:39,740\nGet the men off this landing pad\nand beyond the city gates!\n\n118\n00:07:51,070 --> 00:07:53,450\nSenator Amidala,\ncome in, please.\n\n119\n00:07:53,490 --> 00:07:55,280\nWhat is it, Commander Thorn?\n\n120\n00:07:55,320 --> 00:07:57,490\nWe're under attack\nby the Separatist garrison.\n\n121\n00:07:57,530 --> 00:07:59,240\nLooks to be a full invasion.\n\n122\n00:07:59,280 --> 00:08:00,660\nInvasion?\n\n123\n00:08:00,700 --> 00:08:02,240\nWe can't get to you.\n\n124\n00:08:02,290 --> 00:08:05,160\nI suggest you get to a ship\nas soon as you can.\n\n125\n00:08:09,250 --> 00:08:10,290\nBoom!\n\n126\n00:08:14,420 --> 00:08:15,670\nAhh!\n\n127\n00:08:28,640 --> 00:08:29,760\nLet's move!\n\n128\n00:08:29,800 --> 00:08:31,050\nHurry!\n\n129\n00:08:54,740 --> 00:08:55,740\nAh!\n\n130\n00:08:59,360 --> 00:09:01,280\nFor the Republic!\n\n131\n00:09:04,370 --> 00:09:05,620\nAh!\n\n132\n00:09:34,300 --> 00:09:37,180\nOur garrison has been attacked\nby the Separatists,\n\n133\n00:09:37,220 --> 00:09:39,760\nand it appears they are staging\nan invasion of Scipio.\n\n134\n00:09:39,810 --> 00:09:41,220\nAn invasion?\n\n135\n00:09:41,270 --> 00:09:43,190\nWhat do they hope to achieve?\n\n136\n00:09:43,230 --> 00:09:45,860\nWith this news, the Senate\nwill vote immediately\n\n137\n00:09:45,890 --> 00:09:47,230\nto attack Scipio.\n\n138\n00:09:47,270 --> 00:09:50,230\nIt appears war has already\ncome to Scipio.\n\n139\n00:09:50,270 --> 00:09:52,440\nI want you off that planet\nimmediately.\n\n140\n00:09:52,480 --> 00:09:53,940\nI can't.\n\n141\n00:09:53,980 --> 00:09:56,270\nSurely you can get to a ship.\n\n142\n00:09:56,320 --> 00:09:59,570\nGeneral Skywalker,\nI'm afraid I'm trapped.\n\n143\n00:10:03,240 --> 00:10:04,240\nLet me go!\n\n144\n00:10:05,700 --> 00:10:07,700\nInvoke an emergency meeting\nof the Senate.\n\n145\n00:10:07,740 --> 00:10:09,700\nThere is no time to lose.\n\n146\n00:10:11,740 --> 00:10:13,240\nI feel it is only right\n\n147\n00:10:13,290 --> 00:10:15,990\nthat you should handle\nthis matter, my boy.\n\n148\n00:10:16,040 --> 00:10:18,200\nA lot will be entrusted to you.\n\n149\n00:10:26,420 --> 00:10:28,130\nDon't touch me!\n\n150\n00:10:29,880 --> 00:10:30,920\nWhat have you done to her?\n\n151\n00:10:32,170 --> 00:10:34,840\nClovis, what is going on?\n\n152\n00:10:34,880 --> 00:10:36,880\nI didn't want this, Padm.\n\n153\n00:10:36,930 --> 00:10:39,090\nWhy don't you tell her\nwhat you did want\n\n154\n00:10:39,140 --> 00:10:41,940\nand how you got it.\n\n155\n00:10:41,970 --> 00:10:43,260\nDooku.\n\n156\n00:10:46,600 --> 00:10:48,720\nPadm, this is not what it seems.\n\n157\n00:10:48,770 --> 00:10:51,060\nHasn't she joined our cause?\n\n158\n00:10:51,100 --> 00:10:54,140\nClovis here told me\nhow instrumental you were\n\n159\n00:10:54,190 --> 00:10:55,350\nin getting him to power.\n\n160\n00:10:55,400 --> 00:10:56,410\nIf I had known...\n\n161\n00:10:56,440 --> 00:10:57,810\nEither you are with us\n\n162\n00:10:57,860 --> 00:10:59,530\nor you are against us.\n\n163\n00:10:59,570 --> 00:11:00,740\nArrest her!\n\n164\n00:11:00,770 --> 00:11:02,440\nWe can't do this, Dooku.\n\n165\n00:11:02,480 --> 00:11:05,110\nThe Separatist Senate\nwill never approve.\n\n166\n00:11:06,280 --> 00:11:07,280\nHey!\n\n167\n00:11:11,990 --> 00:11:13,530\nNo. No.\n\n168\n00:11:13,570 --> 00:11:14,620\nNo!\n\n169\n00:11:16,580 --> 00:11:17,590\nNo!\n\n170\n00:11:19,660 --> 00:11:20,830\nAre you insane?\n\n171\n00:11:20,870 --> 00:11:22,750\nThis was not part of the deal.\n\n172\n00:11:22,790 --> 00:11:24,250\nWhat deal?\n\n173\n00:11:24,290 --> 00:11:26,250\nWhat have you done here, Clovis?\n\n174\n00:11:26,290 --> 00:11:28,250\nHe's given us the banks.\n\n175\n00:11:28,290 --> 00:11:29,670\nGone are our debts,\n\n176\n00:11:29,710 --> 00:11:33,500\nand gone is any credit\nfor the Republic.\n\n177\n00:11:33,540 --> 00:11:37,130\nAll of your idealism\nwas just a front.\n\n178\n00:11:37,170 --> 00:11:39,050\nThere was nothing I could do.\n\n179\n00:11:39,090 --> 00:11:42,880\nEveryone has their price,\nmy dear.\n\n180\n00:11:49,890 --> 00:11:52,050\nIt is with grave news\n\n181\n00:11:52,100 --> 00:11:54,100\nI come before you.\n\n182\n00:11:54,140 --> 00:11:57,350\nCount Dooku and his\nSeparatist betrayers\n\n183\n00:11:57,390 --> 00:11:59,850\nhave manipulated us, my friends.\n\n184\n00:11:59,890 --> 00:12:02,230\nThe war must go to Scipio!\n\n185\n00:12:02,270 --> 00:12:04,900\nClovis has been\ntheir puppet of deceit\n\n186\n00:12:04,940 --> 00:12:09,150\nas the Separatists are\ncurrently invading Scipio.\n\n187\n00:12:09,190 --> 00:12:11,990\nWe must stop them\nand secure the planet!\n\n188\n00:12:12,030 --> 00:12:15,110\nWe have handed\nthe entire economic system\n\n189\n00:12:15,150 --> 00:12:17,030\nover to Count Dooku.\n\n190\n00:12:17,070 --> 00:12:18,700\nWe are doomed!\n\n191\n00:12:18,740 --> 00:12:19,870\nInvade!\n\n192\n00:12:23,240 --> 00:12:26,450\nAs Supreme Chancellor,\nI must abide\n\n193\n00:12:26,490 --> 00:12:28,910\nby the consensus of the Senate.\n\n194\n00:12:28,960 --> 00:12:32,170\nWe shall commence\na mercy mission to Scipio\n\n195\n00:12:32,210 --> 00:12:36,080\nto be led by\nGeneral Anakin Skywalker.\n\n196\n00:12:36,130 --> 00:12:39,890\nThe banks will be secured\nat all costs,\n\n197\n00:12:39,920 --> 00:12:43,170\nand the Republic\nwill not crumble!\n\n198\n00:12:44,860 --> 00:12:45,856\nVictory!\n\n199\n00:12:45,880 --> 00:12:48,800\nWe will take victory.\n\n200\n00:12:48,840 --> 00:12:50,760\nWar on Scipio!\n\n201\n00:12:53,300 --> 00:12:55,600\nGreat emotions\nyou will find on Scipio,\n\n202\n00:12:55,640 --> 00:12:59,560\nwill you not?\n\n203\n00:12:59,600 --> 00:13:02,310\nI am worried\nfor Senator Amidala.\n\n204\n00:13:02,350 --> 00:13:03,890\nI'm afraid we may be too late.\n\n205\n00:13:03,940 --> 00:13:06,530\nCorrect you were about Clovis,\n\n206\n00:13:06,560 --> 00:13:10,190\nbut let go of your selfishness\nyou must\n\n207\n00:13:10,230 --> 00:13:12,520\nif you are to see clearly.\n\n208\n00:13:12,570 --> 00:13:16,230\nNot all is as it seems.\n\n209\n00:13:16,280 --> 00:13:18,790\nI understand, Master.\n\n210\n00:13:45,460 --> 00:13:48,750\nLord Tyranus, the Republic fleet\n\n211\n00:13:48,800 --> 00:13:51,300\nwill be arriving shortly.\n\n212\n00:13:51,340 --> 00:13:52,970\nVery good, my lord.\n\n213\n00:13:53,010 --> 00:13:55,800\nClovis has blindly\nplayed his part.\n\n214\n00:13:55,840 --> 00:13:57,970\nIt now appears he coordinated\n\n215\n00:13:58,010 --> 00:14:00,970\nthe entire Separatist takeover.\n\n216\n00:14:01,010 --> 00:14:03,510\nAnd because of this treachery,\n\n217\n00:14:03,560 --> 00:14:06,640\nthe banks will be firmly placed\n\n218\n00:14:06,680 --> 00:14:11,140\nunder the control\nof the Supreme Chancellor.\n\n219\n00:14:24,110 --> 00:14:26,440\nWhy are you doing this?\n\n220\n00:14:26,490 --> 00:14:28,990\nYou wouldn't understand.\n\n221\n00:14:29,030 --> 00:14:30,860\nI had to strike a deal\nwith Dooku,\n\n222\n00:14:30,910 --> 00:14:31,860\nbut don't worry.\n\n223\n00:14:31,910 --> 00:14:33,570\nI am the one in control.\n\n224\n00:14:33,620 --> 00:14:35,320\nAs soon as things\nhave settled down,\n\n225\n00:14:35,370 --> 00:14:38,240\nI can get rid of him,\nand I'll control it all again.\n\n226\n00:14:38,290 --> 00:14:39,450\nListen to yourself.\n\n227\n00:14:39,500 --> 00:14:41,260\nThe Republic is sending\nits armada\n\n228\n00:14:41,290 --> 00:14:43,040\nto take back the banks.\n\n229\n00:14:43,080 --> 00:14:46,710\nYou've brought war\nright where there cannot be war.\n\n230\n00:14:46,750 --> 00:14:48,330\nYour actions\nhave destroyed the banks\n\n231\n00:14:48,380 --> 00:14:50,220\nonce and for all!\n\n232\n00:15:00,720 --> 00:15:03,680\nRex, have you gotten a fix\non Senator Amidala's position?\n\n233\n00:15:03,720 --> 00:15:06,390\nWe'll have a better lock\nonce we get near the city,\n\n234\n00:15:06,430 --> 00:15:09,010\nbut initial scans suggest\nshe's still alive, sir.\n\n235\n00:15:09,060 --> 00:15:10,560\nGood.\n\n236\n00:15:10,600 --> 00:15:12,100\nHawk, we're gonna need\nair support\n\n237\n00:15:12,140 --> 00:15:13,220\nonce we're on the ground.\n\n238\n00:15:13,270 --> 00:15:14,430\nYou'll have it, General.\n\n239\n00:15:14,480 --> 00:15:16,560\nMe and the boys\nare ready to fly.\n\n240\n00:15:52,420 --> 00:15:53,670\nMy Lord,\n\n241\n00:15:53,710 --> 00:15:56,040\nwe have fully engaged\nRepublic forces,\n\n242\n00:15:56,090 --> 00:15:58,550\nbut we are suffering\nheavy losses.\n\n243\n00:15:58,590 --> 00:16:01,050\nWe have accomplished\nwhat we came here for.\n\n244\n00:16:01,090 --> 00:16:02,960\nIt is time to withdraw.\n\n245\n00:16:03,010 --> 00:16:06,090\nBut sir, our forces\nare still engaged\n\n246\n00:16:06,130 --> 00:16:08,300\nin battle on the planet.\n\n247\n00:16:08,340 --> 00:16:09,680\nLeave them.\n\n248\n00:16:09,720 --> 00:16:12,090\nAs you wish, Count Dooku.\n\n249\n00:16:29,110 --> 00:16:32,690\nSir, a Republic attack fleet\nhas just entered orbit\n\n250\n00:16:32,730 --> 00:16:34,110\nand is approaching the city.\n\n251\n00:16:36,070 --> 00:16:37,780\nGet me Count Dooku.\n\n252\n00:16:37,820 --> 00:16:41,200\nIt appears Count Dooku\nhas left the planet's surface.\n\n253\n00:16:41,240 --> 00:16:42,740\nWhat?\n\n254\n00:16:42,780 --> 00:16:45,700\nAnd the Separatist forces\nare in full retreat.\n\n255\n00:16:45,740 --> 00:16:47,740\nWe are alone.\n\n256\n00:17:16,800 --> 00:17:19,180\nRex, hold the droid forces here.\n\n257\n00:17:19,220 --> 00:17:20,930\nI'm gonna push on and get Padm.\n\n258\n00:17:20,970 --> 00:17:21,970\nCopy that.\n\n259\n00:17:34,520 --> 00:17:37,270\nSuch plans I had.\n\n260\n00:17:37,320 --> 00:17:41,660\nYou know, I've spent so much\nof my life misunderstood.\n\n261\n00:17:41,690 --> 00:17:43,860\nWhat will they say about me now?\n\n262\n00:17:43,900 --> 00:17:46,150\nWhat will I have left behind?\n\n263\n00:17:46,200 --> 00:17:49,200\nClovis, you have to\nturn yourself in.\n\n264\n00:17:58,910 --> 00:18:00,620\nIt's over, Clovis.\n\n265\n00:18:11,840 --> 00:18:13,300\nStay away from me!\n\n266\n00:18:13,340 --> 00:18:14,920\nI didn't do anything wrong!\n\n267\n00:18:14,960 --> 00:18:16,630\nYou have to believe me!\n\n268\n00:18:16,670 --> 00:18:19,010\nYou don't want to do this.\n\n269\n00:18:19,050 --> 00:18:20,760\nYou don't understand.\n\n270\n00:18:20,800 --> 00:18:22,300\nYou've all been deceived.\n\n271\n00:18:22,340 --> 00:18:23,720\nYeah, by you.\n\n272\n00:18:23,760 --> 00:18:24,890\nNo!\n\n273\n00:18:24,930 --> 00:18:25,930\nBy Dooku.\n\n274\n00:18:27,560 --> 00:18:29,070\nI'm not the villain here.\n\n275\n00:18:29,100 --> 00:18:31,310\nTell him, Padm.\n\n276\n00:18:31,350 --> 00:18:32,640\nLet me go, Clovis.\n\n277\n00:19:12,710 --> 00:19:15,540\nI can't hold both of you.\n\n278\n00:19:16,960 --> 00:19:18,590\nLet me go.\n\n279\n00:19:18,630 --> 00:19:20,500\nNo, Anakin, don't.\n\n280\n00:19:24,720 --> 00:19:26,260\nTry and climb.\n\n281\n00:19:28,720 --> 00:19:30,180\nI am!\n\n282\n00:19:30,220 --> 00:19:32,010\nI'm losing you!\n\n283\n00:19:33,010 --> 00:19:34,890\nI'm sorry, Padm.\n\n284\n00:19:36,640 --> 00:19:37,720\nNo.\n\n285\n00:19:51,900 --> 00:19:53,190\nIt's okay.\n\n286\n00:19:53,240 --> 00:19:54,410\nYou're okay.\n\n287\n00:19:54,440 --> 00:19:56,530\nI'm sorry, Anakin.\n\n288\n00:19:56,570 --> 00:19:58,150\nI'm sorry.\n\n289\n00:19:58,200 --> 00:19:59,950\nIt's over now.\n\n290\n00:19:59,990 --> 00:20:01,570\nIt's all over now.\n\n291\n00:20:06,830 --> 00:20:09,200\nIt is clear to the Banking Clan\n\n292\n00:20:09,250 --> 00:20:12,510\nit was Rush Clovis who was\nbehind the corruption\n\n293\n00:20:12,540 --> 00:20:14,960\nthat almost caused our collapse.\n\n294\n00:20:15,000 --> 00:20:17,120\nIn hope of a better tomorrow,\n\n295\n00:20:17,170 --> 00:20:19,790\nwe cede control of the banks\n\n296\n00:20:19,840 --> 00:20:23,810\nto the office of the Chancellor\nof the Galactic Republic.\n\n297\n00:20:26,800 --> 00:20:30,340\nIt is with great humility\n\n298\n00:20:30,380 --> 00:20:34,680\nthat I take on\nthis immense responsibility.\n\n299\n00:20:34,720 --> 00:20:38,010\nRest assured,\nwhen the Clone Wars end,\n\n300\n00:20:38,050 --> 00:20:40,060\nI shall reinstate the banks\n\n301\n00:20:40,100 --> 00:20:42,220\nas we once knew them,\n\n302\n00:20:42,270 --> 00:20:46,270\nbut during these\ntreacherous times,\n\n303\n00:20:46,310 --> 00:20:48,890\nwe cannot in good conscience\nallow our money\n\n304\n00:20:48,940 --> 00:20:50,940\nto fall under the manipulations\n\n305\n00:20:50,980 --> 00:20:53,900\nof a madman like Count Dooku\n\n306\n00:20:53,940 --> 00:20:56,020\nor Separatist control again.\n\n307\n00:21:00,030 --> 00:21:04,240\nMay there be prosperity\nand stability\n\n308\n00:21:04,280 --> 00:21:06,320\nin all our Republic lands.\n\n309\n00:21:06,360 --> 00:21:11,070\nMay our people be free and safe.\n\n310\n00:21:11,120 --> 00:21:12,240\nLong live the banks!\n\n311\n00:21:13,660 --> 00:21:15,450\n<i>Long live the banks!</i>\n\n312\n00:21:15,500 --> 00:21:17,380\n<i>Long live the banks!</i>\n\n313\n00:21:17,410 --> 00:21:19,450\n<i>Long live the banks!</i>\n\n314\n00:21:19,500 --> 00:21:23,500\n<i>Long live the banks!\nLong live the banks!</i>\n\n315\n00:21:23,540 --> 00:21:25,330\n<i>Long live the banks!</i>\n\n316\n00:21:25,380 --> 00:21:29,130\n<i>Long live the banks!\nLong live the banks!</i>\n\n","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"}},{"pageContent":"Foo\nBar\nBaz\n\n","metadata":{"source":"examples/src/document_loaders/example_data/example.txt"}},{"pageContent":"# Testing the notion markdownloader\n\n# 🦜️🔗 LangChain.js\n\n⚡ Building applications with LLMs through composability ⚡\n\n**Production Support:** As you move your LangChains into production, we'd love to offer more comprehensive support.\nPlease fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.\n\n## Quick Install\n\n`yarn add langchain`\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n## 🤔 What is this?\n\nLarge language models (LLMs) are emerging as a transformative technology, enabling\ndevelopers to build applications that they previously could not.\nBut using these LLMs in isolation is often not enough to\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\n\nThis library is aimed at assisting in the development of those types of applications.\n\n## Relationship with Python LangChain\n\nThis is built to integrate as seamlessly as possible with the [LangChain Python package](https://github.com/hwchase17/langchain). Specifically, this means all objects (prompts, LLMs, chains, etc) are designed in a way where they can be serialized and shared between languages.\n\nThe [LangChainHub](https://github.com/hwchase17/langchain-hub) is a central place for the serialized versions of these prompts, chains, and agents.\n\n## 📖 Documentation\n\nFor full documentation of prompts, chains, agents and more, please see [here](https://hwchase17.github.io/langchainjs/docs/overview).\n\n## 💁 Contributing\n\nAs an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.\n\nCheck out [our contributing guidelines](CONTRIBUTING.md) for instructions on how to contribute.\n","metadata":{"source":"examples/src/document_loaders/example_data/notion.md"}},{"pageContent":"import { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nexport const run = async () => {\n  const loader = new GitbookLoader(\"https://docs.gitbook.com\");\n  const docs = await loader.load(); // load single path\n  console.log(docs);\n  const allPathsLoader = new GitbookLoader(\"https://docs.gitbook.com\", {\n    shouldLoadAllPaths: true,\n  });\n  const docsAllPaths = await allPathsLoader.load(); // loads all paths of the given gitbook\n  console.log(docsAllPaths);\n};\n","metadata":{"source":"examples/src/document_loaders/gitbook.ts"}},{"pageContent":"import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nexport const run = async () => {\n  const loader = new GithubRepoLoader(\n    \"https://github.com/hwchase17/langchainjs\",\n    { branch: \"main\", recursive: false, unknown: \"warn\" }\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/github.ts"}},{"pageContent":"import { HNLoader } from \"langchain/document_loaders/web/hn\";\n\nexport const run = async () => {\n  const loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/hn.ts"}},{"pageContent":"import { IMSDBLoader } from \"langchain/document_loaders/web/imsdb\";\n\nexport const run = async () => {\n  const loader = new IMSDBLoader(\n    \"https://imsdb.com/scripts/BlacKkKlansman.html\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/imsdb.ts"}},{"pageContent":"import { NotionLoader } from \"langchain/document_loaders/fs/notion\";\n\nexport const run = async () => {\n  /** Provide the directory path of your notion folder */\n  const directoryPath = \"Notion_DB\";\n  const loader = new NotionLoader(directoryPath);\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/notion_markdown.ts"}},{"pageContent":"import { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\nexport const run = async () => {\n  const loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\n  /**  Loader use evaluate function ` await page.evaluate(() => document.body.innerHTML);` as default evaluate */\n  const docs = await loader.load();\n  console.log({ docs });\n\n  const loaderWithOptions = new PuppeteerWebBaseLoader(\n    \"https://www.tabnews.com.br/\",\n    {\n      launchOptions: {\n        headless: true,\n      },\n      gotoOptions: {\n        waitUntil: \"domcontentloaded\",\n      },\n      /**  Pass custom evaluate , in this case you get page and browser instances */\n      async evaluate(page, browser) {\n        await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n        const result = await page.evaluate(() => document.body.innerHTML);\n        await browser.close();\n        return result;\n      },\n    }\n  );\n  const docsFromLoaderWithOptions = await loaderWithOptions.load();\n  console.log({ docsFromLoaderWithOptions });\n};\n","metadata":{"source":"examples/src/document_loaders/puppeteer_web.ts"}},{"pageContent":"import { S3Loader } from \"langchain/document_loaders/web/s3\";\n\nconst loader = new S3Loader({\n  bucket: \"my-document-bucket-123\",\n  key: \"AccountingOverview.pdf\",\n  s3Config: {\n    region: \"us-east-1\",\n    accessKeyId: \"AKIAIOSFODNN7EXAMPLE\",\n    secretAccessKey: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n  },\n  unstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);\n","metadata":{"source":"examples/src/document_loaders/s3.ts"}},{"pageContent":"import { SRTLoader } from \"langchain/document_loaders/fs/srt\";\n\nexport const run = async () => {\n  const loader = new SRTLoader(\n    \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/srt.ts"}},{"pageContent":"import { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nexport const run = async () => {\n  const loader = new TextLoader(\n    \"src/document_loaders/example_data/example.txt\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/text.ts"}},{"pageContent":"import { UnstructuredLoader } from \"langchain/document_loaders/fs/unstructured\";\n\nexport const run = async () => {\n  const loader = new UnstructuredLoader(\n    \"http://localhost:8000/general/v0/general\",\n    \"src/document_loaders/example_data/notion.md\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};\n","metadata":{"source":"examples/src/document_loaders/unstructured.ts"}},{"pageContent":"import { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nexport const run = async () => {\n  const model = new CohereEmbeddings();\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/embeddings/cohere.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIEmbeddings({\n    maxConcurrency: 1,\n  });\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/embeddings/max_concurrency.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIEmbeddings();\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/embeddings/openai.ts"}},{"pageContent":"import { AutoGPT } from \"langchain/experimental/autogpt\";\nimport { ReadFileTool, WriteFileTool, SerpAPI } from \"langchain/tools\";\nimport { NodeFileStore } from \"langchain/stores/file/node\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst store = new NodeFileStore();\n\nconst tools = [\n  new ReadFileTool({ store }),\n  new WriteFileTool({ store }),\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"San Francisco,California,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n\nconst vectorStore = new HNSWLib(new OpenAIEmbeddings(), {\n  space: \"cosine\",\n  numDimensions: 1536,\n});\n\nconst autogpt = AutoGPT.fromLLMAndTools(\n  new ChatOpenAI({ temperature: 0 }),\n  tools,\n  {\n    memory: vectorStore.asRetriever(),\n    aiName: \"Tom\",\n    aiRole: \"Assistant\",\n  }\n);\n\nawait autogpt.run([\"write a weather report for SF today\"]);\n/*\n{\n    \"thoughts\": {\n        \"text\": \"I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.\",\n        \"reasoning\": \"I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.\",\n        \"plan\": \"- Use the search command to find the current weather conditions for SF\\n- Write a weather report based on the information found\",\n        \"criticism\": \"I need to make sure that the information I find is accurate and up-to-date.\",\n        \"speak\": \"I will use the search command to find the current weather conditions for SF.\"\n    },\n    \"command\": {\n        \"name\": \"search\",\n        \"args\": {\n            \"input\": \"current weather conditions San Francisco\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have found the current weather conditions for SF. I need to write a weather report based on this information.\",\n        \"reasoning\": \"I have the information I need to write a weather report, so I should use the write_file command to save it to a file.\",\n        \"plan\": \"- Use the write_file command to save the weather report to a file\",\n        \"criticism\": \"I need to make sure that the weather report is clear and concise.\",\n        \"speak\": \"I will use the write_file command to save the weather report to a file.\"\n    },\n    \"command\": {\n        \"name\": \"write_file\",\n        \"args\": {\n            \"file_path\": \"weather_report.txt\",\n            \"text\": \"San Francisco Weather Report:\\n\\nMorning: 53°, Chance of Rain 1%\\nAfternoon: 59°, Chance of Rain 0%\\nEvening: 52°, Chance of Rain 3%\\nOvernight: 48°, Chance of Rain 2%\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have completed all my objectives. I will use the finish command to signal that I am done.\",\n        \"reasoning\": \"I have completed the task of writing a weather report for SF today, so I don't need to do anything else.\",\n        \"plan\": \"- Use the finish command to signal that I am done\",\n        \"criticism\": \"I need to make sure that I have completed all my objectives before using the finish command.\",\n        \"speak\": \"I will use the finish command to signal that I am done.\"\n    },\n    \"command\": {\n        \"name\": \"finish\",\n        \"args\": {\n            \"response\": \"I have completed all my objectives.\"\n        }\n    }\n}\n*/\n","metadata":{"source":"examples/src/experimental/autogpt/weather.ts"}},{"pageContent":"import { AutoGPT } from \"langchain/experimental/autogpt\";\nimport { ReadFileTool, WriteFileTool, SerpAPI } from \"langchain/tools\";\nimport { InMemoryFileStore } from \"langchain/stores/file/in_memory\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst store = new InMemoryFileStore();\n\nconst tools = [\n  new ReadFileTool({ store }),\n  new WriteFileTool({ store }),\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"San Francisco,California,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n\nconst autogpt = AutoGPT.fromLLMAndTools(\n  new ChatOpenAI({ temperature: 0 }),\n  tools,\n  {\n    memory: vectorStore.asRetriever(),\n    aiName: \"Tom\",\n    aiRole: \"Assistant\",\n  }\n);\n\nawait autogpt.run([\"write a weather report for SF today\"]);\n/*\n{\n    \"thoughts\": {\n        \"text\": \"I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.\",\n        \"reasoning\": \"I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.\",\n        \"plan\": \"- Use the search command to find the current weather conditions for SF\\n- Write a weather report based on the information found\",\n        \"criticism\": \"I need to make sure that the information I find is accurate and up-to-date.\",\n        \"speak\": \"I will use the search command to find the current weather conditions for SF.\"\n    },\n    \"command\": {\n        \"name\": \"search\",\n        \"args\": {\n            \"input\": \"current weather conditions San Francisco\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have found the current weather conditions for SF. I need to write a weather report based on this information.\",\n        \"reasoning\": \"I have the information I need to write a weather report, so I should use the write_file command to save it to a file.\",\n        \"plan\": \"- Use the write_file command to save the weather report to a file\",\n        \"criticism\": \"I need to make sure that the weather report is clear and concise.\",\n        \"speak\": \"I will use the write_file command to save the weather report to a file.\"\n    },\n    \"command\": {\n        \"name\": \"write_file\",\n        \"args\": {\n            \"file_path\": \"weather_report.txt\",\n            \"text\": \"San Francisco Weather Report:\\n\\nMorning: 53°, Chance of Rain 1%\\nAfternoon: 59°, Chance of Rain 0%\\nEvening: 52°, Chance of Rain 3%\\nOvernight: 48°, Chance of Rain 2%\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have completed all my objectives. I will use the finish command to signal that I am done.\",\n        \"reasoning\": \"I have completed the task of writing a weather report for SF today, so I don't need to do anything else.\",\n        \"plan\": \"- Use the finish command to signal that I am done\",\n        \"criticism\": \"I need to make sure that I have completed all my objectives before using the finish command.\",\n        \"speak\": \"I will use the finish command to signal that I am done.\"\n    },\n    \"command\": {\n        \"name\": \"finish\",\n        \"args\": {\n            \"response\": \"I have completed all my objectives.\"\n        }\n    }\n}\n*/\n","metadata":{"source":"examples/src/experimental/autogpt/weather_browser.ts"}},{"pageContent":"import path from \"path\";\nimport url from \"url\";\n\nconst [exampleName, ...args] = process.argv.slice(2);\n\nif (!exampleName) {\n  console.error(\"Please provide path to example to run\");\n  process.exit(1);\n}\n\n// Allow people to pass all possible variations of a path to an example\n// ./src/foo.ts, ./dist/foo.js, src/foo.ts, dist/foo.js, foo.ts\nlet exampleRelativePath = exampleName;\n\nif (exampleRelativePath.startsWith(\"./examples/\")) {\n  exampleRelativePath = exampleName.slice(11);\n} else if (exampleRelativePath.startsWith(\"examples/\")) {\n  exampleRelativePath = exampleName.slice(9);\n}\n\nif (exampleRelativePath.startsWith(\"./src/\")) {\n  exampleRelativePath = exampleRelativePath.slice(6);\n} else if (exampleRelativePath.startsWith(\"./dist/\")) {\n  exampleRelativePath = exampleRelativePath.slice(7);\n} else if (exampleRelativePath.startsWith(\"src/\")) {\n  exampleRelativePath = exampleRelativePath.slice(4);\n} else if (exampleRelativePath.startsWith(\"dist/\")) {\n  exampleRelativePath = exampleRelativePath.slice(5);\n}\n\nlet runExample;\ntry {\n  ({ run: runExample } = await import(\n    path.join(\n      path.dirname(url.fileURLToPath(import.meta.url)),\n      exampleRelativePath\n    )\n  ));\n} catch (e) {\n  throw new Error(`Could not load example ${exampleName}: ${e}`);\n}\n\nif (runExample) {\n  const maybePromise = runExample(args);\n\n  if (maybePromise instanceof Promise) {\n    maybePromise.catch((e) => {\n      console.error(`Example failed with:`);\n      console.error(e);\n      process.exit(1);\n    });\n  }\n}\n","metadata":{"source":"examples/src/index.ts"}},{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nexport const run = async () => {\n  const text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\n    This is a weird text to write, but gotta test the splittingggg some how.\\n\\n\n    Bye!\\n\\n-H.`;\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 10,\n    chunkOverlap: 1,\n  });\n  const output = splitter.createDocuments([text]);\n  console.log(output);\n};\n","metadata":{"source":"examples/src/indexes/recursive_text_splitter.ts"}},{"pageContent":"import { Document } from \"langchain/document\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\n\nexport const run = async () => {\n  /* Split text */\n  const text = \"foo bar baz 123\";\n  const splitter = new CharacterTextSplitter({\n    separator: \" \",\n    chunkSize: 7,\n    chunkOverlap: 3,\n  });\n  const output = splitter.createDocuments([text]);\n  console.log({ output });\n  /* Split documents */\n  const docOutput = splitter.splitDocuments([\n    new Document({ pageContent: text }),\n  ]);\n  console.log({ docOutput });\n};\n","metadata":{"source":"examples/src/indexes/text_splitter.ts"}},{"pageContent":"import { Document } from \"langchain/document\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\nimport fs from \"fs\";\nimport path from \"path\";\n\nexport const run = async () => {\n  /* Split text */\n  const text = fs.readFileSync(\n    path.resolve(__dirname, \"../../state_of_the_union.txt\"),\n    \"utf8\"\n  );\n\n  const splitter = new TokenTextSplitter({\n    encodingName: \"r50k_base\",\n    chunkSize: 10,\n    chunkOverlap: 0,\n    allowedSpecial: [\"<|endoftext|>\"],\n    disallowedSpecial: [],\n  });\n\n  const output = splitter.createDocuments([text]);\n  console.log({ output });\n\n  const docOutput = splitter.splitDocuments([\n    new Document({ pageContent: text }),\n  ]);\n\n  console.log({ docOutput });\n};\n","metadata":{"source":"examples/src/indexes/token_text_splitter.ts"}},{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await HNSWLib.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib.ts"}},{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nexport const run = async () => {\n  // Create docs with a loader\n  const loader = new TextLoader(\n    \"src/document_loaders/example_data/example.txt\"\n  );\n  const docs = await loader.load();\n\n  // Load the docs into the vector store\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n  // Search for the most similar document\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_fromdocs.ts"}},{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  // Create a vector store through any method, here from texts as an example\n  const vectorStore = await HNSWLib.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  // Save the vector store to a directory\n  const directory = \"your/directory/here\";\n  await vectorStore.save(directory);\n\n  // Load the vector store from the same directory\n  const loadedVectorStore = await HNSWLib.load(\n    directory,\n    new OpenAIEmbeddings()\n  );\n\n  // vectorStore and loadedVectorStore are identical\n\n  const result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\n  console.log(result);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_saveload.ts"}},{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await MemoryVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/memory.ts"}},{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { similarity } from \"ml-distance\";\n\nexport const run = async () => {\n  const vectorStore = await MemoryVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    { similarity: similarity.pearson }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/memory_custom_similarity.ts"}},{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nexport const run = async () => {\n  // Create docs with a loader\n  const loader = new TextLoader(\n    \"src/document_loaders/example_data/example.txt\"\n  );\n  const docs = await loader.load();\n\n  // Load the docs into the vector store\n  const vectorStore = await MemoryVectorStore.fromDocuments(\n    docs,\n    new OpenAIEmbeddings()\n  );\n\n  // Search for the most similar document\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/memory_fromdocs.ts"}},{"pageContent":"import { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await Milvus.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/milvus.ts"}},{"pageContent":"import { MongoVectorStore } from \"langchain/vectorstores/mongo\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { MongoClient } from \"mongodb\";\n\nexport const run = async () => {\n  const client = new MongoClient(process.env.MONGO_URI || \"\");\n\n  const collection = client.db(\"langchain\").collection(\"test\");\n\n  await MongoVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new CohereEmbeddings(),\n    {\n      client,\n      collection,\n      // indexName: \"default\", // make sure that this matches the index name in atlas if not using \"default\"\n    }\n  );\n\n  // remember to close the client\n  await client.close();\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/mongo_fromTexts.ts"}},{"pageContent":"import { MongoVectorStore } from \"langchain/vectorstores/mongo\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { MongoClient } from \"mongodb\";\n\nexport const run = async () => {\n  const client = new MongoClient(process.env.MONGO_URI || \"\");\n\n  const collection = client.db(\"langchain\").collection(\"test\");\n\n  const vectorStore = new MongoVectorStore(new CohereEmbeddings(), {\n    client,\n    collection,\n    // indexName: \"default\", // make sure that this matches the index name in atlas if not using \"default\"\n  });\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n\n  console.log(resultOne);\n\n  // remember to close the client\n  await client.close();\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/mongo_search.ts"}},{"pageContent":"# Reference:\n#   https://opensearch.org/docs/latest/install-and-configure/install-opensearch/docker/#sample-docker-composeyml\nversion: '3'\nservices:\n  opensearch:\n    image: opensearchproject/opensearch:2.6.0\n    container_name: opensearch\n    environment:\n      - cluster.name=opensearch\n      - node.name=opensearch\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true\n      - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"DISABLE_INSTALL_DEMO_CONFIG=true\"\n      - \"DISABLE_SECURITY_PLUGIN=true\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - opensearch_data:/usr/share/opensearch/data\n    ports:\n      - 9200:9200\n      - 9600:9600\n    networks:\n      - opensearch\n  opensearch-dashboards:\n    image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes\n    container_name: opensearch-dashboards\n    ports:\n      - 5601:5601 # Map host port 5601 to container port 5601\n    expose:\n      - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards\n    environment:\n      OPENSEARCH_HOSTS: '[\"http://opensearch:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query\n      DISABLE_SECURITY_DASHBOARDS_PLUGIN: \"true\" # disables security dashboards plugin in OpenSearch Dashboards\n    networks:\n      - opensearch\nnetworks:\n  opensearch:\nvolumes:\n  opensearch_data:","metadata":{"source":"examples/src/indexes/vector_stores/opensearch/docker-compose.yml"}},{"pageContent":"import { Client } from \"@opensearch-project/opensearch\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\n\nexport async function run() {\n  const client = new Client({\n    nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n  });\n\n  const vectorStore = await OpenSearchVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      indexName: \"documents\",\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n  console.log(resultOne);\n}\n","metadata":{"source":"examples/src/indexes/vector_stores/opensearch/opensearch.ts"}},{"pageContent":"import { PineconeClient } from \"@pinecone-database/pinecone\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\n// To run this example, first [create a Pinecone index](https://app.pinecone.io/organizations)\n// It must have 1536 dimensions, to match the OpenAI embedding size.\n// It should use the metric \"cosine\" to get the results below.\n// Point to this index from your .env.\n\nexport const run = async () => {\n  if (\n    !process.env.PINECONE_API_KEY ||\n    !process.env.PINECONE_ENVIRONMENT ||\n    !process.env.PINECONE_INDEX\n  ) {\n    throw new Error(\n      \"PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set\"\n    );\n  }\n\n  const client = new PineconeClient();\n  await client.init({\n    apiKey: process.env.PINECONE_API_KEY,\n    environment: process.env.PINECONE_ENVIRONMENT,\n  });\n  const index = client.Index(process.env.PINECONE_INDEX);\n\n  const vectorStore = await PineconeStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }],\n    new OpenAIEmbeddings(),\n    { pineconeIndex: index }\n  );\n\n  /* Without metadata filtering */\n  let result = await vectorStore.similaritySearchWithScore(\"Hello world\", 3);\n  console.dir(result, { depth: null });\n  /*\n  [\n    [\n      Document { pageContent: 'Hello world', metadata: { foo: 'bar' } },\n      1\n    ],\n    [\n      Document {\n        pageContent: 'hello nice world',\n        metadata: { foo: 'qux' }\n      },\n      0.939860761\n    ],\n    [\n      Document { pageContent: 'Bye bye', metadata: { foo: 'baz' } },\n      0.827194452\n    ]\n  ]\n  */\n\n  /* With metadata filtering */\n  result = await vectorStore.similaritySearchWithScore(\"Hello world\", 3, {\n    foo: \"bar\",\n  });\n  console.dir(result, { depth: null });\n  /*\n  [\n    [\n      Document { pageContent: 'Hello world', metadata: { foo: 'bar' } },\n      0.999995887\n    ]\n  ]\n  */\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/pinecone.ts"}},{"pageContent":"# Add DATABASE_URL to .env file in this directory\nDATABASE_URL=postgresql://[USERNAME]:[PASSWORD]@[ADDR]/[DBNAME]","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/.env.example"}},{"pageContent":"data\ndocker-compose.yml","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/.gitignore"}},{"pageContent":"services:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - ./data:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=\n      - POSTGRES_USER=\n      - POSTGRES_DB=","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/docker-compose.example.yml"}},{"pageContent":"import { PrismaVectorStore } from \"langchain/vectorstores/prisma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PrismaClient, Prisma, Document } from \"@prisma/client\";\n\nexport const run = async () => {\n  const db = new PrismaClient();\n\n  const vectorStore = PrismaVectorStore.withModel<Document>(db).create(\n    new OpenAIEmbeddings(),\n    {\n      prisma: Prisma,\n      tableName: \"Document\",\n      vectorColumnName: \"vector\",\n      columns: {\n        id: PrismaVectorStore.IdColumn,\n        content: PrismaVectorStore.ContentColumn,\n      },\n    }\n  );\n\n  const texts = [\"Hello world\", \"Bye bye\", \"What's this?\"];\n  await vectorStore.addModels(\n    await db.$transaction(\n      texts.map((content) => db.document.create({ data: { content } }))\n    )\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n  console.log(resultOne.at(0)?.metadata.content);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma.ts"}},{"pageContent":"-- CreateTable\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE TABLE \"Document\" (\n    \"id\" TEXT NOT NULL,\n    \"content\" TEXT NOT NULL,\n    \"vector\" vector,\n\n    CONSTRAINT \"Document_pkey\" PRIMARY KEY (\"id\")\n);\n","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/migrations/00_init/migration.sql"}},{"pageContent":"# Please do not edit this file manually\n# It should be added in your version-control system (i.e. Git)\nprovider = \"postgresql\"","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/migrations/migration_lock.toml"}},{"pageContent":"// This is your Prisma schema file,\n// learn more about it in the docs: https://pris.ly/d/prisma-schema\n\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\nmodel Document {\n  id      String                 @id @default(cuid())\n  content String\n  vector  Unsupported(\"vector\")?\n}\n","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/schema.prisma"}},{"pageContent":"import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const vectorStore = await SupabaseVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      tableName: \"documents\",\n      queryName: \"match_documents\",\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n\n  console.log(resultOne);\n};\n","metadata":{"source":"examples/src/indexes/vector_stores/supabase.ts"}},{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Something wrong with the weaviate-ts-client types, so we need to disable\n  const client = (weaviate as any).client({\n    scheme: process.env.WEAVIATE_SCHEME || \"https\",\n    host: process.env.WEAVIATE_HOST || \"localhost\",\n    apiKey: new (weaviate as any).ApiKey(\n      process.env.WEAVIATE_API_KEY || \"default\"\n    ),\n  });\n\n  // Create a store and fill it with some texts + metadata\n  await WeaviateStore.fromTexts(\n    [\"hello world\", \"hi there\", \"how are you\", \"bye now\"],\n    [{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }, { foo: \"bar\" }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      indexName: \"Test\",\n      textKey: \"text\",\n      metadataKeys: [\"foo\"],\n    }\n  );\n}\n","metadata":{"source":"examples/src/indexes/vector_stores/weaviate_fromTexts.ts"}},{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Something wrong with the weaviate-ts-client types, so we need to disable\n  const client = (weaviate as any).client({\n    scheme: process.env.WEAVIATE_SCHEME || \"https\",\n    host: process.env.WEAVIATE_HOST || \"localhost\",\n    apiKey: new (weaviate as any).ApiKey(\n      process.env.WEAVIATE_API_KEY || \"default\"\n    ),\n  });\n\n  // Create a store for an existing index\n  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {\n    client,\n    indexName: \"Test\",\n    metadataKeys: [\"foo\"],\n  });\n\n  // Search the index without any filters\n  const results = await store.similaritySearch(\"hello world\", 1);\n  console.log(results);\n  /*\n  [ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]\n  */\n\n  // Search the index with a filter, in this case, only return results where\n  // the \"foo\" metadata key is equal to \"baz\", see the Weaviate docs for more\n  // https://weaviate.io/developers/weaviate/api/graphql/filters\n  const results2 = await store.similaritySearch(\"hello world\", 1, {\n    where: {\n      operator: \"Equal\",\n      path: [\"foo\"],\n      valueText: \"baz\",\n    },\n  });\n  console.log(results2);\n  /*\n  [ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]\n  */\n}\n","metadata":{"source":"examples/src/indexes/vector_stores/weaviate_search.ts"}},{"pageContent":"import { Cohere } from \"langchain/llms/cohere\";\n\nexport const run = async () => {\n  const model = new Cohere({\n    temperature: 0.7,\n    maxTokens: 20,\n    maxRetries: 5,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/llms/cohere.ts"}},{"pageContent":"import { HuggingFaceInference } from \"langchain/llms/hf\";\n\nexport const run = async () => {\n  const model = new HuggingFaceInference({\n    model: \"gpt2\",\n    temperature: 0.7,\n    maxTokens: 50,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/llms/hf.ts"}},{"pageContent":"import { OpenAIChat } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIChat({\n    prefixMessages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant that answers in pirate language\",\n      },\n    ],\n    maxTokens: 50,\n  });\n  const res = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/llms/openai-chat.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI({\n    modelName: \"gpt-4\",\n    temperature: 0.7,\n    maxTokens: 1000,\n    maxRetries: 5,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/llms/openai.ts"}},{"pageContent":"import { Replicate } from \"langchain/llms/replicate\";\n\nexport const run = async () => {\n  const model = new Replicate({\n    model:\n      \"replicate/flan-t5-xl:3ae0799123a1fe11f8c89fd99632f843fc5f7a761630160521c4253149754523\",\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/llms/replicate.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst memory = new BufferMemory({ memoryKey: \"chat_history\" });\nconst model = new OpenAI({ temperature: 0.9 });\nconst prompt =\n  PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{chat_history}\nHuman: {input}\nAI:`);\nconst chain = new LLMChain({ llm: model, prompt, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n","metadata":{"source":"examples/src/memory/buffer.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new BufferWindowMemory({ memoryKey: \"chat_history\", k: 1 });\n  const model = new OpenAI({ temperature: 0.9 });\n  const template = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n    Current conversation:\n    {chat_history}\n    Human: {input}\n    AI:`;\n\n  const prompt = PromptTemplate.fromTemplate(template);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1 });\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2 });\n};\n","metadata":{"source":"examples/src/memory/buffer_window.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationSummaryMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new ConversationSummaryMemory({\n    memoryKey: \"chat_history\",\n    llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  });\n\n  const model = new ChatOpenAI();\n  const prompt =\n    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n  Current conversation:\n  {chat_history}\n  Human: {input}\n  AI:`);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res1: {\n      text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'\n    }\n  }\n  */\n\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res2: {\n      text: \"Your name is Jim. It's nice to meet you, Jim. How can I assist you today?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/memory/summary_chat.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationSummaryMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new ConversationSummaryMemory({\n    memoryKey: \"chat_history\",\n    llm: new OpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  });\n\n  const model = new OpenAI({ temperature: 0.9 });\n  const prompt =\n    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n  Current conversation:\n  {chat_history}\n  Human: {input}\n  AI:`);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res1: {\n      text: \" Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'\n    }\n  }\n  */\n\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res2: { text: ' You told me your name is Jim.' },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'\n    }\n  }\n  */\n};\n","metadata":{"source":"examples/src/memory/summary_llm.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage, SystemChatMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });\n  // Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.\n  const responseA = await chat.call([\n    new HumanChatMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ]);\n  console.log(responseA);\n  // AIChatMessage { text: '\\n\\nRainbow Sox Co.' }\n\n  // You can also pass in multiple messages to start a conversation.\n  // The first message is a system message that describes the context of the conversation.\n  // The second message is a human message that starts the conversation.\n  const responseB = await chat.call([\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\"Translate: I love programming.\"),\n  ]);\n  console.log(responseB);\n  // AIChatMessage { text: \"J'aime programmer.\" }\n\n  // Similar to LLMs, you can also use `generate` to generate chat completions for multiple sets of messages.\n  const responseC = await chat.generate([\n    [\n      new SystemChatMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanChatMessage(\n        \"Translate this sentence from English to French. I love programming.\"\n      ),\n    ],\n    [\n      new SystemChatMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanChatMessage(\n        \"Translate this sentence from English to French. I love artificial intelligence.\"\n      ),\n    ],\n  ]);\n  console.log(responseC);\n  /*\n  {\n    generations: [\n      [\n        {\n          text: \"J'aime programmer.\",\n          message: AIChatMessage { text: \"J'aime programmer.\" },\n        }\n      ],\n      [\n        {\n          text: \"J'aime l'intelligence artificielle.\",\n          message: AIChatMessage { text: \"J'aime l'intelligence artificielle.\" }\n        }\n      ]\n    ]\n  }\n  */\n};\n","metadata":{"source":"examples/src/models/chat/chat.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI();\n  // Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.\n  const response = await chat.call([\n    new HumanChatMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ]);\n  console.log(response);\n  // AIChatMessage { text: '\\n\\nRainbow Sox Co.' }\n};\n","metadata":{"source":"examples/src/models/chat/chat_quick_start.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({\n    maxTokens: 25,\n    streaming: true,\n    callbacks: [\n      {\n        handleLLMNewToken(token: string) {\n          console.log({ token });\n        },\n      },\n    ],\n  });\n\n  const response = await chat.call([new HumanChatMessage(\"Tell me a joke.\")]);\n\n  console.log(response);\n  // { token: '' }\n  // { token: '\\n\\n' }\n  // { token: 'Why' }\n  // { token: ' don' }\n  // { token: \"'t\" }\n  // { token: ' scientists' }\n  // { token: ' trust' }\n  // { token: ' atoms' }\n  // { token: '?\\n\\n' }\n  // { token: 'Because' }\n  // { token: ' they' }\n  // { token: ' make' }\n  // { token: ' up' }\n  // { token: ' everything' }\n  // { token: '.' }\n  // { token: '' }\n  // AIChatMessage {\n  //   text: \"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\"\n  // }\n};\n","metadata":{"source":"examples/src/models/chat/chat_streaming.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({\n  streaming: true,\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        process.stdout.write(token);\n      },\n    },\n  ],\n});\n\nawait chat.call([\n  new HumanChatMessage(\"Write me a song about sparkling water.\"),\n]);\n/*\nVerse 1:\nBubbles rise, crisp and clear\nRefreshing taste that brings us cheer\nSparkling water, so light and pure\nQuenches our thirst, it's always secure\n\nChorus:\nSparkling water, oh how we love\nIts fizzy bubbles and grace above\nIt's the perfect drink, anytime, anyplace\nRefreshing as it gives us a taste\n\nVerse 2:\nFrom morning brunch to evening feast\nIt's the perfect drink for a treat\nA sip of it brings a smile so bright\nOur thirst is quenched in just one sip so light\n...\n*/\n","metadata":{"source":"examples/src/models/chat/chat_streaming_stdout.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanChatMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI(\n    { temperature: 1, timeout: 1000 } // 1s timeout\n  );\n\n  const response = await chat.call([\n    new HumanChatMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ]);\n  console.log(response);\n  // AIChatMessage { text: '\\n\\nRainbow Sox Co.' }\n};\n","metadata":{"source":"examples/src/models/chat/chat_timeout.ts"}},{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\nconst model = new ChatAnthropic({\n  temperature: 0.9,\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.ANTHROPIC_API_KEY\n});\n","metadata":{"source":"examples/src/models/chat/integration_anthropic.ts"}},{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\n","metadata":{"source":"examples/src/models/chat/integration_openai.ts"}},{"pageContent":"import { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new CohereEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};\n","metadata":{"source":"examples/src/models/embeddings/cohere.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new OpenAIEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};\n","metadata":{"source":"examples/src/models/embeddings/openai.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const embeddings = new OpenAIEmbeddings({\n    timeout: 1000, // 1s timeout\n  });\n  /* Embed queries */\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};\n","metadata":{"source":"examples/src/models/embeddings/openai_timeout.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const modelA = new OpenAI();\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await modelA.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  // { resA: '\\n\\nSocktastic Colors' }\n\n  // `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).\n  const resB = await modelA.generate([\n    \"What would be a good company name a company that makes colorful socks?\",\n    \"What would be a good company name a company that makes colorful sweaters?\",\n  ]);\n\n  // `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.\n  // `generations` is a `Generation[][]`, each `Generation` having a `text` field.\n  // Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.\n  console.log(JSON.stringify(resB, null, 2));\n  /*\n  {\n      \"generations\": [\n          [{\n              \"text\": \"\\n\\nVibrant Socks Co.\",\n              \"generationInfo\": {\n                  \"finishReason\": \"stop\",\n                  \"logprobs\": null\n              }\n          }],\n          [{\n              \"text\": \"\\n\\nRainbow Knitworks.\",\n              \"generationInfo\": {\n                  \"finishReason\": \"stop\",\n                  \"logprobs\": null\n              }\n          }]\n      ],\n      \"llmOutput\": {\n          \"tokenUsage\": {\n              \"completionTokens\": 17,\n              \"promptTokens\": 29,\n              \"totalTokens\": 46\n          }\n      }\n  }\n  */\n\n  // We can specify additional parameters the specific model provider supports, like `temperature`:\n  const modelB = new OpenAI({ temperature: 0.9 });\n  const resC = await modelA.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resC });\n  // { resC: '\\n\\nKaleidoSox' }\n\n  // We can get the number of tokens for a given input for a specific model.\n  const numTokens = modelB.getNumTokens(\"How many tokens are in this input?\");\n  console.log({ numTokens });\n  // { numTokens: 8 }\n};\n","metadata":{"source":"examples/src/models/llm/llm.ts"}},{"pageContent":"import { LLMResult } from \"langchain/schema\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  // We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.\n  const model = new OpenAI({\n    verbose: true,\n    callbacks: [\n      {\n        handleLLMStart: async (llm: { name: string }, prompts: string[]) => {\n          console.log(JSON.stringify(llm, null, 2));\n          console.log(JSON.stringify(prompts, null, 2));\n        },\n        handleLLMEnd: async (output: LLMResult) => {\n          console.log(JSON.stringify(output, null, 2));\n        },\n        handleLLMError: async (err: Error) => {\n          console.error(err);\n        },\n      },\n    ],\n  });\n\n  await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  // {\n  //     \"name\": \"openai\"\n  // }\n  // [\n  //     \"What would be a good company name a company that makes colorful socks?\"\n  // ]\n  // {\n  //   \"generations\": [\n  //     [\n  //         {\n  //             \"text\": \"\\n\\nSocktastic Splashes.\",\n  //             \"generationInfo\": {\n  //                 \"finishReason\": \"stop\",\n  //                 \"logprobs\": null\n  //             }\n  //         }\n  //     ]\n  //  ],\n  //   \"llmOutput\": {\n  //     \"tokenUsage\": {\n  //         \"completionTokens\": 9,\n  //          \"promptTokens\": 14,\n  //          \"totalTokens\": 23\n  //     }\n  //   }\n  // }\n};\n","metadata":{"source":"examples/src/models/llm/llm_debugging.ts"}},{"pageContent":"import { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new PromptLayerOpenAI({ temperature: 0.9 });\n  const res = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/models/llm/llm_promptlayer.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI();\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  // { resA: '\\n\\nSocktastic Colors' }\n};\n","metadata":{"source":"examples/src/models/llm/llm_quick_start.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  // To enable streaming, we pass in `streaming: true` to the LLM constructor.\n  // Additionally, we pass in a handler for the `handleLLMNewToken` event.\n  const chat = new OpenAI({\n    maxTokens: 25,\n    streaming: true,\n    callbacks: [\n      {\n        handleLLMNewToken(token: string) {\n          console.log({ token });\n        },\n      },\n    ],\n  });\n\n  const response = await chat.call(\"Tell me a joke.\");\n  console.log(response);\n  /*\n  { token: '\\n' }\n  { token: '\\n' }\n  { token: 'Q' }\n  { token: ':' }\n  { token: ' Why' }\n  { token: ' did' }\n  { token: ' the' }\n  { token: ' chicken' }\n  { token: ' cross' }\n  { token: ' the' }\n  { token: ' playground' }\n  { token: '?' }\n  { token: '\\n' }\n  { token: 'A' }\n  { token: ':' }\n  { token: ' To' }\n  { token: ' get' }\n  { token: ' to' }\n  { token: ' the' }\n  { token: ' other' }\n  { token: ' slide' }\n  { token: '.' }\n\n\n  Q: Why did the chicken cross the playground?\n  A: To get to the other slide.\n  */\n};\n","metadata":{"source":"examples/src/models/llm/llm_streaming.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\n// To enable streaming, we pass in `streaming: true` to the LLM constructor.\n// Additionally, we pass in a handler for the `handleLLMNewToken` event.\nconst chat = new OpenAI({\n  streaming: true,\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        process.stdout.write(token);\n      },\n    },\n  ],\n});\n\nawait chat.call(\"Write me a song about sparkling water.\");\n/*\nVerse 1\nCrystal clear and made with care\nSparkling water on my lips, so refreshing in the air\nFizzy bubbles, light and sweet\nMy favorite beverage I can’t help but repeat\n\nChorus\nA toast to sparkling water, I’m feeling so alive\nLet’s take a sip, and let’s take a drive\nA toast to sparkling water, it’s the best I’ve had in my life\nIt’s the best way to start off the night\n\nVerse 2\nIt’s the perfect drink to quench my thirst\nIt’s the best way to stay hydrated, it’s the first\nA few ice cubes, a splash of lime\nIt will make any day feel sublime\n...\n*/\n","metadata":{"source":"examples/src/models/llm/llm_streaming_stdout.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI(\n    { temperature: 1, timeout: 1000 } // 1s timeout\n  );\n\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n\n  console.log({ resA });\n  // '\\n\\nSocktastic Colors' }\n};\n","metadata":{"source":"examples/src/models/llm/llm_timeout.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SystemChatMessage, HumanChatMessage } from \"langchain/schema\";\nimport * as process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0.9 });\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n\n  const chat = new ChatOpenAI({ temperature: 0 });\n  const system_message = new SystemChatMessage(\"You are to chat with a user.\");\n  const message = new HumanChatMessage(\"Hello!\");\n  const resB = await chat.call([system_message, message]);\n  console.log({ resB });\n};\n","metadata":{"source":"examples/src/models/llm/llm_with_tracing.ts"}},{"pageContent":"import { Replicate } from \"langchain/llms/replicate\";\n\nexport const run = async () => {\n  const modelA = new Replicate({\n    model:\n      \"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\n  });\n\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await modelA.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  /*\n  {\n    resA: 'Color Box'\n  }\n  */\n\n  // `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).\n  const resB = await modelA.generate([\n    \"What would be a good company name a company that makes colorful socks?\",\n    \"What would be a good company name a company that makes colorful sweaters?\",\n  ]);\n  // `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.\n  // `generations` is a `Generation[][]`, each `Generation` having a `text` field.\n  // Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.\n  console.log(JSON.stringify(resB, null, 2));\n  /*\n  {\n    \"generations\": [\n      [\n        {\n          \"text\": \"apron string\"\n        }\n      ],\n      [\n        {\n          \"text\": \"Kulut\"\n        }\n      ]\n    ]\n  }\n  */\n\n  const text2image = new Replicate({\n    model:\n      \"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n  });\n\n  const image = await text2image.call(\"A cat\");\n  console.log({ image });\n  /*\n  {\n    \"image\": \"https://replicate.delivery/pbxt/Nc8qkJ8zkdpDPdNSYuMaDErImcXVMUAybFrLk9Kane7IKOWIA/out-0.png\"\n  }\n  */\n};\n","metadata":{"source":"examples/src/models/llm/replicate.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  StructuredOutputParser,\n  RegexParser,\n  CombiningOutputParser,\n} from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  const answerParser = StructuredOutputParser.fromNamesAndDescriptions({\n    answer: \"answer to the user's question\",\n    source: \"source used to answer the user's question, should be a website.\",\n  });\n\n  const confidenceParser = new RegexParser(\n    /Confidence: (A|B|C), Explanation: (.*)/,\n    [\"confidence\", \"explanation\"],\n    \"noConfidence\"\n  );\n\n  const parser = new CombiningOutputParser(answerParser, confidenceParser);\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template:\n      \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n    inputVariables: [\"question\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({\n    question: \"What is the capital of France?\",\n  });\n  const response = await model.call(input);\n\n  console.log(input);\n  /*\n  Answer the users question as best as possible.\n  For your first output: The output should be a markdown code snippet formatted in the following schema:\n\n  ```json\n  {\n    \"answer\": string // answer to the user's question\n    \"source\": string // source used to answer the user's question, should be a website.\n  }\n  ```\n\n  Including the leading and trailing \"```json\" and \"```\"\n\n  Complete that output fully. Then produce another output: Your response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/\n  What is the capital of France?\n  */\n\n  console.log(response);\n  /*\n  ```json\n  {\n    \"answer\": \"Paris\",\n    \"source\": \"https://en.wikipedia.org/wiki/Paris\"\n  }\n  ```\n  Confidence: A, Explanation: Paris is the capital of France, according to Wikipedia.\n  */\n\n  console.log(await parser.parse(response));\n  /*\n  {\n    answer: 'Paris',\n    source: 'https://en.wikipedia.org/wiki/Paris',\n    confidence: 'A',\n    explanation: 'Paris is the capital of France, according to Wikipedia.'\n  }\n  */\n};\n","metadata":{"source":"examples/src/prompts/combining_parser.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { CommaSeparatedListOutputParser } from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  // With a `CommaSeparatedListOutputParser`, we can parse a comma separated list.\n  const parser = new CommaSeparatedListOutputParser();\n\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template: \"List five {subject}.\\n{format_instructions}\",\n    inputVariables: [\"subject\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({ subject: \"ice cream flavors\" });\n  const response = await model.call(input);\n\n  console.log(input);\n  /*\n   List five ice cream flavors.\n   Your response should be a list of comma separated values, eg: `foo, bar, baz`\n  */\n\n  console.log(response);\n  // Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream\n\n  console.log(parser.parse(response));\n  /*\n  [\n    'Vanilla',\n    'Chocolate',\n    'Strawberry',\n    'Mint Chocolate Chip',\n    'Cookies and Cream'\n  ]\n  */\n};\n","metadata":{"source":"examples/src/prompts/comma_list_parser.ts"}},{"pageContent":"import { FewShotPromptTemplate, PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  // First, create a list of few-shot examples.\n  const examples = [\n    { word: \"happy\", antonym: \"sad\" },\n    { word: \"tall\", antonym: \"short\" },\n  ];\n\n  // Next, we specify the template to format the examples we have provided.\n  const exampleFormatterTemplate = \"Word: {word}\\nAntonym: {antonym}\\n\";\n  const examplePrompt = new PromptTemplate({\n    inputVariables: [\"word\", \"antonym\"],\n    template: exampleFormatterTemplate,\n  });\n  // Finally, we create the `FewShotPromptTemplate`\n  const fewShotPrompt = new FewShotPromptTemplate({\n    /* These are the examples we want to insert into the prompt. */\n    examples,\n    /* This is how we want to format the examples when we insert them into the prompt. */\n    examplePrompt,\n    /* The prefix is some text that goes before the examples in the prompt. Usually, this consists of intructions. */\n    prefix: \"Give the antonym of every input\",\n    /* The suffix is some text that goes after the examples in the prompt. Usually, this is where the user input will go */\n    suffix: \"Word: {input}\\nAntonym:\",\n    /* The input variables are the variables that the overall prompt expects. */\n    inputVariables: [\"input\"],\n    /* The example_separator is the string we will use to join the prefix, examples, and suffix together with. */\n    exampleSeparator: \"\\n\\n\",\n    /* The template format is the formatting method to use for the template. Should usually be f-string. */\n    templateFormat: \"f-string\",\n  });\n\n  // We can now generate a prompt using the `format` method.\n  console.log(await fewShotPrompt.format({ input: \"big\" }));\n  /*\n  Give the antonym of every input\n\n  Word: happy\n  Antonym: sad\n\n\n  Word: tall\n  Antonym: short\n\n\n  Word: big\n  Antonym:\n  */\n};\n","metadata":{"source":"examples/src/prompts/few_shot.ts"}},{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  StructuredOutputParser,\n  OutputFixingParser,\n} from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  const parser = StructuredOutputParser.fromZodSchema(\n    z.object({\n      answer: z.string().describe(\"answer to the user's question\"),\n      sources: z\n        .array(z.string())\n        .describe(\"sources used to answer the question, should be websites.\"),\n    })\n  );\n  /** This is a bad output because sources is a string, not a list */\n  const badOutput = `\\`\\`\\`json\n  {\n    \"answer\": \"foo\",\n    \"sources\": \"foo.com\"\n  }\n  \\`\\`\\``;\n  try {\n    await parser.parse(badOutput);\n  } catch (e) {\n    console.log(\"Failed to parse bad output: \", e);\n    /*\n    Failed to parse bad output:  OutputParserException [Error]: Failed to parse. Text: ```json\n      {\n        \"answer\": \"foo\",\n        \"sources\": \"foo.com\"\n      }\n      ```. Error: [\n      {\n        \"code\": \"invalid_type\",\n        \"expected\": \"array\",\n        \"received\": \"string\",\n        \"path\": [\n          \"sources\"\n        ],\n        \"message\": \"Expected array, received string\"\n      }\n    ]\n    at StructuredOutputParser.parse (/Users/ankushgola/Code/langchainjs/langchain/src/output_parsers/structured.ts:71:13)\n    at run (/Users/ankushgola/Code/langchainjs/examples/src/prompts/fix_parser.ts:25:18)\n    at <anonymous> (/Users/ankushgola/Code/langchainjs/examples/src/index.ts:33:22)\n   */\n  }\n  const fixParser = OutputFixingParser.fromLLM(\n    new ChatOpenAI({ temperature: 0 }),\n    parser\n  );\n  const output = await fixParser.parse(badOutput);\n  console.log(\"Fixed output: \", output);\n  // Fixed output:  { answer: 'foo', sources: [ 'foo.com' ] }\n};\n","metadata":{"source":"examples/src/prompts/fix_parser.ts"}},{"pageContent":"import {\n  LengthBasedExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\n\nexport async function run() {\n  // Create a prompt template that will be used to format the examples.\n  const examplePrompt = new PromptTemplate({\n    inputVariables: [\"input\", \"output\"],\n    template: \"Input: {input}\\nOutput: {output}\",\n  });\n\n  // Create a LengthBasedExampleSelector that will be used to select the examples.\n  const exampleSelector = await LengthBasedExampleSelector.fromExamples(\n    [\n      { input: \"happy\", output: \"sad\" },\n      { input: \"tall\", output: \"short\" },\n      { input: \"energetic\", output: \"lethargic\" },\n      { input: \"sunny\", output: \"gloomy\" },\n      { input: \"windy\", output: \"calm\" },\n    ],\n    {\n      examplePrompt,\n      maxLength: 25,\n    }\n  );\n\n  // Create a FewShotPromptTemplate that will use the example selector.\n  const dynamicPrompt = new FewShotPromptTemplate({\n    // We provide an ExampleSelector instead of examples.\n    exampleSelector,\n    examplePrompt,\n    prefix: \"Give the antonym of every input\",\n    suffix: \"Input: {adjective}\\nOutput:\",\n    inputVariables: [\"adjective\"],\n  });\n\n  // An example with small input, so it selects all examples.\n  console.log(await dynamicPrompt.format({ adjective: \"big\" }));\n  /*\n   Give the antonym of every input\n\n   Input: happy\n   Output: sad\n\n   Input: tall\n   Output: short\n\n   Input: energetic\n   Output: lethargic\n\n   Input: sunny\n   Output: gloomy\n\n   Input: windy\n   Output: calm\n\n   Input: big\n   Output:\n   */\n\n  // An example with long input, so it selects only one example.\n  const longString =\n    \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\";\n  console.log(await dynamicPrompt.format({ adjective: longString }));\n  /*\n   Give the antonym of every input\n\n   Input: happy\n   Output: sad\n\n   Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n   Output:\n   */\n}\n","metadata":{"source":"examples/src/prompts/length_based_example_selector.ts"}},{"pageContent":"import { loadPrompt } from \"langchain/prompts/load\";\n\nexport const run = async () => {\n  const prompt = await loadPrompt(\"lc://prompts/hello-world/prompt.yaml\");\n  const res = await prompt.format({});\n  console.log({ res });\n};\n","metadata":{"source":"examples/src/prompts/load_from_hub.ts"}},{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  // The `partial` method returns a new `PromptTemplate` object that can be used to format the prompt with only some of the input variables.\n  const promptA = new PromptTemplate({\n    template: \"{foo}{bar}\",\n    inputVariables: [\"foo\", \"bar\"],\n  });\n  const partialPromptA = await promptA.partial({ foo: \"foo\" });\n  console.log(await partialPromptA.format({ bar: \"bar\" }));\n  // foobar\n\n  // You can also explicitly specify the partial variables when creating the `PromptTemplate` object.\n  const promptB = new PromptTemplate({\n    template: \"{foo}{bar}\",\n    inputVariables: [\"foo\"],\n    partialVariables: { bar: \"bar\" },\n  });\n  console.log(await promptB.format({ foo: \"foo\" }));\n  // foobar\n\n  // You can also use partial formatting with function inputs instead of string inputs.\n  const promptC = new PromptTemplate({\n    template: \"Tell me a {adjective} joke about the day {date}\",\n    inputVariables: [\"adjective\", \"date\"],\n  });\n  const partialPromptC = await promptC.partial({\n    date: () => new Date().toLocaleDateString(),\n  });\n  console.log(await partialPromptC.format({ adjective: \"funny\" }));\n  // Tell me a funny joke about the day 3/22/2023\n\n  const promptD = new PromptTemplate({\n    template: \"Tell me a {adjective} joke about the day {date}\",\n    inputVariables: [\"adjective\"],\n    partialVariables: { date: () => new Date().toLocaleDateString() },\n  });\n  console.log(await promptD.format({ adjective: \"funny\" }));\n  // Tell me a funny joke about the day 3/22/2023\n};\n","metadata":{"source":"examples/src/prompts/partial.ts"}},{"pageContent":"import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  const template = \"What is a good name for a company that makes {product}?\";\n  const promptA = new PromptTemplate({ template, inputVariables: [\"product\"] });\n\n  // The `formatPromptValue` method returns a `PromptValue` object that can be used to format the prompt as a string or a list of `ChatMessage` objects.\n  const responseA = await promptA.formatPromptValue({\n    product: \"colorful socks\",\n  });\n  const responseAString = responseA.toString();\n  console.log({ responseAString });\n  /*\n    {\n        responseAString: 'What is a good name for a company that makes colorful socks?'\n    }\n    */\n\n  const responseAMessages = responseA.toChatMessages();\n  console.log({ responseAMessages });\n  /*\n    {\n        responseAMessages: [\n            HumanChatMessage {\n                text: 'What is a good name for a company that makes colorful socks?'\n            }\n        ]\n    }\n    */\n\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  // `formatPromptValue` also works with `ChatPromptTemplate`.\n  const responseB = await chatPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  const responseBString = responseB.toString();\n  console.log({ responseBString });\n  /*\n    {\n        responseBString: '[{\"text\":\"You are a helpful assistant that translates English to French.\"},{\"text\":\"I love programming.\"}]'\n    }\n    */\n\n  const responseBMessages = responseB.toChatMessages();\n  console.log({ responseBMessages });\n  /*\n    {\n        responseBMessages: [\n            SystemChatMessage {\n                text: 'You are a helpful assistant that translates English to French.'\n            },\n            HumanChatMessage { text: 'I love programming.' }\n        ]\n    }\n    */\n};\n","metadata":{"source":"examples/src/prompts/prompt_value.ts"}},{"pageContent":"import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  // A `PromptTemplate` consists of a template string and a list of input variables.\n  const template = \"What is a good name for a company that makes {product}?\";\n  const promptA = new PromptTemplate({ template, inputVariables: [\"product\"] });\n\n  // We can use the `format` method to format the template with the given input values.\n  const responseA = await promptA.format({ product: \"colorful socks\" });\n  console.log({ responseA });\n  /*\n  {\n    responseA: 'What is a good name for a company that makes colorful socks?'\n  }\n  */\n\n  // We can also use the `fromTemplate` method to create a `PromptTemplate` object.\n  const promptB = PromptTemplate.fromTemplate(\n    \"What is a good name for a company that makes {product}?\"\n  );\n  const responseB = await promptB.format({ product: \"colorful socks\" });\n  console.log({ responseB });\n  /*\n  {\n    responseB: 'What is a good name for a company that makes colorful socks?'\n  }\n  */\n\n  // For chat models, we provide a `ChatPromptTemplate` class that can be used to format chat prompts.\n  const chatPrompt = ChatPromptTemplate.fromPromptMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  // The result can be formatted as a string using the `format` method.\n  const responseC = await chatPrompt.format({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  console.log({ responseC });\n  /*\n  {\n    responseC: '[{\"text\":\"You are a helpful assistant that translates English to French.\"},{\"text\":\"I love programming.\"}]'\n  }\n  */\n\n  // The result can also be formatted as a list of `ChatMessage` objects by returning a `PromptValue` object and calling the `toChatMessages` method.\n  // More on this below.\n  const responseD = await chatPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  const messages = responseD.toChatMessages();\n  console.log({ messages });\n  /*\n  {\n    messages: [\n        SystemChatMessage {\n          text: 'You are a helpful assistant that translates English to French.'\n        },\n        HumanChatMessage { text: 'I love programming.' }\n      ]\n  }\n  */\n};\n","metadata":{"source":"examples/src/prompts/prompts.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RegexParser } from \"langchain/output_parsers\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const parser = new RegexParser(\n    /Humor: ([0-9]+), Sophistication: (A|B|C|D|E)/,\n    [\"mark\", \"grade\"],\n    \"noConfidence\"\n  );\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template: \"Grade the joke.\\n\\n{format_instructions}\\n\\nJoke: {joke}\",\n    inputVariables: [\"joke\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({\n    joke: \"What time is the appointment? Tooth hurt-y.\",\n  });\n  console.log(input);\n  /*\n  Grade the joke.\n\n  Your response should match the following regex: /Humor: ([0-9]+), Sophistication: (A|B|C|D|E)/\n\n  Joke: What time is the appointment? Tooth hurt-y.\n  */\n\n  const response = await model.call(input);\n  console.log(response);\n  /*\n  Humor: 8, Sophistication: D\n  */\n};\n","metadata":{"source":"examples/src/prompts/regex_parser.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport {\n  SemanticSimilarityExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\n\nexport async function run() {\n  // Create a prompt template that will be used to format the examples.\n  const examplePrompt = new PromptTemplate({\n    inputVariables: [\"input\", \"output\"],\n    template: \"Input: {input}\\nOutput: {output}\",\n  });\n\n  // Create a SemanticSimilarityExampleSelector that will be used to select the examples.\n  const exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(\n    [\n      { input: \"happy\", output: \"sad\" },\n      { input: \"tall\", output: \"short\" },\n      { input: \"energetic\", output: \"lethargic\" },\n      { input: \"sunny\", output: \"gloomy\" },\n      { input: \"windy\", output: \"calm\" },\n    ],\n    new OpenAIEmbeddings(),\n    HNSWLib,\n    { k: 1 }\n  );\n\n  // Create a FewShotPromptTemplate that will use the example selector.\n  const dynamicPrompt = new FewShotPromptTemplate({\n    // We provide an ExampleSelector instead of examples.\n    exampleSelector,\n    examplePrompt,\n    prefix: \"Give the antonym of every input\",\n    suffix: \"Input: {adjective}\\nOutput:\",\n    inputVariables: [\"adjective\"],\n  });\n\n  // Input is about the weather, so should select eg. the sunny/gloomy example\n  console.log(await dynamicPrompt.format({ adjective: \"rainy\" }));\n  /*\n   Give the antonym of every input\n\n   Input: sunny\n   Output: gloomy\n\n   Input: rainy\n   Output:\n   */\n\n  // Input is a measurement, so should select the tall/short example\n  console.log(await dynamicPrompt.format({ adjective: \"large\" }));\n  /*\n   Give the antonym of every input\n\n   Input: tall\n   Output: short\n\n   Input: large\n   Output:\n   */\n}\n","metadata":{"source":"examples/src/prompts/semantic_similarity_example_selector.ts"}},{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  // With a `StructuredOutputParser` we can define a schema for the output.\n  const parser = StructuredOutputParser.fromNamesAndDescriptions({\n    answer: \"answer to the user's question\",\n    source: \"source used to answer the user's question, should be a website.\",\n  });\n\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template:\n      \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n    inputVariables: [\"question\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({\n    question: \"What is the capital of France?\",\n  });\n  const response = await model.call(input);\n\n  console.log(input);\n  /*\n  Answer the users question as best as possible.\n  The output should be a markdown code snippet formatted in the following schema:\n  ```json\n  {\n      \"answer\": string // answer to the user's question\n      \"source\": string // source used to answer the user's question, should be a website.\n  }\n  ```\n  */\n\n  console.log(response);\n  /*\n  ```json\n  {\n      \"answer\": \"Paris\",\n      \"source\": \"https://en.wikipedia.org/wiki/France\"\n  }\n  ```\n  */\n\n  console.log(parser.parse(response));\n  // { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/France' }\n};\n","metadata":{"source":"examples/src/prompts/structured_parser.ts"}},{"pageContent":"import { z } from \"zod\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  // We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.\n  const parser = StructuredOutputParser.fromZodSchema(\n    z.object({\n      answer: z.string().describe(\"answer to the user's question\"),\n      sources: z\n        .array(z.string())\n        .describe(\"sources used to answer the question, should be websites.\"),\n    })\n  );\n\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template:\n      \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n    inputVariables: [\"question\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({\n    question: \"What is the capital of France?\",\n  });\n  const response = await model.call(input);\n\n  console.log(input);\n  /*\n  Answer the users question as best as possible.\n  The output should be a markdown code snippet formatted in the following schema:\n  */\n\n  console.log(response);\n  /*\n  ```json\n  {\n      \"answer\": \"The capital of France is Paris.\",\n      \"sources\": [\"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\"]\n  }\n  ```\n  */\n\n  console.log(parser.parse(response));\n  /*\n  {\n    answer: 'The capital of France is Paris.',\n    sources: [\n      'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html'\n    ]\n  }\n  */\n};\n","metadata":{"source":"examples/src/prompts/structured_parser_zod.ts"}},{"pageContent":"import { ChatGPTPluginRetriever } from \"langchain/retrievers/remote\";\n\nexport const run = async () => {\n  const retriever = new ChatGPTPluginRetriever({\n    url: \"http://0.0.0.0:8000\",\n    auth: {\n      bearer: \"super-secret-jwt-token-with-at-least-32-characters-long\",\n    },\n  });\n\n  const docs = await retriever.getRelevantDocuments(\"hello world\");\n\n  console.log(docs);\n};\n","metadata":{"source":"examples/src/retrievers/chatgpt-plugin.ts"}},{"pageContent":"import { DataberryRetriever } from \"langchain/retrievers/databerry\";\n\nexport const run = async () => {\n  const retriever = new DataberryRetriever({\n    datastoreUrl: \"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc\",\n    apiKey: \"DATABERRY_API_KEY\", // optional: needed for private datastores\n    topK: 8, // optional: default value is 3\n  });\n\n  const docs = await retriever.getRelevantDocuments(\"hello\");\n\n  console.log(docs);\n};\n","metadata":{"source":"examples/src/retrievers/databerry.ts"}},{"pageContent":"/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport Metal from \"@getmetal/metal-sdk\";\nimport { MetalRetriever } from \"langchain/retrievers/metal\";\n\nexport const run = async () => {\n  const MetalSDK = Metal.default;\n\n  const client = new MetalSDK(\n    process.env.METAL_API_KEY!,\n    process.env.METAL_CLIENT_ID!,\n    process.env.METAL_INDEX_ID\n  );\n  const retriever = new MetalRetriever({ client });\n\n  const docs = await retriever.getRelevantDocuments(\"hello\");\n\n  console.log(docs);\n};\n","metadata":{"source":"examples/src/retrievers/metal.ts"}},{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\nimport { SupabaseHybridSearch } from \"langchain/retrievers/supabase\";\n\nexport const run = async () => {\n  const client = createClient(\n    process.env.SUPABASE_URL || \"\",\n    process.env.SUPABASE_PRIVATE_KEY || \"\"\n  );\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const retriever = new SupabaseHybridSearch(embeddings, {\n    client,\n    //  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.\n    similarityK: 2,\n    keywordK: 2,\n    tableName: \"documents\",\n    similarityQueryName: \"match_documents\",\n    keywordQueryName: \"kw_match_documents\",\n  });\n\n  const results = await retriever.getRelevantDocuments(\"hello bye\");\n\n  console.log(results);\n};\n","metadata":{"source":"examples/src/retrievers/supabase_hybrid.ts"}},{"pageContent":"import { WebBrowser } from \"langchain/tools/webbrowser\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const embeddings = new OpenAIEmbeddings();\n\n  const browser = new WebBrowser({ model, embeddings });\n\n  const result = await browser.call(\n    `\"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth\",\"who is joseph campbell\"`\n  );\n\n  console.log(result);\n  /*\n  Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucas’s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbell’s death, in what became one of the most popular in the history of public television.\n\n  Relevant Links:\n  - [The Holstee Manifesto](http://holstee.com/manifesto-bp)\n  - [The Silent Music of the Mind: Remembering Oliver Sacks](https://www.themarginalian.org/2015/08/31/remembering-oliver-sacks)\n  - [Joseph Campbell series](http://billmoyers.com/spotlight/download-joseph-campbell-and-the-power-of-myth-audio/)\n  - [Bill Moyers](https://www.themarginalian.org/tag/bill-moyers/)\n  - [books](https://www.themarginalian.org/tag/books/)\n  */\n}\n","metadata":{"source":"examples/src/tools/webbrowser.ts"}}]